{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking LSTMs\n",
    "\n",
    "This notebook contains experiments on 3 different sequence tasks:\n",
    "* __Sequence Labelling__\n",
    "    * A many-to-one, _Sentiment Analysis_ task on the IMDb dataset available with _torchtext_.\n",
    "* __Sequence to Sequence - Same__\n",
    "    * A many-to-many-same, _Predict missing word_ task on the Facebook bAbi dataset.\n",
    "* __Sequence to Sequence - Different__\n",
    "    * A many-to-one-different, _NTM toy_ task.\n",
    "    \n",
    "For each of the tasks, we will run our implementation of vanilla LSTM. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T23:18:33.946357Z",
     "start_time": "2019-07-10T23:18:32.653217Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sequence Labelling\n",
    "\n",
    "All code pertaining to this task will have a pre-/post-fix **SeqLabel**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T23:18:34.867719Z",
     "start_time": "2019-07-10T23:18:34.864536Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "split_ratio = 0.8\n",
    "learning_rate = 0.001\n",
    "epochs = 1 #200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T23:18:39.350227Z",
     "start_time": "2019-07-10T23:18:36.294738Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6adf81052c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDB_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m train_loader, valid_loader, test_loader = imdb.create_data_loader(batch_size, \n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-project/SequenceModelling/src/imdb.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, split_ratio, random_state, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m## create field - tokenize text & create label classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'spacy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sequential, use_vocab, init_token, eos_token, fix_length, dtype, preprocessing, postprocessing, lower, tokenize, include_lengths, batch_first, pad_token, unk_token, pad_first, truncate_first, stop_words, is_target)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mspacy_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from imdb import IMDB_dataset\n",
    "\n",
    "imdb = IMDB_dataset(split_ratio, seed)\n",
    "imdb.load(verbose = True)\n",
    "imdb.build_vocab(embed_dim)\n",
    "train_loader, valid_loader, test_loader = imdb.create_data_loader(batch_size, \n",
    "                                                                  device)\n",
    "vocab_len = len(imdb.TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension/dimension for LSTM cell inputs\n",
    "embed_dim = 300\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 256\n",
    "# Number of output nodes\n",
    "output_dim = 1\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = False\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation\n",
    "\n",
    "from seq_label import LSTMSeqLabel, SeqLabel\n",
    "\n",
    "# Initializing model\n",
    "model = LSTMSeqLabel(vocab_len, embed_dim, hidden_dim, output_dim, \n",
    "                      imdb.pretrained_weights, layers, bidirectional,\n",
    "                      layernorm)\n",
    "model.to(device)\n",
    "\n",
    "# Initializing optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Initializing task\n",
    "task = SeqLabel(model, optimizer, loss_criterion, device)\n",
    "\n",
    "# Training\n",
    "freq = 5    # epoch interval to calculate F1 score and save models\n",
    "out_dir = \"results/seq_label/\"\n",
    "# out_dir = \"/content/drive/My Drive/colab/seq_label/\"\n",
    "model, stats = task.train(epochs, train_loader, valid_loader, freq, out_dir)\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "f1_test = task.evaluate(test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation\n",
    "\n",
    "from seq_label import SentimentNetworkBaseline\n",
    "\n",
    "# Initializing model\n",
    "model = SentimentNetworkBaseline(vocab_len, embed_dim, hidden_dim, output_dim, \n",
    "                       imdb.pretrained_weights, layers, bidirectional)\n",
    "model.to(device)\n",
    "\n",
    "# Initializing optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Initializing task\n",
    "task = SeqLabel(model, optimizer, loss_criterion, device)\n",
    "\n",
    "# Training\n",
    "freq = 5    # epoch interval to calculate F1 score and save models\n",
    "out_dir = \"results/seq_label/pytorch/\"\n",
    "# out_dir = \"/content/drive/My Drive/colab/seq_label/\"\n",
    "model, stats = task.train(epochs, train_loader, valid_loader, freq, out_dir)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Testing\n",
    "f1_test = task.evaluate(test_loader, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
