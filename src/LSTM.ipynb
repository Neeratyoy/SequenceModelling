{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sCld3t9r8VMI",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as tdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVPi0mmSqqu7",
        "colab_type": "text"
      },
      "source": [
        "## Defining cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9dEFUgB948F",
        "colab": {}
      },
      "source": [
        "class RNNCell(nn.Module):\n",
        "    \"\"\"A vanilla RNN cell\n",
        "    \n",
        "    An RNN cell which takes two inputs: x and previous hidden state\n",
        "    and outputs the current hidden state and output.\n",
        "    \n",
        "    The RNN cell can be used as components for the following designs:\n",
        "    * One-to-one: https://stanford.edu/~shervine/images/rnn-one-to-one.png\n",
        "    * One-to-many: https://stanford.edu/~shervine/images/rnn-one-to-many.png\n",
        "    * Many-to-one: https://stanford.edu/~shervine/images/rnn-many-to-one.png\n",
        "    * Many-to-many\n",
        "        * Same: https://stanford.edu/~shervine/images/rnn-many-to-many-same.png\n",
        "        * Different: https://stanford.edu/~shervine/images/rnn-many-to-many-different.png\n",
        "    * Bidirectional RNNs: https://stanford.edu/~shervine/images/bidirectional-rnn.png\n",
        "    * Deep RNNs: https://stanford.edu/~shervine/images/deep-rnn.png\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    input_dim: Dimension of input data\n",
        "    output_dim: Dimension of outputs for each input\n",
        "    hidden_dim: Size of hidden state\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, xavier_init=False):\n",
        "        super().__init__()\n",
        "        self.weights_hidden = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "        self.weights_input = nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
        "        self.weights_output = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
        "        self.bias_state = nn.Parameter(torch.randn(hidden_dim))\n",
        "        self.bias_output = nn.Parameter(torch.randn(output_dim))\n",
        "        self.g1 = nn.Sigmoid()\n",
        "        self.g2 = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        # Design from: \n",
        "        #    https://stanford.edu/~shervine/images/description-block-rnn.png\n",
        "        #    https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
        "        \n",
        "        # W_{ax}x^t\n",
        "        input_actv = torch.matmul(x, self.weights_input)\n",
        "        # W_{aa}a^{t-1}\n",
        "        old_state_actv = torch.matmul(hidden, self.weights_hidden)\n",
        "        # updated hidden state (new state)\n",
        "        # g_1(W_{aa}a^{t-1} + W_{ax}x^t + b_a)\n",
        "        hidden = self.g1(old_state_actv + input_actv + self.bias_state)\n",
        "        \n",
        "        # W_{ya}a^t + b_y\n",
        "        output_actv = torch.matmul(hidden, self.weights_output)\n",
        "        # g_2(W_{ya}a^t + b_y)\n",
        "        output = self.g2(output_actv + self.bias_output)\n",
        "        \n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbc_6cidqqvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNNCell(512, 2, 256)\n",
        "inputs = torch.randn(3,5,512)\n",
        "hidden = torch.randn(1, 5, 256)\n",
        "o, h = rnn(inputs, hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq0E0CkQqqvD",
        "colab_type": "code",
        "outputId": "46d0b941-3144-428f-9e32-12dd7cb287a4",
        "colab": {}
      },
      "source": [
        "print(inputs.shape)\n",
        "print(o.shape)\n",
        "print(hidden.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5, 512])\n",
            "torch.Size([3, 5, 2])\n",
            "torch.Size([1, 5, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWS7G3zhqqvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Placeholder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiM9F8ZJqqvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    \"\"\"A vanilla LSTM cell       \n",
        "    \n",
        "    An LSTM cell which takes two inputs: x and previous hidden state\n",
        "    and outputs the current cell state and hidden state.\n",
        "    Conditionally, an output cell can also be learnt where the final\n",
        "    hidden state is reprojected to the required dimension.\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    input_dim: Dimension of input data\n",
        "    hidden_dim: Size of hidden state\n",
        "    output_dim: Dimension of outputs for each input\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=None, xavier_init=False):\n",
        "        super().__init__()\n",
        "        dim_size = input_dim + hidden_dim\n",
        "        if xavier_init:\n",
        "            n = (dim_size + hidden_dim) / 2\n",
        "            self.initialize_with_Xavier(n, dim_size, hidden_dim, output_dim)\n",
        "        else:\n",
        "            self.weights_forget = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
        "            self.bias_forget = nn.Parameter(torch.randn(hidden_dim))\n",
        "            self.weights_input = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
        "            self.bias_input = nn.Parameter(torch.randn(hidden_dim))\n",
        "            self.weights_candidate = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
        "            self.bias_candidate = nn.Parameter(torch.randn(hidden_dim))\n",
        "            self.weights_output = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
        "            self.bias_output = nn.Parameter(torch.randn(hidden_dim))\n",
        "            if output_dim is not None:\n",
        "                self.output_projection = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
        "                self.softmax = nn.Softmax(dim=2)\n",
        "        \n",
        "        self.g1 = nn.Sigmoid()\n",
        "        self.g2 = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x, hidden_state, old_state):\n",
        "        # Design and notations from:\n",
        "        #    https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "        \n",
        "        # h^{t-1}x^t\n",
        "        concat_inputs = torch.cat((x, hidden_state), dim=2)\n",
        "        \n",
        "        # Forget gate: $\\Gamma_f = \\sigma(W_f.[h_{t-1}, x_t] + b_f)$\n",
        "        # Determines what(or how much) to throw away from old state\n",
        "        forget_g = self.g1(torch.matmul(concat_inputs, self.weights_forget) + \n",
        "                           self.bias_forget)\n",
        "        \n",
        "        # Input gate: $\\Gamma_i = \\sigma(W_i.[h_{t-1}, x_t] + b_i)$\n",
        "        # Decides which part of input is to be added\n",
        "        input_g = self.g1(torch.matmul(concat_inputs, self.weights_input) + \n",
        "                          self.bias_input)\n",
        "        \n",
        "        # Current candidate: $\\widetilde{C_t} = tanh(W_C.[h_{t-1}, x_t] + b_C)$\n",
        "        # Potential candidate to update the state with\n",
        "        candidate_new = self.g2(torch.matmul(concat_inputs, self.weights_candidate) + \n",
        "                                self.bias_candidate)\n",
        "        \n",
        "        # New state: $C_t = f_t * C_{t-1} + i_t * \\widetilde{C_t}$\n",
        "        new_state = torch.mul(forget_g, old_state) + \\\n",
        "                    torch.mul(input_g, candidate_new)\n",
        "        \n",
        "        # Output gate: $\\Gamma_o = \\sigma(W_o.[h_{t-1}, x_t] + b_o)$\n",
        "        # Determines what to output based on current input and\n",
        "        #    previous hidden state\n",
        "        output_g = self.g1(torch.matmul(concat_inputs, self.weights_output) + \n",
        "                           self.bias_output)\n",
        "        # New hidden state/output: $h_t = \\Gamma_o * tanh(C_t)$\n",
        "        hidden_state = torch.mul(output_g, self.g2(new_state))\n",
        "        \n",
        "        output = None\n",
        "        if hasattr(self, 'output_projection') and hasattr(self, 'softmax'):\n",
        "            output = self.softmax(torch.matmul(hidden_state, self.output_projection))\n",
        "        \n",
        "        return new_state, hidden_state, output\n",
        "    \n",
        "    def initialize_with_Xavier(self, n_var, dim_size, hidden_dim, output_dim):\n",
        "        gauss = tdist.Normal(0, 1 / np.sqrt(n_var))\n",
        "        self.weights_forget = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
        "        self.bias_forget = nn.Parameter(gauss.sample((hidden_dim,)))\n",
        "        self.weights_input = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
        "        self.bias_input = nn.Parameter(gauss.sample((hidden_dim,)))\n",
        "        self.weights_candidate = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
        "        self.bias_candidate = nn.Parameter(gauss.sample((hidden_dim,)))\n",
        "        self.weights_output = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
        "        self.bias_output = nn.Parameter(gauss.sample((hidden_dim,)))\n",
        "        if output_dim is not None:\n",
        "            self.output_projection = nn.Parameter(gauss.sample((hidden_dim, output_dim)))\n",
        "            self.softmax = nn.Softmax(dim=2)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6YQla7dEw0u",
        "colab_type": "text"
      },
      "source": [
        "#### trials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABCxMwJmqqvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM without Output cell and Xavier initialization\n",
        "\n",
        "lstm = LSTMCell(512, 256)\n",
        "batch = 2\n",
        "inputs = torch.randn(3, batch, 512)\n",
        "hidden = torch.randn(1, batch, 256)\n",
        "state = torch.randn(256)\n",
        "new_state, h, _ = lstm(inputs[0].unsqueeze(0), hidden, state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOYrGtgpqqvQ",
        "colab_type": "code",
        "outputId": "6e3a4c1b-5085-4698-dbed-c9ff1a1db7a8",
        "colab": {}
      },
      "source": [
        "print(inputs[0].unsqueeze(0).shape)\n",
        "print(new_state.shape)\n",
        "print(h.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 512])\n",
            "torch.Size([1, 2, 256])\n",
            "torch.Size([1, 2, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A85qgJtqqqvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM with Output cell and Xavier initialization\n",
        "\n",
        "lstm = LSTMCell(512, 256, 2, True)\n",
        "batch = 5\n",
        "inputs = torch.randn(3, batch, 512)\n",
        "hidden = torch.randn(1, batch, 256)\n",
        "state = torch.randn(256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu0Ka1_RqqvY",
        "colab_type": "code",
        "outputId": "31ed15a5-a80d-436f-c6dd-b67c78e5dbbd",
        "colab": {}
      },
      "source": [
        "new_state, h, o = lstm(inputs[0].unsqueeze(0), hidden, state)\n",
        "print(o.shape)\n",
        "print(h.shape)\n",
        "print(new_state.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 2])\n",
            "torch.Size([1, 5, 256])\n",
            "torch.Size([1, 5, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMGyT6Bqqvb",
        "colab_type": "code",
        "outputId": "3bc7e806-b71b-476e-b797-02de9b5f9fd1",
        "colab": {}
      },
      "source": [
        "print(torch.argmax(o, dim=2))\n",
        "gt = (torch.ones(batch).long() - torch.argmax(o, dim=2)[0].long())\n",
        "print(gt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 0, 0, 0]])\n",
            "tensor([1, 0, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO4p9Z4Yqqvf",
        "colab_type": "code",
        "outputId": "a0b54391-de65-43fd-f59a-8eae2c6cbaab",
        "colab": {}
      },
      "source": [
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "loss = loss_criterion(o.view(batch, -1), gt)\n",
        "print(\"Loss: {}\".format(loss.item()))\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.7407665252685547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0uec2jyqqvi",
        "colab_type": "text"
      },
      "source": [
        "### Full LSTM Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WroLBaAwqqvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"Creates an LSTM network       \n",
        "    \n",
        "    An LSTM architecture which takes the number of LSTM cells to\n",
        "    arrange to create a history. Each cell is sequentially connected \n",
        "    and sequence of inputs are fed in each pass with a shift of one.\n",
        "    Till the last position in the sequence is read by the last LSTM\n",
        "    cell in the architecture sequence.\n",
        "    In case of a Bidirectional LSTM, the similar thing happens but \n",
        "    the input is read right to left. The output is obtained on the \n",
        "    last LSTM cell which reads the first position in the sequence.\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    ncell: Number of LSTM cells to arrange sequentially (or history length)\n",
        "    input_dim: Dimension of input data\n",
        "    hidden_dim: Size of hidden state\n",
        "    output_dim: Dimension of outputs for each input\n",
        "    bidirectional: True/False indicacting to create bidirectional LSTM\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, ncell, input_dim, hidden_dim, output_dim, \n",
        "                 bidirectional=False, xavier_init=True):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Creating architecture for Left-to-Right Sequence\n",
        "        archi = []\n",
        "        for i in range(ncell):\n",
        "            archi.append(LSTMCell(input_dim, hidden_dim, \n",
        "                                  None, xavier_init))\n",
        "        self.archi = nn.ModuleList(archi)\n",
        "        self.final_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=2) # use nn.LogSoftmax?\n",
        "        \n",
        "        if bidirectional:\n",
        "            # Creating architecture for Right-to-Left Sequence\n",
        "            archi_rev = []\n",
        "            for i in range(ncell):\n",
        "                archi_rev.append(LSTMCell(input_dim, hidden_dim, \n",
        "                                          None, xavier_init))\n",
        "            self.archi_rev = nn.ModuleList(archi_rev)\n",
        "            self.final_layer = nn.Linear(2 * hidden_dim, output_dim)               \n",
        "        \n",
        "    def forward(self, x, hidden_state, cell_state):\n",
        "        history = len(self.archi)\n",
        "        seq_length = x.shape[0]\n",
        "        hidden_state_rev = hidden_state.clone()\n",
        "        cell_state_rev = cell_state.clone()\n",
        "        assert(seq_length >= history)\n",
        "        # Forward pass for Left-to-Right sequence\n",
        "        for i in range(seq_length - history + 1):\n",
        "            for j in range(history):\n",
        "                inputs = x[i+j].unsqueeze(0)\n",
        "                cell_state, hidden_state, _ = self.archi[j](inputs, \n",
        "                                                            hidden_state, \n",
        "                                                            cell_state)\n",
        "        if self.bidirectional:\n",
        "            # Forward pass for Right-to-Left sequence\n",
        "            for i in range(seq_length - history + 1):\n",
        "                for j in range(history):\n",
        "                    inputs = x[i+j].unsqueeze(0)\n",
        "                    cell_state_rev, \n",
        "                    hidden_state_rev,\n",
        "                    _ = self.archi_rev[history-j-1](inputs, hidden_state_rev, cell_state_rev)\n",
        "            hidden_state = torch.cat((hidden_state, hidden_state_rev), dim=-1)\n",
        "        \n",
        "        output = self.softmax(self.final_layer(hidden_state))\n",
        "        return cell_state, hidden_state, output\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3zx21dNqqvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history = 4\n",
        "# seq_length = 10\n",
        "# for i in range(seq_length - history + 1):\n",
        "#     for j in range(history):\n",
        "#         print(i+j, end=\" \")\n",
        "#     print()\n",
        "# print()\n",
        "# archi = [\"archi_1\", \"archi_2\", \"archi_3\", \"archi_4\"]\n",
        "# for i in range(0, seq_length - history + 1)[::-1]:\n",
        "#     for j in range(history)[::-1]:\n",
        "#         print(i+j, end=\" \")\n",
        "#         #print(\"{} {}\".format(history-j-1, i+j), end=\"\\t\")\n",
        "#     print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJTa8Pswqqvo",
        "colab_type": "text"
      },
      "source": [
        "### Training modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGWmIyeyqqvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data_loader, batch_size, optimizer, epochs, validate=False):\n",
        "    \"\"\"\n",
        "    Trains the model of class LSTM using data from data_loader passed as argument\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    model: object of class LSTM\n",
        "    data_loader: an object containing data and functions to sample batches\n",
        "        The sample() function of the object returns two tensors\n",
        "        x - which is a 3D tensor of [sequence, batch, dim]\n",
        "            Example-A batch of 10 sentences of 5 words each where each word has\n",
        "            an embedding vector of size 256, the 3D tensor shape will be [5, 10, 256]\n",
        "        y - a 1D tensor of length batch_size containing the classes as integers (torch.long())\n",
        "        The valid_data() function of the object returns a tuple of two tensors\n",
        "        x - which is a 3D tensor of [sequence, valid_data_len, dim]\n",
        "        y - a 1D tensor of length valid_data_len containing the classes as integers (torch.long())\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "    Trained model object of class LSTM, list containing loss progress\n",
        "    (, and list containing validation F1 score)\n",
        "    \n",
        "    \"\"\"\n",
        "    loss_list = []\n",
        "    val_acc = []\n",
        "    for i in range(epochs):\n",
        "        loss_tracker = []\n",
        "        for i, x, y in enumerate(data_loader.sample_batch(batch_size)):\n",
        "            hidden_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
        "            cell_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
        "            cell_state, hidden_state, output = lstm(x=x_train, \n",
        "                                                    hidden_state=hidden_state, \n",
        "                                                    cell_state=cell_state)\n",
        "            loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
        "            print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, int(x_size/batch_size), \n",
        "                                                               loss.item()), end='\\r')\n",
        "            loss_tracker.append(loss.item())\n",
        "            # backward pass for the batch (+ weight updates)\n",
        "            loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print()\n",
        "        print(\"Epoch #{}: Average loss is {}\".format(i+1, loss_list[-1]))\n",
        "        if validate:\n",
        "            f1 = evaluate(model, data_loader.valid_data(), verbose=True)\n",
        "            val_acc.append(f1)\n",
        "            print(\"Epoch #{}: Validation accuracy is {}\".format(i+1, val_acc[-1]))\n",
        "        loss_list.append(np.mean(loss_tracker))\n",
        "        loss_tracker = []\n",
        "        val_acc = []\n",
        "        print()\n",
        "    if validate:\n",
        "        return model, loss_list, val_acc\n",
        "    return model, loss_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqTTKlHPqqvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "def evaluate(model, test_data, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluates the model of class LSTM using test data passed\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    model: object of class LSTM\n",
        "    test_data: a tuple containing (x, y) \n",
        "        x - a 3D tensor of [sequence, len_test_data, dim]\n",
        "            Example-A batch of 10 sentences of 5 words each where each word has\n",
        "            an embedding vector of size 256, the 3D tensor shape will be [5, 10, 256]\n",
        "        y - a 1D tensor containing the classes as integers (torch.long())\n",
        "    verbose: prints the confusion matrix and F-score\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "    F-score (float)\n",
        "    \n",
        "    \"\"\"\n",
        "    x, y = test_data\n",
        "    batch_size = x.shape[1]\n",
        "    hidden_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
        "    cell_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, _, output = lstm(x=x_train, hidden_state=hidden_state, cell_state=cell_state)   \n",
        "    if verbose:\n",
        "        print(confusion_matrix(y, torch.argmax(output, 2)[0].numpy()))\n",
        "        print(f1_score(y, torch.argmax(output, 2)[0].numpy()))\n",
        "    return f1_score(y, torch.argmax(output, 2)[0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypdjA0QDqqvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbc9clL7qqvy",
        "colab_type": "text"
      },
      "source": [
        "## Creating datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNNXLFcTqqvz",
        "colab_type": "text"
      },
      "source": [
        "### Counting sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRzwjP5fqqv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randint(0, 2, (1000,100)).float()\n",
        "y = torch.sum(x, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29sgcExqqv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 1\n",
        "seq_length = 100\n",
        "hidden_dim = 50\n",
        "output_dim = 100\n",
        "lstm = LSTMCell(input_dim=input_dim, hidden_dim=hidden_dim, \n",
        "                output_dim=output_dim, xavier_init=True)\n",
        "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
        "loss_criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6JDJFZQqqv9",
        "colab_type": "code",
        "outputId": "a29ef5ce-538b-4b77-9fba-37514b9ee27d",
        "colab": {}
      },
      "source": [
        "# Training Loop\n",
        "batch_size = 10\n",
        "# for i in range(10):\n",
        "for j in range(int(1000/batch_size)):\n",
        "    start = j * batch_size\n",
        "    end = min((j+1) * batch_size, 1000)\n",
        "    x_train = torch.tensor([]).float()\n",
        "    y_train = y[start:end]\n",
        "    # creating the 3D tensor for the batch: seq x batch x dim\n",
        "    for data in x[start:end]:\n",
        "        x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
        "    # initializing states\n",
        "    hidden_state = torch.zeros(1, batch_size, hidden_dim)\n",
        "    cell_state = torch.zeros(1, batch_size, hidden_dim)\n",
        "    # forward pass for the batch\n",
        "    for i, seq_i in enumerate(x_train):        \n",
        "        cell_state, hidden_state, output = lstm(x=seq_i.unsqueeze(0), \n",
        "                                                hidden_state=hidden_state, \n",
        "                                                old_state=cell_state)\n",
        "    loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
        "    print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, int(1000/batch_size), \n",
        "                                                       loss.item()), end='\\r')\n",
        "    # backward pass for the batch (+ weight updates)\n",
        "    loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #100: Batch 100/100 -- Loss = 4.553095817565918\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8EkfNE9qqwD",
        "colab_type": "code",
        "outputId": "0596267f-40fd-419f-e66a-0c80f9f2caea",
        "colab": {}
      },
      "source": [
        "print(torch.argmax(output, dim=2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[51, 51, 51, 51, 51, 51, 51, 51, 51, 51]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDTcfmzZqqwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QANKTACqqwI",
        "colab_type": "text"
      },
      "source": [
        "### Classifying product as positive/negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBZzS8BqqwJ",
        "colab_type": "code",
        "outputId": "fe5f413c-f270-4105-e7a5-341aa3e3bf12",
        "colab": {}
      },
      "source": [
        "input_dim = 1\n",
        "seq_length = 10\n",
        "hidden_dim = 50\n",
        "output_dim = 2\n",
        "x_size = 1000\n",
        "\n",
        "x = torch.randn((x_size, seq_length)).float()\n",
        "y = torch.prod(x, 1)\n",
        "y[y >= 0] = 1\n",
        "y[y < 0] = 0\n",
        "# print(torch.histc(y, bins=2))\n",
        "\n",
        "lstm = LSTM(ncell=4, input_dim=input_dim, hidden_dim=hidden_dim, \n",
        "            output_dim=output_dim, bidirectional=False, xavier_init=True)\n",
        "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "print(lstm.count_parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRmcrm7TqqwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing of batch creating and forward pass\n",
        "\n",
        "# x_train = x\n",
        "# y_train = y\n",
        "\n",
        "# batch_size = 4\n",
        "# j = 0\n",
        "# start = j * batch_size\n",
        "# end = min((j+1) * batch_size, 1000)\n",
        "\n",
        "# x_train = torch.tensor([]).float()\n",
        "# y_train = y[start:end]\n",
        "# # creating the 3D tensor for the batch: seq x batch x dim\n",
        "# for data in x[start:end]:\n",
        "#     x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
        "\n",
        "# # initializing states\n",
        "# hidden_state = torch.zeros(1, batch_size, hidden_dim)\n",
        "# cell_state = torch.zeros(1, batch_size, hidden_dim)\n",
        "\n",
        "# print(x_train.shape, y_train.shape)\n",
        "# print(hidden_state.shape, cell_state.shape)\n",
        "\n",
        "# s,h,o = lstm(x_train, hidden_state, cell_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goHNu-2FqqwO",
        "colab_type": "code",
        "outputId": "ac0f92d5-253c-44f4-b6ae-f353d85ecdb4",
        "colab": {}
      },
      "source": [
        "# TRAINING LOOP\n",
        "\n",
        "# Each batch passed to LSTM() is expected to be a 3D Tensor where\n",
        "# the dimensions describe [sequence_size, batch_size, dim_size]\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "for i in range(10):\n",
        "    loss_tracker = []\n",
        "    for j in range(int(x_size/batch_size)):\n",
        "        start = j * batch_size\n",
        "        end = min((j+1) * batch_size, x_size)\n",
        "        x_train = torch.tensor([]).float()\n",
        "        y_train = y[start:end]\n",
        "        # creating the 3D tensor for the batch: seq x batch x dim\n",
        "        for data in x[start:end]:\n",
        "            x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
        "        # initializing states\n",
        "        hidden_state = torch.zeros(1, batch_size, hidden_dim)\n",
        "        cell_state = torch.zeros(1, batch_size, hidden_dim)\n",
        "        cell_state, hidden_state, output = lstm(x=x_train, \n",
        "                                                hidden_state=hidden_state, \n",
        "                                                cell_state=cell_state)\n",
        "        loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
        "        print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, int(x_size/batch_size), \n",
        "                                                           loss.item()), end='\\r')\n",
        "        loss_tracker.append(loss.item())\n",
        "        # backward pass for the batch (+ weight updates)\n",
        "        loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"\\nEpoch #{}: Average loss is {}\".format(i+1, np.mean(loss_tracker)))\n",
        "    loss_tracker = []\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1: Batch 250/250 -- Loss = 0.7007292509078984\n",
            "Epoch #1: Average loss is 0.694420922756195\n",
            "\n",
            "Epoch #2: Batch 250/250 -- Loss = 0.7049501538276672\n",
            "Epoch #2: Average loss is 0.6934642205238343\n",
            "\n",
            "Epoch #3: Batch 250/250 -- Loss = 0.7077709436416626\n",
            "Epoch #3: Average loss is 0.6930509006977081\n",
            "\n",
            "Epoch #4: Batch 250/250 -- Loss = 0.7110102176666261\n",
            "Epoch #4: Average loss is 0.6925781621932984\n",
            "\n",
            "Epoch #5: Batch 250/250 -- Loss = 0.7147052884101868\n",
            "Epoch #5: Average loss is 0.6917244708538055\n",
            "\n",
            "Epoch #6: Batch 250/250 -- Loss = 0.7159007191658026\n",
            "Epoch #6: Average loss is 0.6891825330257416\n",
            "\n",
            "Epoch #7: Batch 250/250 -- Loss = 0.6925382018089294\n",
            "Epoch #7: Average loss is 0.6872168350219726\n",
            "\n",
            "Epoch #8: Batch 250/250 -- Loss = 0.6890038251876831\n",
            "Epoch #8: Average loss is 0.6850257174968719\n",
            "\n",
            "Epoch #9: Batch 250/250 -- Loss = 0.6967091560363772\n",
            "Epoch #9: Average loss is 0.683526422739029\n",
            "\n",
            "Epoch #10: Batch 250/250 -- Loss = 0.6947144865989685\n",
            "Epoch #10: Average loss is 0.6801455161571502\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IGy0vLBqqwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validate(lstm, x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFZMfZeaqqwa",
        "colab_type": "code",
        "outputId": "7032ef42-38aa-4445-bafc-c754ad1a4975",
        "colab": {}
      },
      "source": [
        "# Validating on entire generated data\n",
        "\n",
        "x_train = torch.tensor([]).float()\n",
        "y_train = y\n",
        "# creating the 3D tensor for the batch: seq x batch x dim\n",
        "for data in x:\n",
        "    x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "evaluate(lstm, (x_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 1000, 1])\n",
            "torch.Size([1000])\n",
            "[[291 198]\n",
            " [213 298]]\n",
            "0.5918570009930486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5918570009930486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZncpfjaBqqwg",
        "colab_type": "code",
        "outputId": "886851c6-d611-4660-ee75-29bd03b8734c",
        "colab": {}
      },
      "source": [
        "# Validating on newly generated test data\n",
        "\n",
        "x_test = torch.randn((x_size, seq_length)).float()\n",
        "y_test = torch.prod(x_test, 1)\n",
        "y_test[y_test >= 0] = 1\n",
        "y_test[y_test < 0] = 0\n",
        "\n",
        "x_train = torch.tensor([]).float()\n",
        "y_train = y_test\n",
        "# creating the 3D tensor for the batch: seq x batch x dim\n",
        "for data in x_test:\n",
        "    x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
        "print(x_train.shape)\n",
        "print(y.shape)\n",
        "\n",
        "evaluate(lstm, (x_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 1000, 1])\n",
            "torch.Size([1000])\n",
            "[[263 251]\n",
            " [251 235]]\n",
            "0.4835390946502058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4835390946502058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8odT33rcqqwq",
        "colab_type": "text"
      },
      "source": [
        "### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJGlEZ0Uqqwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
        "#   author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
        "#   title     = {Learning Word Vectors for Sentiment Analysis},\n",
        "#   booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
        "#   month     = {June},\n",
        "#   year      = {2011},\n",
        "#   address   = {Portland, Oregon, USA},\n",
        "#   publisher = {Association for Computational Linguistics},\n",
        "#   pages     = {142--150},\n",
        "#   url       = {http://www.aclweb.org/anthology/P11-1015}\n",
        "# }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDWnM_ydqqwt",
        "colab_type": "code",
        "outputId": "8a1c8821-1e21-4d70-8058-57389b614b12",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='../img/sentiment.png') \n",
        "# source:(https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtkAAAI+CAIAAADmboHzAAAx+ElEQVR42uzdDXjU9YEv+l8kaMCglBWIQDXaBLPIVk83thjikrqsJmCP9M3rbXcv2rNP4OmjTc729Fk9l7tr93JX9+k9FRZvt+S6q9497e3S0xZPgcSe1uIa0rTNtmiRRRJtfMMgLEaJgBLNeeYlyUySCS9CMvPP5/P4tJn3md93fjPf+f3/M+T39fUFALJA9/G3vtn52FNvdu45/PLON347AUegbNq8ssK5V11QvPqy6qLzpntKTBB5ughANmh67de3/3pD17HXDUUIYfrk8+9f+IXbLrneUOgiAIxREan52V8ZhyH+7qrVq4tvNA66CABnV/fxt3738TutiAxXMOncf71+Q/HUWYYi2s4xBADja91zP1RERnTs3Xe++uw/GQddBICz66k3Ow1CJhNzH15dBABvt9liT88rBkEXAeDs6jzymkHI5Ni77xgEXQQAQBcBAHQRAABdBADQRQAAdBEAQBcBAFJ1rF+cl5e3eH3HGb9I46q8vLy8VY26CABEUuKtPkU2vOtn6h/vv5ekPtxTak66CAAnY9oft9/8g76B/65ZcqILFK+7/gd9N99Ve5q39z4vng069u4ackzDsvF+kz7Zu37KSzGNq/KWNQwebKlfOZ4PVBcBiJqKsgf6rv90yb51eY9+Mu/RTy5+9pUwp77v+j+uMDQnoXZbX1z7uth4tWzaOr5lpGZj7M5srDnZ40+yimyOFZGKde2pj1UXAeAMWbLmirkh/HzVL59Ifubd83+vPxzCtEW3TOtfL0kukyQWMx5YN6143fX3100LIXxs480/iLeWJdtu/kF72ZL4GWL/bZsbv8TJXjwC2ne3xP5v4fyS5LrD4vXrV6VtuknfpjNsc8nWVcO3gKRfZNhCxggXOeE2mth9K61viS9ulCaOGnqZ4csmpQsqUldDSup27KgrSTv30Mc00oMdcVhOZ9OPLgIQLXOvjX1W3vezlBX4zt2HYycsn1uc4TKd9Y//x1hfCT9f9egn8x7/r/F34VByRf3yff8x79HYSTXlD6ybFk7p4jmqYVnifTSxCaN2xcDKQ0t9/eCYxt6GUzdyxC+X2hhSz91SX5o4KbEckXKetC0jI17ktNZSVtTG7tDm/s70tVhVqbhlecnAOUrq1tSGgfqSXlsSxSb5mOKnjPpg04Zl2Kafk3sUughA9O06fFr/2u3h763c0xlC56Z9r4Qwd/4FE2zUKta1p28ESW6+2VgTOrZuaknZntO3Lf7OPvDun7r9I3HSrr0dAxtWUo5v2d0eRr/ICZXU7UhuY0lcPnaPk2VkbbzpJApQ7Zq6krS+snHghhL9K14aErWl/3ElTz7Rgx0cluRNbUs5X+qY6CIAE9jCaXNDCO2HO9/PlZROK54IYzXwltu3I/3dO1QsKO3/M7EBZ3DRJPHmP8oiRbJ0pGz/SF9oyHyR01sZ+UqsnbRs2trRsX5tQ/r6ztBCkmwyyeaSuhhUszHebE7wYAeHJbn3b/rKUrAuAjDhvPly7C1lzrUp7xfFC6aFEF7Z++b7uuL3WWUiaPAz/5CNL6mSu4nG3rAbV5XWt/SvfmzL/J2jlIucrpLlt8TLyNe+tqklhIp1X0mrIrFKNLjxpGT+wvQL96/IdKxfPHiuk3iwySsabHMnu3utLgIQLYf/69f2hRA+9pWy5DJGRdl/qpsWwr7v1h8O4fBL7QNNZe6tdYO7gCT2KZm3IHWnkGmffiR2JcmdYTe/cooXj7DEusPACkBiCSDtHT+5J0b/SYM7a/SfMHzVIONFTtLAvqthYI+QloaGlgxXNXDfk7cX34iTXJFJ3pHkniMnfrDpCzop13ySv4CiiwBE7uP6Lz+5+NlXSq64P/EVmB1XzG1s+2Re8ms1Tyxr+3nyCy9XvLzq2cH9SBr2fK8jzK27PuWLMId/3j7n/r6b62tCaGy7r+FULx5lJXU70r8IW7stbZtOxbptgyfXbkucVLMxZTGktnbIwsiIFznZe7Om/8oGV1MGNqUM3VUkfu/TlmUG94yp2Zi6YFO7LX7siR5sShnZONp6T0Z5fX195i3AOMp79JPZd6eWbLu5vubw9xZnwZdi+m7+gSfJ6Uh+J6Z222n/DMlYsS4CAFGsIsmvv6yoyfq7qosAgCoynmyjARjvF+Js3EaTRWyjiTzrIgCALgIA6CIAALoIAKCLADCGpk8+3yCgiwAwbq6+8DKDkEnx1FkGQRcBQBcxOOgiANG1qvjGgknnGocR1V1+k0HQRQA4u8oK5/7lFf+LcRhu9WXVVRctNA66CABn3V2ln7pr/qeNQ6rbLrn+/oVfMA4Tgd+AB8gWra/v/eqz/7Tzjd92HXt9wg7C9MnnX33hZX9e+qnqWf/OU0IXAWAi6unpKSwsNA6MGdtoABjU29v74IMP9vT0GAp0EQDGwfbt27u7u7ds2WIo0EUAGGvd3d1tbW0hhIMHD3Z2dhoQdBEAxlRTU1NlZWUIYenSpU1NTQYEXQSAsdPZ2dnV1bVo0aIQQllZ2fTp01tbWw0LuggAY6Spqam6ujo/Pz9xsKqqqrW11U6s6CIAjIXW1taCgoKysrKBY4qKisrKypqbmw0OuggAZ1dPT09zc3N1dfWQ46uqqnbt2tXV1WWI0EUAOIv27NmzcOHCoqKiIccXFBRUVlbu3LnTEHFW5RsCgAmuvLw800mJXVnhrLIuAgDoIgCALgIAoIsAALoIAIAuAgDoIgAAuggAoIsAAOgiAIAuAgCgiwAAuggAgC4CAOgiAAC6CACgiwAA6CIAgC4CAKCLAAC6CACALgIA6CIAALoIAKCLAADoIgCALgIA6CIAALoIAKCLAADoIgCALgIAoIsAALoIAIAuAgDoIgAAuggAoIsAAOgiAIAuAgCgiwAAuggAgC4CAOgiAAC6CACgiwCQY6qqqgwCYymvr6/PKAAA48W6CAAwnvIH/jry1rGfbvvFS7/tevWlAy8+/+oEHIuL5828+IMzP3hZ0ceXffTCDxRG7NHJNxr5ytE8lS/Rk9xG85t/af/7+7/3xus9RiSEMPX8gv+1dlnl0o9E5hHJNxr5ytE8lS+RNOmee+75zb+0f/0vHnn72DuGI+H48d5ft/7rhR8ovKx0bjRe4OQbgXzlaJ7Kl6g658hbx/7+/u8ZiOH+//9328H9r+f6o5BvNPKVo3kqX6LcRX60ucWS4Mit/J3eR7/901x/FPKNRr5yNE/lS5S7yEu/7TIKmURg3zH5RiNfOZqn8iXKXUTMo3j15QOmsXzlKEf5ypez20VsihvF8Xd6c/0hyDca+crRPJUvUe4ihgAA0EUAAF0EAEAXAQB0EQAAXQQA0EUAAHK3i3z4zq1rH9r6uWvH9Mpnf3rj2oe21n36cuGebfKVoxzlK1/OgPwzdk2X/9G9G5YUxf/s+u6Gux/eb3AjRb5yRL6Q1V0kMQFefuIvVv2Pl2KN+A+vffjbPwtPb1j+9Fm752f1ypGvHOUoX/mSW12keGasib+8/6XYgf3fW/Xt+LGzP73xzpvm7W5Y/u2fhRCu/9xDX14weJGXn/iLvwlf3LAkfHdT27W33DQvdtyv/suatsVraxcl/97weOKsH75z6y0fSV7swJY713/v+cxX/vLuX0n1jJOvHOUoX/ly1pyh/UU6D3SFEBbd8tDGP/pgpsL+5QVd391w+/INW14OIexuiDX3mKLPfjz8zZrb73yiK4SPfHntv39pw+3LN/0q9ndiG2RiAuxuWL4mftmZN20Yvg3yw3d+eUFsUi1fc/s/hY/ME+uZJl85ylG+8iXbu8jz/+Pu+JM4zFvyV1vXjjATYoX9QNs/7w9h/y9+lv6PLr68+xfPh/D8023xufHfH94fwtNtrSGEmXMuD+H6hR8Joeu7P/lZvOnHLzuz/A9mp11D4jw/ezo2qR7/SXyOcUbJV45ylK98yfYukpgGy5O1Osxb8sXbZg8r7Inn7uyPXjszvHzgJJ+oH7xkZurBl148cMLzcLZe5uQrRznKV75kdRcZmAn/ZXcIoeiDs4efWPTZOx/aeudN8w5s+ZvkwuAJDXnSJ57u+17cf8KJwdl6pZOvHOUoX/mShV3kg7fV3dtfwK9dvCCE8KsdaTtXf/APFhSFA1vuXHP78jW3L0/s9HRyHt/1qxCKrv1wfLEx3uXD7rbHw7Cy33+e6//wJtspzzT5ylGO8pUvZ8+Z+R7NSw9/p23jnQ99NnkwZdfrgTP89FefveWmDWtvSh7Rv9/1iT294c7Z925Y8ldbl2S8YOxDwMyHvhw/z8tPNHz3QO1nJXsmyVeOcpSvfDl78m5b9r+Pwc1c+3+srV3U/zWwxJfgWzfd/n/mwBfTH9q6NqcDvn35GvlGIF85mqfyJcLG5t+jmT1nXgjhwL7EkmDiS/BEh3zliHzh9OWPya3s/96qTXO23lK7dW1t4ogcKePIV47GRb4QjS7ip4IjT75yRL5wms4xBACALgIA6CIAALoIADCxusjU8wuMQoTJV47IF7K9i1zyoYuNQiYXzf5Arj8E+UYjXzmap/Il0l3kcnMg8wtE7g+OfKMxOHI0T+VLlLtIVc01k8/NNxAj+qObr831hyDfaOQrR/NUvkS5i1w8b+bNn7veQAz38WUfLfu9y3L9Ucg3GvnK0TyVLxE26Z577pl/5aW9x3vbd79gOAZULv3IH6++adKkKHzPSL7RyFeO5ql8iaq8vr6+xF/P7Xnp0W8//uLzr77xes+EHY6p5xdc8qGLl33mD37v90sj9tDkG4185Wieypcod5Es0dPTU1hYKJiokq8ckS8MkV1rX729vQ8++GBPT49gIkm+ckS+kO1dZPv27d3d3Vu2bBFMJMlXjsgXsrqLdHd3t7W1hRAOHjzY2dkpm4iRrxyRL2R7F2lqaqqsrAwhLF26tKmpSTYRI185Il/I6i7S2dnZ1dW1aNGiEEJZWdn06dNbW1vFExnylSPyhWzvIk1NTdXV1fn5yV8erKqqam1ttfNUlD5syVeOyBeyt4u0trYWFBSUlZUNHFNUVFRWVtbc3CyhCJCvHJEvZHUX6enpaW5urq6uHnJ8VVXVrl27urq6hJTT5CtH5AvZ3kX27NmzcOHCoqKiIccXFBRUVlbu3LlTSDlNvnJEvjC68f+XIcvLyzOdlNiFipwmXzkiXxidf3MIANBFAABdBABAFwEAdBEAAF0EANBFAAB0EQBAFwEA0EUAAF0EAEAXAQB0EQAAXQQA0EUAAHQRAEAXAQDQRQAAXQQAQBcBAHQRAABdBADQRQAAdBEAQBcBANBFAABdBADQRQAAdBEAQBcBANBFAABdBABAFwEAdBEAAF0EANBFAAB0EQBAFwEA0EUAAF0EAEAXAQB0EQAAXQQA0EUAAHQRAEAXAQDQRQAAXQQAQBcBAHQRAABdBADQRQAAXQQAQBcBACam/ME/e7vDvm+Gt54KR/aEnp3jdYeqLqsKT+QJJqrkK0fkm42mlsX+O/+qMGd1OLfIc2CM5fX19cX+/1BTePb28E6XEQFgAn9Cnx4+dH8ous1IjHkXOdQUflNjLAAgpvTvwpzVhmEMu8jx18Mvf9eKCAAknVMQrvnXUFBsJMZovMPL6xQRABj03rHwwlcNwxh2kbeeMgoAkGb8vsMxIbuI4QaAIY7sMQZj2EWOdRoFAEjz3jFjMIZdBABAFwEAdBEAAF0EANBFAAB0EQBAFwEA0EXgjKoI1/SFJduG/R3tx9sepooe0EXg7L7dpvw38yzcyMxtg9d/zbosKFKnrXZcHwJAmnxDQIQ0hieWna3rLm4Pl5aEA6vC7obkweJNobPFoAO8T9ZFiK6p62Kf/hfUxg+ccDvFkDMMWTmojRWR0JgsIiGEztL+IlKbsh6TfvHi2sHVmpmjHz9kaSflfiYeRfLMFeGaHfGTalLuXoYLDqziXLPmpIYrddWnuGLUARnpFhP3szhxb7d59gG6CJyqlnCgI4SSMKsi/sa8Iva/Bzb1v08nDm4edqnasGRjfD0mLzyxOBwpCdektIFLN4bOvPDEqtjfC1Lenkc4PlEyEteTFw6UhGu2Ja//mrpwZH38+FWh+Jbwy8XhSEje4i/rR7vggprkBTvDifcUmbouFO/tfxQhXLom84BkusXEQ6sLu/PO4uoUoItAdqsZYXng5HV+Lf6me0v8f2tC6Aiv9W+COb80w1pC/B36hbUjtJkQYlXgQAihIf6/pYN3afjxU2+J/e9A1znQGHssM4dUooZ4+RjSIU7iggfWxuvLqI7U9195S3ir/16NOCCZbjHtoQGcAvuLECXp+4ucch1pCAc2hpnLw9RN4fwQjmwdfAt/qz1WMoYb0lEyne2Ezp8ff7/fGF9lGXb9b7Wc5gVPyYLhO/yONCAzM9xicgR2exYCugi8Dwcaw8yacOUjsR7zwqaUt9i98U//K2Jvz6OUj8HqsPB0bv2FxUN3hj0/8b8V4UDLqV0wnGIrKm6PFZHdeeFAeinJNCDDb9EXhoHTZRsN0XUk/hk9sbVi5pqTerNMbM6YWpK2gSaxCeNAfBtQck/Y+J6exRXJTRWJrRihIsyM7996GhspEtdz6bCdTF/bmnL9tfFdR1O2oYxywVh5Gti8cnKPPXTEr7k2bXVk+IBkukUAXQSGaQi7G5M7kRTvDS90nMRFEvt8pG+gSdidF7uGmRv7v2myN74w0BB+uT5MrYsfmdijc9lp3tUnVqXs79L/VZQj9SnX/5XwTHyXjs71IZSEaxLfasl8wd2NyQtmeuzJq43vXvPa15LXuWRjfBeQUQYkwy0CnK68vu0GAVLM3BYW1Iy01cOAGBAmmCV9xsC6CIzLW29NGH13UQMCoIvA2TJ1XXxviUZfTDUgwJixjQYARmIbzVixLgIA6CIAgC4CAKCLAAATrYvkTzcKAMD4dZHCq40CAKQpKDYGuggAjB9vjmPaRS5eFc4pMBAAMGhunTEYwy4ytSxc+pcGAgCS5qwO06sMwxh2kRDCJXfF/gMAim4LH7rfMIylvL6+/t+4fbM1vPDV0LMzvNNlXACYWPKnh8Krwwf/PMyoNhjj10WyQ09PT2FhoWCiSr5yRLgwRHb91llvb++DDz7Y09MjmEiSrxzJiXC7u7sNBRO3i2zfvr27u3vLli2CiST5ypEs19ra2t3d3dTUZCiYoF2ku7u7ra0thHDw4MHOzk7ZRIx85UiW6+npaW5uzs/Pf/nll/fs2WNAmIhdpKmpqbKyMoSwdOlSrTx65CtHslxzc/PVV19dWFiYCLe3t9eYMLG6SGdnZ1dX16JFi0IIZWVl06dPb21tFU9kyFeOZLmurq6dO3cmimZxcfG8efOam5sNCxOrizQ1NVVXV+fn5ycOVlVVtba22jkuSh+m5StHstn27durqqoKCwunT5/e3d1dXV3d2tp68OBBI8NE6SKtra0FBQVlZWUDxxQVFZWVlWnl0SBfOZLlOjo6Dh48WF5ePnBMYWFhVVWVzXBMlC6S2Fuqunrob8tUVVXt2rWrq8sPr+U2+cqR7PfjH/946dKlAyteCeXl5T09PXZiZUJ0kT179ixcuLCoqGjI8QUFBZWVlTt37hRSTpOvHMlynZ2dQ1a8EvLz86urq4XLGMgf93uQuio4RGIXOXKafOVIlisqKrrttttGPKk4zhBxtp1jCAAmsoKCAoOALgIA6CIAALoIAKCLADBxJX7rzDigiwAAuggAgC4CAOgiAAC6CACgiwAA6CIAgC4CAKCLAHBm+a0zdBEAQBcBANBFAABdBABAFwEAdBEAiLSO9Yvz8vLyVjWelasdavH6jtC46mzcoC4CABO5y8Q6hvujiwBA1iip29EXt602drB2W+LQjrqSULMx9tfGmgk/RroIAIP81tkQie0oqVtWhh2/eH1Hx/rFeaX1LSGElvrSk97ukrqNJv73qsbBqx3ciDNkQ07qPTrV+zPiZeMLKIvXd6Tfti4CAFlRRTY3pB5sqV8Zf5tuXLWs4SzcWsOywauNlYjBQw3LBipL6i231JfGjz+p+5PhsgOH0m57TPdi0UUAIJPEZpSUjSwtu9sHTqxY157c3FJSt6OvfV3FwHGnvd0lsQknsTkn/dCuvR391ah/O0/8hIbNjSd3f0a/bP+F+28u9SRdBADGT8rXYFIWFWo2xt6xE9s/zuQXYWpXxEtM6YJEjfhKyqH4ndm7K7FIMvQOncT9yXjZYfVrRe1YD7IuAgAZNK4qrW/pXzHYVjtswSS+9tCwdqx2ryiZvzCkrG0kJNZgTnh/RrnskMccXz+pWFCqiwDAeOhfOBhcX0guN6QsJQwsliT2D011KvuunrrEmsXgXUze1EndnwyXTT9j/8OsuGV5iS4CAOMuse0jqba2Nn2NIXn0th11JfEv767pP/3sLSrUbExfnjmV+zPyZftV1NZWDL2KsZLX19eXVbnfE+f5H1XylSNZbufOnZ2dnStWrDAUE0bH+sWJLVFjW0BSWBcBYFB+fn5vb69xQBcBYHwUFhb29PQYB8a0ARsCAJjASup29NWN6z2wLgIA6CIAgC4CAKCLAAC6CACALgIA6CIATCx+6wxdBIDx5LfO0EUAAF0EAEAXAQB0EQAAXQQA0EUAAHQRAEAXASC6/NYZuggA48lvnaGLAAC6CACALgIA6CIAALoIAKCLAADoIgBA1OQP/NV9vO+bne889eZ7ew6/u/ONd8frDlVdfG3eo2+My02XTZtUVnjOVRecs/qy84rOy4tY0vKNRr5yNE/HwMrCefJlLOX19fWFEJpe673910e7jr1nREII0yfn3b+w4LZLzo3MI5JvNPKVo3kqXyLbRZpe66352VvGYoi/u2rK6uJzo/ECJ98I5CtH81S+RLaLvP7Oe7/7eI8mPlzBpLx/vb6weGpu71LTfbxPvhHIV47mqXyJsHPWPfe2CTCiY+/2ffXZt3P9Ucg3GvnK0TyVL1HuIk+9aQJkNI77jp0p8o1GvnI0T+VLlLuImEexp+c901i+cpSjfOXL2e0inUfEnNGxd/ty/SHINxr5ytE8lS9R7iKGAADQRQAAXQQAQBcBAHQRAABdBADQRQAAdBFOYM+ePT09PcZBjsgXclH+2b+Jydtunlpz+Njix99uGThu2nnt1xeUxP/sePZw6SuTBw6meHf942/Pv35qTfxAY9sby14ZfvHjqx490jDhU+zq6tq8efOiRYsqKyvz8/PH9sblK0c5yle+5Ny6SOIZfPjY4kffyHv08NY5BbWH3y6N/f3G4mffTcyKvNjBnvrDycnQcTjUXHFeRf8VVMydXBI/koSqqqrVq1d3dXWtW7du165d43xv5CtH81S+kO1d5IJJsSp9+L14PX+v/vETFur3tu57N0ybfMu0ZMFfc8WksO/4VumlmD59+q233vqZz3xm+/btDz/8cFdX17jdFfnK0TyVL2R7F3nz3Y4QwpypfdcPVuzR7X7leEeYtHxu/N7OnVwTQuO+48Ibrri4+I477igrK3v44Ye3bNkyPhun5StHOcoXTkX+ONzm4bdLHw/t1xeUTCvYcXNBGLIJc+SLHN96uKBuzuSKPW8vnDM5hOObXwkLrhiLO7s9LhejbWtr27VrV8ncmo4LiuWbu/nK0TyVL7rIWZsGj76d3GA5reCRsuOle0b/Zyrf27Tv3borJt8yNyyfE8K+4w0hrBuTe1oVl1uhdnd3NzU1HTx4sLq6+u5nZso3R/OVo3kqX3SRMZkJbZP6yieXTJsUwgn+yeyWV453XFFQVz4phHfX7znu28gjOnbsWHNz886dOxctWvSZz3wmPz8/PPOGfOUoR/nKl2w2Ds+kirLC9rLk7dbOmRxOcqPj4eNbDyf/2GTP7ZG0tbU98MADPT09q1evHo8vDcpXjnKUL2Tzukhik2RIfFv9yNZrpvX1b2VM+8L6aBLLg5M69h1vkVu6zs7Opqam/Pz8W2+9dd68eeNwD+QrRznKF05XXtjcbRRG0Xfzhdl/J7ds2VJUVFReXj5CwI++IcRcyVeO5im5my+5sC7C2XTTTTcZBDkiX8hR9jwCAHQRAEAXAQDQRQAAXQQAQBcBACZEF5k+Oc8oRJh85Yh8Idu7yNUXTjIKmRRPzfl1I/lGI185mqfyRReZoCIwOPKNxuDI0TyVL1HuIquKzy2YZHlwZHWXn5vrD0G+0chXjuapfIlyFykrPOcvrzjPQAy3+rLzqi7K+d/Il2808pWjeSpfImzSPffcU/k7+W/35TX/W6/hGHDbJedu+PCU/Eh8UJFvNPKVo3kqX6Iqr6+vL/FX6+vvfvXZt3e+8W7Xsfcm7HBMn5x39YWT/rz0vOpZUWvi8o1GvnI0T+VLlLtIlujp6SksLBRMVMlXjsgXhsiu70r19vY++OCDPT09gokk+coR+UK2d5Ht27d3d3dv2bJFMJEkXzkiX8jqLtLd3d3W1hZCOHjwYGdnp2wiRr5yRL6Q7V2kqampsrIyhLB06dKmpibZRIx85Yh8Iau7SGdnZ1dX16JFi0IIZWVl06dPb21tFU9kyFeOyBeyvYs0NTVVV1fn5ye/wVVVVdXa2mrnqSh92JKvHJEvZG8XaW1tLSgoKCsrGzimqKiorKysublZQhEgXzkiX8jqLtLT09Pc3FxdXT3k+Kqqql27dnV1dQkpp8lXjsgXsr2L7NmzZ+HChUVFRUOOLygoqKys3Llzp5BymnzliHxhdOP/C7vl5eWZTkrsQkVOk68ckS+M7hxDAADoIgCALgIAoIsAALoIAIAuAgDoIgAAuggAoIsAAOgiAIAuAgCgiwAAuggAgC4CAOgiAAC6CACgiwAA6CIAgC4CAKCLAAC6CACALgIA6CIAALoIAKCLAADoIgCALgIA6CIAALoIAKCLAADoIgCALgIAoIsAALoIAIAuAgDoIgAAuggAoIsAAOgiAIAuAgCgiwAAuggAgC4CAOgiAAC6CACgiwAA6CIAgC4CAKCLAAC6CACALgIA6CIAgC4CADBu8vr6+hJ/HX275+e7Nr96sOPA6y/uO9g+Xnco/8jlvVOfH5ebnvmBS2d94JKLLyr52MIV06bOiFjS8o1GvnI0T+VLZLvIsy/8/L/95K8PHzlkREIIU84rvKnyS7//uzWReUTyjUa+cjRP5UskTbrnnnuefeHnD/3wP71z/KjhSOh9953dv32ycOqMebPKovECJ98I5CtH81S+RNU5R9/u+W8/+WsDMdyWJze8/mZXrj8K+UYjXzmap/Ilyl1kx1ObLAlmauU//uVDuf4o5BuNfOVonsqXKHeRVw92GIVMXj3QnvMPQb6RyFeO5ql8iXIX2WcOZPba6y/k+kOQbzTylaN5Kl+i3EVef/NVo5BJ77vv5PpDkG808pWjeSpfotxFDAEAoIsAALoIAIAuAgDoIgAAuggAoIsAAOR6F7nxT+548r7Prb74dC5beuPnnrzvjm/deNEox8ev/457rxbm+JCvHOUoX/nyvuSfylN5zZWpRzy39q7Gx4xghF6q5CtH5AtZ3UXiDn1r/be/mWU/EPjYPz5gKp4h8pWjHOUrX7K9iwxx0eo/u/XzoW3tM5ev+fiM2BHP/Oi633zoyVs/lPz7H/cOnvfDNU/WDT2+9MbP/UPigiG8+NPvfP6xg/E/59973w2V8SnX/EzqzY18fOJKmr/zwN074x8aZqbfn+RtDVw2eydz9pGvHOUoX/ly9p2B/UVmlf9v4UfX3fWdb70WwpU3PHn961+464G1z8T+Ttl2OOPzs5677q4HvvDTQ+HKGxLbFweeu9fd9cB133nuko/fmjj/jX9yQ2XsOfrAdXe1hStnDFxFpuOH3581s9quS7sPF63+sxsqX2v7QuIOhND8HRNAvnKUo3zlS052kRmfr7vjyfvuSN+P6dCTTx8M4eBPnok/vR5vbQ/hsd88F0K4ZPbgeb7141gvbn/6+RdDuGTWjBAu+sPY8/i57Tvjp+98rjmEyt+bH8L8qitDeO35n8Seo3v/If6UTRTqDMePtIAZv62U+zDj0lnhxWc62vvvAPKVoxzlK1+yxfvbX+Si07zVmTNKQ7h0VgjhQ2vuu2NN6kkXz7hkxItkOv7k7vYLr4XKK0tKHzsYPnz5JeHQk/slL185ylG+8iUnu8gZcuBQe+KP19q+8PXW9rTT5scK+/CLvHpo5ONP3qzyf7ivPLFB1MKgfJGjfOVLthir3zqb8fml80MINy4tvySE5t/sDWHv9mdiz8svXD1Cdw6zLv/Di2Pz4Qsfn3Gi40/CxSXXzYo99a+764Hr7nqgf88s5Isc5Stfcm9dZMbn6+74fPLv59be9cuTvdxrbWtfK3/yvhtCfJ/qu+PbJh/7xwfCn9yx5tY7nrw1ea74PtgHv/n1H1163w3xGzr0re+0XXJrefzETMefhFdb/79nytd8/NYnP556Q8KXrxzlKF/5kgXy/nxDZeQf5NU1T976of7vqsW//zbrpH8g6L47nszpx37XA9fJNwL5ytE8lS8RNhH+PZrS2R8IIby4P7EkOCO+rxbyRY7Il+yQPwEeY/tj3147K3UR0q8myxc5Il90kbHl94nlixyRL1nqHEMAAOgiAIAuAgCgiwAAE6uLTDmv0ChEmHzliHwh27vIxReVGoVMPnDBxbn+EOQbjXzlaJ7Klyh3kTkzzYGM5lxUkvMPQb6RyFeO5ql8iXIX+diV/z5/0rkGYkSLr/psrj8E+UYjXzmap/Ilyl1k5gcuXfrR2w3EcIsWrrh87r/L9Uch32jkK0fzVL5E2KR77rmneM6He9893vnq04ZjwO//7rKbl9Sfc86kCDwW+UYjXzmap/IlqvL6+voSf73Y9cxPfvnQvgPth48cmrDDMeW8wosvKl3ykc9fcenHIvbQ5BuNfOVonsqXKHeRLNHT01NY6OttkSVfOSJfGCK7fuust7f3wQcf7OnpEUwkyVeOyBeyvYts3769u7t7y5Ytgokk+coR+UJWd5Hu7u62trYQwsGDBzs7O2UTMfKVI/KFbO8iTU1NlZWVIYSlS5c2NTXJJmLkK0fkC1ndRTo7O7u6uhYtWhRCKCsrmz59emtrq3giQ75yRL6Q7V2kqampuro6Pz8/cbCqqqq1tdXOU1H6sCVfOSJfyN4u0traWlBQUFZWNnBMUVFRWVlZc3OzhCJAvnJEvpDVXaSnp6e5ubm6unrI8VVVVbt27erq6hJSTpOvHJEvZHsX2bNnz8KFC4uKioYcX1BQUFlZuXPnTiHlNPnKEfnC6PLH/R6Ul5dnOimxCxU5Tb5yRL4wunMMAQCgiwAAuggAgC4CAOgiAAC6CACgiwAA6CIAgC4CAKCLAAC6CACALgIA6CIAALoIAKCLAADoIgCALgIAoIsAALoIAIAuAgDoIgAAuggAoIsAAOgiAIAuAgCgiwAAuggAoIsAAOgiAIAuAgCgiwAAuggAgC4CAOgiAAC6CACgiwAA6CIAQGTk9fX1hRD6jh97q/nBdzqa3+n8xbtv7h/HO/SrqYs/cmTHuFWzKReee9lHzy257vzKPz1nyoWRiVm+0chXjuapfIlsFzn+8tOHHl7Zu3+v4Rgw6YLZH7jtkfNKr4vAY5FvNPKVo3kqXyLbRd575+hr/9dHev/tBWMxvJvPWvPrSRfMzvVPWvKNQL5yNE/lS5SD7tn+/5gAI3rv6BuHG/861x+FfKORrxzNU/kS5S7yTseTRiGTt9v/Odcfgnyjka8czVP5Euku8ttfGIVMIrDtVr7RyFeO5ql8iXIXee/oG0YhwuQrR+QL2d5FDAEAoIsAALoIAIAuAgDoIgAAYyR/PG+8/PtzV96YekTvj5bs/2Fb/ieemn1DSfKop774yoOPyCnXlF+w5olps0MIHYfvverNfStnbPjGlMFTHzt056eOjnxOQ5flwQ7O2aOPnH+ozYjkqIxTcsqfvjXjqsSRqVPSPB1R46q8ZQ2hdlvfxhqD8f6M67pI26deufP8xH+HnoodcfzVtlD+/dk3lPT+aEnyyKu+MaNcTDn3jvWfp83u6B3yb3s99cX+uAeKSIZzkp3m3Dv76kdjCd77t70hTFn5/SnGJKcNm5LxIvLYoeSRKZ1jQszTjvWL80aweH3HGNWavLy8VY26yPi+xl1wVQj7//bNtpB/8YcGjj6+v8PLRU5+5Fp5Y++Pvn70TJ6TLLDv7v2JRcp9e44bjSh2zQuuCr0/+uuj5ikTs4vk//6y/BB6n/pebwi9P4w94/NveGLGJ+79nRtKEgWFHJL/iT+bEh5784fPnMFzkl3Kb54S+1T9qDenaHWRK/JDCFf9/dwNb8X+W3Nv/sSapyV1O/rittXGDtZuSxzaUVcyFrdeszF2YxN3W082dJGVF9xQEkLH0X9JlI5HDt35xaMhTLnhS/khHG26u9drRC6JpTnSR6sQrvpG8jXuT1ee4Jxkb9F8KpbgyhtDeOyQHblyXfqUTK5JP/UfXrnz/P0/6gizv/Q7nyg3TxMSW1D6jbglJbmJp3+TTuolBjbzxM+zeH3j4NaggZNSttGk31j/rY14hbrImf+M9fXktsny78/d8I0p+/92f6KRrHzL/iI5ZMqffiP+EWrIWtYj/Vuglxzen9wHKMM5yWq9P7wqnuMXj4YbZ2wwN3PXCFMyLeh/2dYbQv7sK83TRIFY1pB6TMOyYXWkcVVpfUuoWNceX0ZJ7NM6oKW+NOX8LfXL6lsGD6w8Qa+oWFB6givURc5IE7mg+sYQwtGdyc9YU66+MYSOw/9wd2945NAjj8WPWemVI0eUTy4KIf4uNXdDYq/7kml3P3XBnIEztB3viv3f5ItXnuicZPU72dGnEjkqI7luYEqW97763LBTy8zT0LF1U0vKJpvkFpyGzSldYHO8KfQXkRAaNzekXCJ+gbTzV6xrj53Qvq4i1it2tw+5xcTmmuQNVdyyvOSEV6iLvG9zPj0l9vx+7Gh/7Y7vr1qSH3+uJ9YMe/fbnyBnXtfeXHv+K6mftxLfAJzz/dmfSLxprZxyVYhvj3tk5HP6rmBWV82BHBOlc2C7Krkb5cCUbAv7nu0NIf+qT+f378N3dOfd5mlo3x2vIiv69+SoWVE75BwNDQ3p7WXvruTqSVz6mkp/vYj1uvkLM95qY2q9OdEVRkH+ON98Yq/VlI2RvT/8D4evemLayrfmxldDEr844qUjxyvKp/7tE0/N3ZDYA0znkCPZGeXd+x+5Yu7KL83e8CW/HzNUw+bGjTU1g4seqWq39a3YnLesob501fy+jTWJjtHyPn52pHHIOsv7vkJd5ER6f3jVKz8c4bP1m576kVgjeXO0oCWee0bNkdyPsu1Tr7SZp+lqvrKuoqG+pWFZXkoHqVj3lbRWULNxW23DsoaGZatW9G2sqVlRGxoa0i5xCj2iY/3a+OVa6kvz6hNbdHa8ryvMCX4DHgAyKanbkdizI6UGDP+ib83G+Jkalq1qjO/xsa32jBaiM32FWSfv5Tumeq6NYu6Gt3L6/r9y5/lCjEC+cjRP5UuEWRcBAHQRAGDCdpG8yQVGIcLkK0fkC9neRfJnzzcKGUdnyoW5/hDkG4185Wieypcop3xe6R8YhUwiMDjyjcbgyNE8lS9R7iKFVV+0PJhJ4R/9Wc4/BPlGIl85mqfyJcpdZNKMS6d//pumwXAXfupvzi3+aK4/CvlGI185mqfyJcLy+vr64v/my943//tfvN3+z+8dfWOij8jkgvNKr5tW85+jNAHkG4185WieypcodxEAgHHh90UAAF0EANBFAAB0EQBAFwEA0EUAAF0EAEAXAQB0kbOhY/3ivLxVjaOcuHh9h4gmFrnLBdBFAADG0Bj+Bnzjqrxlu9a176grGfyoVVq/cFvfxho5AMBEZV0EAIh+F4lvbF7WEEJLfWleTNpeIo2r8pJStkcP2T4dP5iUaRcTzq5YTovXd6REkUhiIL+huxOkhpYSW/z86SEOHjVsv4TBZ4fkx5H5mDVzcNir5chzJiWWoXvmZZxTmWPNMJdP52XBjGZEfWNmW20IFevaB49oX1cRvwu121IOJg8kDiXPHv+7/4TY9Qz8Td/YBpiSV//BtMODycQPDuadeuqQc6YdkZp7+jnTT2FsmY9ZMQFHnlBDpkbsfBUVKVMl7aU34wUzxzrKXD6tlwUzmuHGvYukHpN6jtQTh12ScesiKW87Q16hhr/MpL9FpaQ45NRMuQ+9khGulHHoIuZjtk3HtJnRvq6iYt262vT2MXLhSDucKdbR5/L7elkwoxkw7vuLLJxfMvB36YKK0LK7fdh54sfXl/pGYXaJxZKaX/xwckV366aWULGgdFiKm7bGMixZfktFaNicXJ/t2LsrVNyyvGTYZoHYldSuGNyzuWT+whB27fUsyILgzcesmH79MyN1QrXvDrcsr1tR2/9S2ri5YWB+jTanMsR6grl8yi8LZjTjuL/I+1RStyPWrkfc2YSslVoz+193RnjtjL1AjVRFEhqWpWymXtZgVM3HCSx1r43S+paQPqESb+qNmxtiM690QUVigg2v+hnm1GixjjKXT5UZTe52kZiajQOLeg3LvPzlgiGfd2KviSO8do5eRYYv4A5+JxzzcYIVkdL6MLDxo393u8F2EF+p6Ni7K76IEZthsQk2wvwaZU5linW0uXyKzGhyu4sMdvdttZb1sj6mWNUYsrmtfXdLyuez2Dlir52jVBELuOYjg+//8W0lGVt7zYraWBlp3LopJM5TMn9hbIK1706dXyc5p1JjPfFcPvlnixlNNnSR0bYxjqpxVdqXCdc2nM40YOzLSMOygQ9WHesXL2sItWsGPwIlysjXvjbKqkjNV9ZVtNSXDn46S3smME7Mx/GZUrE38oFO0LgqbRtNfxnZvXl3S//2lNIFFS2bVq5tSNvAknlOZYz1xHP5pJnRZDbm++LHxdfphu1EnXrE8G+b9bMHf5Z8j+aEO8UPfL1v5NiSqaafMOKXq0SfVd+jMR/HM4SBUY/NjLQNHompUjF0K87w76lkmFOjxZp5Lr+vlwVPHvqN4W/AAwAM4zfgAQBdBADQRQAAdBEAQBcBANBFAABdBABAFwEAdBEAAF0EANBFAAB0EQBAFwEA0EUAAF0EAOD0/M8AAAD///9d8HqGKSTvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9mPYTnQruzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torchtext import data\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "\n",
        "seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K82BKSB34BXb",
        "colab_type": "text"
      },
      "source": [
        "#### pre-processing - tokenization & loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFsWEZCCqqwx",
        "colab_type": "code",
        "outputId": "2d9d7969-f8fb-4271-edbf-6a5cb3b7ac07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "## create field - tokenize text & create label classes\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "# load dataset\n",
        "train_data, test_data = IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|| 84.1M/84.1M [00:02<00:00, 38.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eCXkL05rxLN",
        "colab_type": "code",
        "outputId": "929c9bd0-f5d2-44d9-a580-f2a2ede30394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# split training into train & validation\n",
        "train_data, valid_data = train_data.split(split_ratio=0.8, random_state = np.random.seed(seed))\n",
        "\n",
        "print('Training data size:   ', len(train_data))\n",
        "print('Validation data size: ', len(valid_data))\n",
        "print('Test data size:       ', len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size:    20000\n",
            "Validation data size:  5000\n",
            "Test data size:        25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32bavnQar8Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## build vocabulary - using top n words\n",
        "# we do not want to remove stop words & punctuations though (since it removes context)\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "# TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
        "# LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoFOblTW55Zc",
        "colab_type": "code",
        "outputId": "e4101359-738b-46cc-cc87-ac8da9cda9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, vectors=GloVe(name='6B', dim=300))\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [00:23, 36.7MB/s]                           \n",
            "100%|| 399448/400000 [00:38<00:00, 10463.07it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4t_1KaWsGCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## creating loader\n",
        "# data loader equivalent in torchtext batch iterator - buckets similar lengths together\n",
        "batch_size = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = batch_size,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW0wTkKdiB5G",
        "colab_type": "text"
      },
      "source": [
        "#### model implementation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2nnnSUssUMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Sentiment Analysis network\n",
        "class SentimentNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input, n_embed, n_hidden, n_output, pretrained_vec=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_dim = n_hidden\n",
        "        \n",
        "        self.embedding = nn.Embedding(n_input, n_embed)\n",
        "        if pretrained_vec is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_vec)\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            \n",
        "        self.lstm = LSTM(ncell=1, input_dim=n_embed, hidden_dim=n_hidden, \n",
        "                         output_dim=None, bidirectional=False, xavier_init=True)\n",
        "        self.fc = nn.Linear(n_hidden, n_output)\n",
        "        \n",
        "    def forward(self, x, h, c):\n",
        "        embed = self.embedding(x)\n",
        "        output, c = self.lstm(embed, h, c)\n",
        "        # getting latest hidden layer (for next iteration)\n",
        "        h = output[:,:,-1:]\n",
        "        y = self.fc(h)\n",
        "        return y, h, c\n",
        "      \n",
        "## Sentiment Analysis network - using PyTorch LSTM module\n",
        "class SentimentNetworkBaseline(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input, n_embed, n_hidden, n_output, pretrained_vec=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_dim = n_hidden\n",
        "        \n",
        "        self.embedding = nn.Embedding(n_input, n_embed)\n",
        "        # not training embedding layer if pretrained embedding is provided\n",
        "        if pretrained_vec is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_vec)\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            \n",
        "        self.lstm = nn.LSTM(n_embed, n_hidden)\n",
        "        self.fc = nn.Linear(n_hidden, n_output)\n",
        "        \n",
        "    def forward(self, x, h, c):\n",
        "        embed = self.embedding(x)\n",
        "        output, (h, c) = self.lstm(embed, (h, c))\n",
        "        y = self.fc(h.squeeze(0))\n",
        "        return y, h, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFo9ZzX25__H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_sentiment(model, train_loader, device, epochs, optimizer, loss_criterion, valid_loader=None):\n",
        "    \"\"\"\n",
        "    Trains the model of class LSTM using data from data_loader passed as argument\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    model: object of class torch.nn.Module\n",
        "    train_loader: an object of class BucketIterator containing training data\n",
        "        The next() function of the object returns a batch object, which has 2 members\n",
        "        text   - which is a 3D tensor of [sequence, batch, dim]\n",
        "            Example-A batch of 10 sentences of 5 words each where each word has\n",
        "            an embedding vector of size 256, the 3D tensor shape will be [5, 10, 256]\n",
        "        labels - a 1D tensor of length batch_size containing the classes as integers (torch.long())\n",
        "    valid_loader: Optional parameter, an object of class BucketIterator containing validation data.\n",
        "        Similar to train_loader\n",
        "        If passed, then it validates with the given data\n",
        "    Returns\n",
        "    =======\n",
        "    Trained model object of class LSTM, list containing loss progress\n",
        "    (, and list containing validation F1 score)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    stats = {'loss': [], 'score': []}\n",
        "    for i in range(epochs):\n",
        "        loss_tracker = []\n",
        "        \n",
        "        model.train()\n",
        "        for j, batch in enumerate(train_loader):\n",
        "          \n",
        "            # generate initial hidden & cell states\n",
        "            hidden_state = torch.zeros(1, batch.label.shape[0], model.hidden_dim).to(device)\n",
        "            cell_state = torch.zeros(1, batch.label.shape[0], model.hidden_dim).to(device)\n",
        "            \n",
        "            # forward pass\n",
        "            output, _, _ = model(x=batch.text, \n",
        "                                 h=hidden_state, \n",
        "                                 c=cell_state)\n",
        "            # backward pass for the batch (+ weight updates)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_criterion(output.squeeze(1), batch.label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, len(train_loader), \n",
        "                                                               loss.item()), end='\\r')\n",
        "            loss_tracker.append(loss.item())\n",
        "        \n",
        "        stats['loss'].append(np.mean(loss_tracker))\n",
        "        loss_tracker = []\n",
        "        print()\n",
        "        print(\"Epoch #{}: Average loss is {}\".format(i+1, stats['loss'][-1]))\n",
        "        if valid_loader is not None:\n",
        "            f1 = evaluate_sentiment(model, valid_loader, device, verbose=False)\n",
        "            stats['score'].append(f1)\n",
        "            print(\"Epoch #{}: Validation F1-score is {}\".format(i+1, stats['score'][-1]))\n",
        "        \n",
        "        print()\n",
        "    return model, stats\n",
        "  \n",
        "\n",
        "def evaluate_sentiment(model, test_loader, device, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluates the model of class LSTM using test data passed\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    model: object of class LSTM\n",
        "    test_data: a tuple containing (x, y) \n",
        "        x - a 3D tensor of [sequence, len_test_data, dim]\n",
        "            Example-A batch of 10 sentences of 5 words each where each word has\n",
        "            an embedding vector of size 256, the 3D tensor shape will be [5, 10, 256]\n",
        "        y - a 1D tensor containing the classes as integers (torch.long())\n",
        "    verbose: prints the confusion matrix and F-score\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "    F-score (float)\n",
        "    \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    preds = []\n",
        "    labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            hidden_state = torch.zeros(1, batch.label.shape[0], model.hidden_dim).to(device)\n",
        "            cell_state = torch.zeros(1, batch.label.shape[0], model.hidden_dim).to(device)\n",
        "            output, _, _ = model(x=batch.text, h=hidden_state, c=cell_state)\n",
        "            # get label predictions - since we predict only probabilities for label 1\n",
        "            pred = torch.round(torch.sigmoid(output)).cpu().detach().numpy()\n",
        "            preds.extend(pred)\n",
        "            labels.extend(batch.label.cpu().detach().numpy())\n",
        "            \n",
        "    if verbose:\n",
        "        print('Confusion Matrix: \\n', confusion_matrix(labels, preds))\n",
        "        print('Classification Report: \\n', classification_report(labels, preds))\n",
        "    return f1_score(labels, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E1xsgnbDfAl",
        "colab_type": "code",
        "outputId": "d18058e0-3967-4462-85aa-6b2a32cdde98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14608
        }
      },
      "source": [
        "# create sentiment analysis model\n",
        "modelSA = SentimentNetworkBaseline(n_input=len(TEXT.vocab), \n",
        "                                   n_embed=300,  # because of GloVe dims\n",
        "                                   n_hidden=256, \n",
        "                                   n_output=1,   # it is enough if we predict only probabilities for label 1\n",
        "                                   pretrained_vec=train_data.fields['text'].vocab.vectors)\n",
        "modelSA.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(modelSA.parameters(), lr=0.001)\n",
        "loss_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model, loss_list = train_sentiment(model=modelSA, \n",
        "                                   train_loader=train_iterator, \n",
        "                                   device=device,\n",
        "                                   epochs=200,\n",
        "                                   optimizer=optimizer,\n",
        "                                   loss_criterion=loss_criterion,\n",
        "                                   valid_loader=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1: Batch 313/313 -- Loss = 0.6968051195144653\n",
            "Epoch #1: Average loss is 0.6935135687882907\n",
            "Epoch #1: Validation F1-score is 0.3563843563843564\n",
            "\n",
            "Epoch #2: Batch 313/313 -- Loss = 0.6903514862060547\n",
            "Epoch #2: Average loss is 0.6927292866828724\n",
            "Epoch #2: Validation F1-score is 0.458153776366523\n",
            "\n",
            "Epoch #3: Batch 313/313 -- Loss = 0.6938179135322571\n",
            "Epoch #3: Average loss is 0.6923124710210977\n",
            "Epoch #3: Validation F1-score is 0.600206611570248\n",
            "\n",
            "Epoch #4: Batch 313/313 -- Loss = 0.6910571455955505\n",
            "Epoch #4: Average loss is 0.6921767335349378\n",
            "Epoch #4: Validation F1-score is 0.5424940428911835\n",
            "\n",
            "Epoch #5: Batch 313/313 -- Loss = 0.6857484579086304\n",
            "Epoch #5: Average loss is 0.6917159705878065\n",
            "Epoch #5: Validation F1-score is 0.4087363494539782\n",
            "\n",
            "Epoch #6: Batch 313/313 -- Loss = 0.689970850944519\n",
            "Epoch #6: Average loss is 0.6910650434966286\n",
            "Epoch #6: Validation F1-score is 0.6003603603603603\n",
            "\n",
            "Epoch #7: Batch 313/313 -- Loss = 0.689975380897522\n",
            "Epoch #7: Average loss is 0.690865975218459\n",
            "Epoch #7: Validation F1-score is 0.15303244005641747\n",
            "\n",
            "Epoch #8: Batch 313/313 -- Loss = 0.6882950067520142\n",
            "Epoch #8: Average loss is 0.6905156031203347\n",
            "Epoch #8: Validation F1-score is 0.6153262518968133\n",
            "\n",
            "Epoch #9: Batch 313/313 -- Loss = 0.68547123670578\n",
            "Epoch #9: Average loss is 0.6893243056516678\n",
            "Epoch #9: Validation F1-score is 0.5002346316283436\n",
            "\n",
            "Epoch #10: Batch 313/313 -- Loss = 0.6870193481445312\n",
            "Epoch #10: Average loss is 0.6879478622549258\n",
            "Epoch #10: Validation F1-score is 0.3158220024721879\n",
            "\n",
            "Epoch #11: Batch 313/313 -- Loss = 0.682127058506012\n",
            "Epoch #11: Average loss is 0.6871283591364901\n",
            "Epoch #11: Validation F1-score is 0.5790942262288046\n",
            "\n",
            "Epoch #12: Batch 313/313 -- Loss = 0.6794144511222839\n",
            "Epoch #12: Average loss is 0.6867785648035165\n",
            "Epoch #12: Validation F1-score is 0.5629807692307692\n",
            "\n",
            "Epoch #13: Batch 313/313 -- Loss = 0.6901760101318359\n",
            "Epoch #13: Average loss is 0.6865758219846902\n",
            "Epoch #13: Validation F1-score is 0.5315712187958884\n",
            "\n",
            "Epoch #14: Batch 313/313 -- Loss = 0.6935093402862549\n",
            "Epoch #14: Average loss is 0.6866381593024768\n",
            "Epoch #14: Validation F1-score is 0.5826771653543308\n",
            "\n",
            "Epoch #15: Batch 313/313 -- Loss = 0.6843001842498779\n",
            "Epoch #15: Average loss is 0.6860868163383045\n",
            "Epoch #15: Validation F1-score is 0.7064491064491065\n",
            "\n",
            "Epoch #16: Batch 313/313 -- Loss = 0.6828511953353882\n",
            "Epoch #16: Average loss is 0.6852349560862532\n",
            "Epoch #16: Validation F1-score is 0.6484978540772531\n",
            "\n",
            "Epoch #17: Batch 313/313 -- Loss = 0.6832407712936401\n",
            "Epoch #17: Average loss is 0.6848044751551204\n",
            "Epoch #17: Validation F1-score is 0.6692225772097977\n",
            "\n",
            "Epoch #18: Batch 313/313 -- Loss = 0.6859518885612488\n",
            "Epoch #18: Average loss is 0.6850814642235875\n",
            "Epoch #18: Validation F1-score is 0.6539306104523497\n",
            "\n",
            "Epoch #19: Batch 313/313 -- Loss = 0.6802920699119568\n",
            "Epoch #19: Average loss is 0.6843583077287522\n",
            "Epoch #19: Validation F1-score is 0.6893845518380833\n",
            "\n",
            "Epoch #20: Batch 313/313 -- Loss = 0.5764778256416321\n",
            "Epoch #20: Average loss is 0.5973385566720566\n",
            "Epoch #20: Validation F1-score is 0.7489571482745545\n",
            "\n",
            "Epoch #21: Batch 313/313 -- Loss = 0.44155988097190857\n",
            "Epoch #21: Average loss is 0.5654080489192146\n",
            "Epoch #21: Validation F1-score is 0.7047459050818984\n",
            "\n",
            "Epoch #22: Batch 313/313 -- Loss = 0.5059027671813965\n",
            "Epoch #22: Average loss is 0.5856260718248142\n",
            "Epoch #22: Validation F1-score is 0.7254658385093169\n",
            "\n",
            "Epoch #23: Batch 313/313 -- Loss = 0.6552351713180542\n",
            "Epoch #23: Average loss is 0.5506730904213537\n",
            "Epoch #23: Validation F1-score is 0.7055941200489996\n",
            "\n",
            "Epoch #24: Batch 313/313 -- Loss = 0.5414605140686035\n",
            "Epoch #24: Average loss is 0.5994494216510663\n",
            "Epoch #24: Validation F1-score is 0.7218530472754888\n",
            "\n",
            "Epoch #25: Batch 313/313 -- Loss = 0.5182418823242188\n",
            "Epoch #25: Average loss is 0.5557114728533041\n",
            "Epoch #25: Validation F1-score is 0.7454057402436506\n",
            "\n",
            "Epoch #26: Batch 313/313 -- Loss = 0.549186110496521\n",
            "Epoch #26: Average loss is 0.5549958336848421\n",
            "Epoch #26: Validation F1-score is 0.7509025270758123\n",
            "\n",
            "Epoch #27: Batch 313/313 -- Loss = 0.5718840956687927\n",
            "Epoch #27: Average loss is 0.5334479601238482\n",
            "Epoch #27: Validation F1-score is 0.7601302460202605\n",
            "\n",
            "Epoch #28: Batch 313/313 -- Loss = 0.5437909364700317\n",
            "Epoch #28: Average loss is 0.5783247961975134\n",
            "Epoch #28: Validation F1-score is 0.7510481133958874\n",
            "\n",
            "Epoch #29: Batch 313/313 -- Loss = 0.5479103922843933\n",
            "Epoch #29: Average loss is 0.5741304102987527\n",
            "Epoch #29: Validation F1-score is 0.7199170124481328\n",
            "\n",
            "Epoch #30: Batch 313/313 -- Loss = 0.5400840044021606\n",
            "Epoch #30: Average loss is 0.5402277493819642\n",
            "Epoch #30: Validation F1-score is 0.7698695136417556\n",
            "\n",
            "Epoch #31: Batch 313/313 -- Loss = 0.5622332692146301\n",
            "Epoch #31: Average loss is 0.5416495006876632\n",
            "Epoch #31: Validation F1-score is 0.6996039191161142\n",
            "\n",
            "Epoch #32: Batch 313/313 -- Loss = 0.5762872099876404\n",
            "Epoch #32: Average loss is 0.5677390398499303\n",
            "Epoch #32: Validation F1-score is 0.7448329148155303\n",
            "\n",
            "Epoch #33: Batch 313/313 -- Loss = 0.6514832377433777\n",
            "Epoch #33: Average loss is 0.5634392359957527\n",
            "Epoch #33: Validation F1-score is 0.6525789593274256\n",
            "\n",
            "Epoch #34: Batch 313/313 -- Loss = 0.5485105514526367\n",
            "Epoch #34: Average loss is 0.5667096388797028\n",
            "Epoch #34: Validation F1-score is 0.762987012987013\n",
            "\n",
            "Epoch #35: Batch 313/313 -- Loss = 0.43342554569244385\n",
            "Epoch #35: Average loss is 0.539233489824941\n",
            "Epoch #35: Validation F1-score is 0.7233391608391608\n",
            "\n",
            "Epoch #36: Batch 313/313 -- Loss = 0.49859148263931274\n",
            "Epoch #36: Average loss is 0.5668205131357089\n",
            "Epoch #36: Validation F1-score is 0.7433841218925421\n",
            "\n",
            "Epoch #37: Batch 313/313 -- Loss = 0.5259652733802795\n",
            "Epoch #37: Average loss is 0.5577478093651537\n",
            "Epoch #37: Validation F1-score is 0.7746853394788158\n",
            "\n",
            "Epoch #38: Batch 313/313 -- Loss = 0.529104471206665\n",
            "Epoch #38: Average loss is 0.4911785111450159\n",
            "Epoch #38: Validation F1-score is 0.8018906699547882\n",
            "\n",
            "Epoch #39: Batch 313/313 -- Loss = 0.6521575450897217\n",
            "Epoch #39: Average loss is 0.568780028972382\n",
            "Epoch #39: Validation F1-score is 0.5309207512597343\n",
            "\n",
            "Epoch #40: Batch 313/313 -- Loss = 0.43607670068740845\n",
            "Epoch #40: Average loss is 0.5016777163115553\n",
            "Epoch #40: Validation F1-score is 0.7978880675818374\n",
            "\n",
            "Epoch #41: Batch 313/313 -- Loss = 0.4263707995414734\n",
            "Epoch #41: Average loss is 0.3819386926702798\n",
            "Epoch #41: Validation F1-score is 0.8249059757626411\n",
            "\n",
            "Epoch #42: Batch 313/313 -- Loss = 0.17667460441589355\n",
            "Epoch #42: Average loss is 0.34442719574363084\n",
            "Epoch #42: Validation F1-score is 0.8560431100846805\n",
            "\n",
            "Epoch #43: Batch 313/313 -- Loss = 0.13280677795410156\n",
            "Epoch #43: Average loss is 0.2768083554677689\n",
            "Epoch #43: Validation F1-score is 0.8616290480863591\n",
            "\n",
            "Epoch #44: Batch 313/313 -- Loss = 0.17797449231147766\n",
            "Epoch #44: Average loss is 0.22553229746155845\n",
            "Epoch #44: Validation F1-score is 0.8595573440643863\n",
            "\n",
            "Epoch #45: Batch 313/313 -- Loss = 0.16199736297130585\n",
            "Epoch #45: Average loss is 0.1765940995357288\n",
            "Epoch #45: Validation F1-score is 0.8620417816318486\n",
            "\n",
            "Epoch #46: Batch 313/313 -- Loss = 0.1709834784269333\n",
            "Epoch #46: Average loss is 0.12681542580929428\n",
            "Epoch #46: Validation F1-score is 0.86748417721519\n",
            "\n",
            "Epoch #47: Batch 313/313 -- Loss = 0.23526614904403687\n",
            "Epoch #47: Average loss is 0.10003956464735178\n",
            "Epoch #47: Validation F1-score is 0.8667558784171286\n",
            "\n",
            "Epoch #48: Batch 313/313 -- Loss = 0.07395759224891663\n",
            "Epoch #48: Average loss is 0.07633532834843325\n",
            "Epoch #48: Validation F1-score is 0.8675691583284284\n",
            "\n",
            "Epoch #49: Batch 313/313 -- Loss = 0.11810746043920517\n",
            "Epoch #49: Average loss is 0.061439029345759\n",
            "Epoch #49: Validation F1-score is 0.8637795275590551\n",
            "\n",
            "Epoch #50: Batch 313/313 -- Loss = 0.08600332587957382\n",
            "Epoch #50: Average loss is 0.06096803152058928\n",
            "Epoch #50: Validation F1-score is 0.8529053230394148\n",
            "\n",
            "Epoch #51: Batch 313/313 -- Loss = 0.01325630210340023\n",
            "Epoch #51: Average loss is 0.04847566049528151\n",
            "Epoch #51: Validation F1-score is 0.8551264457949421\n",
            "\n",
            "Epoch #52: Batch 313/313 -- Loss = 0.027306606993079185\n",
            "Epoch #52: Average loss is 0.037388850708667654\n",
            "Epoch #52: Validation F1-score is 0.8570281124497992\n",
            "\n",
            "Epoch #53: Batch 313/313 -- Loss = 0.007183386012911797\n",
            "Epoch #53: Average loss is 0.03847088074659744\n",
            "Epoch #53: Validation F1-score is 0.8666021297192643\n",
            "\n",
            "Epoch #54: Batch 313/313 -- Loss = 0.0034461196046322584\n",
            "Epoch #54: Average loss is 0.04023450763729481\n",
            "Epoch #54: Validation F1-score is 0.8599014778325123\n",
            "\n",
            "Epoch #55: Batch 313/313 -- Loss = 0.01206234935671091\n",
            "Epoch #55: Average loss is 0.02590645711010578\n",
            "Epoch #55: Validation F1-score is 0.8521986970684039\n",
            "\n",
            "Epoch #56: Batch 313/313 -- Loss = 0.003346480429172516\n",
            "Epoch #56: Average loss is 0.026899706847897283\n",
            "Epoch #56: Validation F1-score is 0.8594377510040162\n",
            "\n",
            "Epoch #57: Batch 313/313 -- Loss = 0.015645531937479973\n",
            "Epoch #57: Average loss is 0.024287748324964494\n",
            "Epoch #57: Validation F1-score is 0.8487481895303124\n",
            "\n",
            "Epoch #58: Batch 313/313 -- Loss = 0.03584564849734306\n",
            "Epoch #58: Average loss is 0.025717067920912308\n",
            "Epoch #58: Validation F1-score is 0.8556910569105691\n",
            "\n",
            "Epoch #59: Batch 313/313 -- Loss = 0.00413993326947093\n",
            "Epoch #59: Average loss is 0.022121471136037747\n",
            "Epoch #59: Validation F1-score is 0.8601583113456466\n",
            "\n",
            "Epoch #60: Batch 313/313 -- Loss = 0.06609629839658737\n",
            "Epoch #60: Average loss is 0.012995169711888384\n",
            "Epoch #60: Validation F1-score is 0.8537774167343624\n",
            "\n",
            "Epoch #61: Batch 313/313 -- Loss = 0.0037642240058630705\n",
            "Epoch #61: Average loss is 0.016170774422289903\n",
            "Epoch #61: Validation F1-score is 0.865804209306816\n",
            "\n",
            "Epoch #62: Batch 313/313 -- Loss = 0.003838342847302556\n",
            "Epoch #62: Average loss is 0.024732928850298253\n",
            "Epoch #62: Validation F1-score is 0.867465148242686\n",
            "\n",
            "Epoch #63: Batch 313/313 -- Loss = 0.01174865011125803\n",
            "Epoch #63: Average loss is 0.01425712498387667\n",
            "Epoch #63: Validation F1-score is 0.8649940262843488\n",
            "\n",
            "Epoch #64: Batch 313/313 -- Loss = 0.008999600075185299\n",
            "Epoch #64: Average loss is 0.01885847306007686\n",
            "Epoch #64: Validation F1-score is 0.8596456301015329\n",
            "\n",
            "Epoch #65: Batch 313/313 -- Loss = 0.10469141602516174\n",
            "Epoch #65: Average loss is 0.017021437097465793\n",
            "Epoch #65: Validation F1-score is 0.8680769230769231\n",
            "\n",
            "Epoch #66: Batch 313/313 -- Loss = 0.008932217955589294\n",
            "Epoch #66: Average loss is 0.016815777808523027\n",
            "Epoch #66: Validation F1-score is 0.8584106494554256\n",
            "\n",
            "Epoch #67: Batch 313/313 -- Loss = 0.005804248619824648\n",
            "Epoch #67: Average loss is 0.01110999788815593\n",
            "Epoch #67: Validation F1-score is 0.8570271364925071\n",
            "\n",
            "Epoch #68: Batch 313/313 -- Loss = 0.0016104073729366064\n",
            "Epoch #68: Average loss is 0.014998433957495295\n",
            "Epoch #68: Validation F1-score is 0.8576051779935274\n",
            "\n",
            "Epoch #69: Batch 313/313 -- Loss = 0.0020340292248874903\n",
            "Epoch #69: Average loss is 0.016967616637219518\n",
            "Epoch #69: Validation F1-score is 0.8649298597194389\n",
            "\n",
            "Epoch #70: Batch 313/313 -- Loss = 0.003522728104144335\n",
            "Epoch #70: Average loss is 0.015327114722142501\n",
            "Epoch #70: Validation F1-score is 0.863312202852615\n",
            "\n",
            "Epoch #71: Batch 313/313 -- Loss = 0.04852338880300522\n",
            "Epoch #71: Average loss is 0.010663191740290998\n",
            "Epoch #71: Validation F1-score is 0.8528042113788217\n",
            "\n",
            "Epoch #72: Batch 313/313 -- Loss = 0.0018352732295170426\n",
            "Epoch #72: Average loss is 0.013556971430154397\n",
            "Epoch #72: Validation F1-score is 0.8560263266145619\n",
            "\n",
            "Epoch #73: Batch 313/313 -- Loss = 0.0009823080617934465\n",
            "Epoch #73: Average loss is 0.017048426367641506\n",
            "Epoch #73: Validation F1-score is 0.862618519265685\n",
            "\n",
            "\n",
            "Epoch #74: Average loss is 0.013140898716551202\n",
            "Epoch #74: Validation F1-score is 0.8589153492095258\n",
            "\n",
            "Epoch #75: Batch 313/313 -- Loss = 0.01470749732106924\n",
            "Epoch #75: Average loss is 0.01087728639772257\n",
            "Epoch #75: Validation F1-score is 0.8667736757624398\n",
            "\n",
            "Epoch #76: Batch 313/313 -- Loss = 0.010502737946808338\n",
            "Epoch #76: Average loss is 0.009533891952544534\n",
            "Epoch #76: Validation F1-score is 0.8677735547109422\n",
            "\n",
            "Epoch #77: Batch 313/313 -- Loss = 0.00045817281352356076\n",
            "Epoch #77: Average loss is 0.006001526885870517\n",
            "Epoch #77: Validation F1-score is 0.8623889437314907\n",
            "\n",
            "Epoch #78: Batch 313/313 -- Loss = 0.11727769672870636\n",
            "Epoch #78: Average loss is 0.010047089523850038\n",
            "Epoch #78: Validation F1-score is 0.8562909958258794\n",
            "\n",
            "Epoch #79: Batch 313/313 -- Loss = 0.004161116201430559\n",
            "Epoch #79: Average loss is 0.014457663153513921\n",
            "Epoch #79: Validation F1-score is 0.8619197482297403\n",
            "\n",
            "Epoch #80: Batch 313/313 -- Loss = 0.001109514618292451\n",
            "Epoch #80: Average loss is 0.007635983728984826\n",
            "Epoch #80: Validation F1-score is 0.8550338322739389\n",
            "\n",
            "Epoch #81: Batch 313/313 -- Loss = 0.0028477029409259558\n",
            "Epoch #81: Average loss is 0.004822555643498165\n",
            "Epoch #81: Validation F1-score is 0.8633791430881165\n",
            "\n",
            "Epoch #82: Batch 313/313 -- Loss = 0.0011002025566995144\n",
            "Epoch #82: Average loss is 0.008135311410315305\n",
            "Epoch #82: Validation F1-score is 0.8641184710826496\n",
            "\n",
            "Epoch #83: Batch 313/313 -- Loss = 0.001407816307619214\n",
            "Epoch #83: Average loss is 0.006754376379668507\n",
            "Epoch #83: Validation F1-score is 0.8663594470046083\n",
            "\n",
            "Epoch #84: Batch 313/313 -- Loss = 0.003210129914805293\n",
            "Epoch #84: Average loss is 0.009859954051625246\n",
            "Epoch #84: Validation F1-score is 0.8605268531754134\n",
            "\n",
            "Epoch #85: Batch 313/313 -- Loss = 0.004403236322104931\n",
            "Epoch #85: Average loss is 0.00591066441393486\n",
            "Epoch #85: Validation F1-score is 0.8689006466784245\n",
            "\n",
            "Epoch #86: Batch 313/313 -- Loss = 0.005079168826341629\n",
            "Epoch #86: Average loss is 0.00718216878846351\n",
            "Epoch #86: Validation F1-score is 0.8655595096876236\n",
            "\n",
            "Epoch #87: Batch 313/313 -- Loss = 0.001894112559966743\n",
            "Epoch #87: Average loss is 0.005302248220814886\n",
            "Epoch #87: Validation F1-score is 0.8378378378378379\n",
            "\n",
            "Epoch #88: Batch 313/313 -- Loss = 0.008635164238512516\n",
            "Epoch #88: Average loss is 0.007921553166640272\n",
            "Epoch #88: Validation F1-score is 0.860759493670886\n",
            "\n",
            "Epoch #89: Batch 313/313 -- Loss = 5.303224315866828e-05\n",
            "Epoch #89: Average loss is 0.0020331342802064074\n",
            "Epoch #89: Validation F1-score is 0.8717454194792672\n",
            "\n",
            "Epoch #90: Batch 313/313 -- Loss = 0.12508690357208252\n",
            "Epoch #90: Average loss is 0.011500057537279888\n",
            "Epoch #90: Validation F1-score is 0.864450127877238\n",
            "\n",
            "Epoch #91: Batch 313/313 -- Loss = 0.025364603847265244\n",
            "Epoch #91: Average loss is 0.0049020209913613454\n",
            "Epoch #91: Validation F1-score is 0.8661544682571776\n",
            "\n",
            "Epoch #92: Batch 313/313 -- Loss = 0.0018640257185325027\n",
            "Epoch #92: Average loss is 0.008747333618623506\n",
            "Epoch #92: Validation F1-score is 0.8678802113916617\n",
            "\n",
            "Epoch #93: Batch 313/313 -- Loss = 0.0004074880271218717\n",
            "Epoch #93: Average loss is 0.004102065054755662\n",
            "Epoch #93: Validation F1-score is 0.8626817447495961\n",
            "\n",
            "Epoch #94: Batch 313/313 -- Loss = 0.00021169667888898402\n",
            "Epoch #94: Average loss is 0.0006212214771664458\n",
            "Epoch #94: Validation F1-score is 0.859071384741256\n",
            "\n",
            "Epoch #95: Batch 313/313 -- Loss = 0.00064504035981372\n",
            "Epoch #95: Average loss is 0.011323625476960298\n",
            "Epoch #95: Validation F1-score is 0.8651572831095973\n",
            "\n",
            "Epoch #96: Batch 313/313 -- Loss = 0.000495404819957912\n",
            "Epoch #96: Average loss is 0.0059513238582660785\n",
            "Epoch #96: Validation F1-score is 0.8663036902601331\n",
            "\n",
            "Epoch #97: Batch 313/313 -- Loss = 0.006407387554645538\n",
            "Epoch #97: Average loss is 0.006504352294439904\n",
            "Epoch #97: Validation F1-score is 0.850225687320476\n",
            "\n",
            "Epoch #98: Batch 313/313 -- Loss = 0.0006036022095941007\n",
            "Epoch #98: Average loss is 0.005926906452907552\n",
            "Epoch #98: Validation F1-score is 0.8610376398779247\n",
            "\n",
            "Epoch #99: Batch 313/313 -- Loss = 7.881689816713333e-05\n",
            "Epoch #99: Average loss is 0.0015127124946196036\n",
            "Epoch #99: Validation F1-score is 0.8574346405228758\n",
            "\n",
            "Epoch #100: Batch 313/313 -- Loss = 0.0028710071928799152\n",
            "Epoch #100: Average loss is 0.010423253785236553\n",
            "Epoch #100: Validation F1-score is 0.8640603294304425\n",
            "\n",
            "Epoch #101: Batch 313/313 -- Loss = 0.00010911394201684743\n",
            "Epoch #101: Average loss is 0.002112088404212667\n",
            "Epoch #101: Validation F1-score is 0.8666666666666667\n",
            "\n",
            "Epoch #102: Batch 313/313 -- Loss = 0.008195115253329277\n",
            "Epoch #102: Average loss is 0.0036222377944888865\n",
            "Epoch #102: Validation F1-score is 0.8673306772908366\n",
            "\n",
            "Epoch #103: Batch 313/313 -- Loss = 0.000439164403360337\n",
            "Epoch #103: Average loss is 0.007351495841983985\n",
            "Epoch #103: Validation F1-score is 0.8495793145906012\n",
            "\n",
            "Epoch #104: Batch 313/313 -- Loss = 0.0002698480093386024\n",
            "Epoch #104: Average loss is 0.007102981425000086\n",
            "Epoch #104: Validation F1-score is 0.8635999999999999\n",
            "\n",
            "Epoch #105: Batch 313/313 -- Loss = 0.018934225663542747\n",
            "Epoch #105: Average loss is 0.0030909048755609126\n",
            "Epoch #105: Validation F1-score is 0.8544123627490849\n",
            "\n",
            "Epoch #106: Batch 313/313 -- Loss = 0.0006635867757722735\n",
            "Epoch #106: Average loss is 0.009442426670333293\n",
            "Epoch #106: Validation F1-score is 0.8565582156742377\n",
            "\n",
            "Epoch #107: Batch 313/313 -- Loss = 0.0006229299469850957\n",
            "Epoch #107: Average loss is 0.006111199763394667\n",
            "Epoch #107: Validation F1-score is 0.8646967340590981\n",
            "\n",
            "Epoch #108: Batch 313/313 -- Loss = 0.05745286867022514\n",
            "Epoch #108: Average loss is 0.003504759017546392\n",
            "Epoch #108: Validation F1-score is 0.8655227454110136\n",
            "\n",
            "Epoch #109: Batch 313/313 -- Loss = 4.742473174701445e-05\n",
            "Epoch #109: Average loss is 0.004284180781256167\n",
            "Epoch #109: Validation F1-score is 0.8632866443085422\n",
            "\n",
            "Epoch #110: Batch 313/313 -- Loss = 0.008554486557841301\n",
            "Epoch #110: Average loss is 0.0018449994436065236\n",
            "Epoch #110: Validation F1-score is 0.858628274345131\n",
            "\n",
            "Epoch #111: Batch 313/313 -- Loss = 6.745893915649503e-05\n",
            "Epoch #111: Average loss is 0.005000849709775272\n",
            "Epoch #111: Validation F1-score is 0.8679245283018868\n",
            "\n",
            "Epoch #112: Batch 313/313 -- Loss = 0.0008317756000906229\n",
            "Epoch #112: Average loss is 0.0085880019394858\n",
            "Epoch #112: Validation F1-score is 0.8623015873015875\n",
            "\n",
            "Epoch #113: Batch 313/313 -- Loss = 0.0019216422224417329\n",
            "Epoch #113: Average loss is 0.0054487747340561665\n",
            "Epoch #113: Validation F1-score is 0.8690171361039984\n",
            "\n",
            "Epoch #114: Batch 313/313 -- Loss = 9.130466060014442e-05\n",
            "Epoch #114: Average loss is 0.0008669892037829913\n",
            "Epoch #114: Validation F1-score is 0.8681297339289182\n",
            "\n",
            "Epoch #115: Batch 313/313 -- Loss = 0.00141788343898952\n",
            "Epoch #115: Average loss is 0.006664381901127883\n",
            "Epoch #115: Validation F1-score is 0.8647023451593507\n",
            "\n",
            "Epoch #116: Batch 313/313 -- Loss = 5.324009180185385e-05\n",
            "Epoch #116: Average loss is 0.0034969541976314273\n",
            "Epoch #116: Validation F1-score is 0.8665207877461706\n",
            "\n",
            "Epoch #117: Batch 313/313 -- Loss = 0.0070243822410702705\n",
            "Epoch #117: Average loss is 0.005285015420772666\n",
            "Epoch #117: Validation F1-score is 0.8637358265367018\n",
            "\n",
            "Epoch #118: Batch 313/313 -- Loss = 3.199961065547541e-05\n",
            "Epoch #118: Average loss is 0.0028707302709559375\n",
            "Epoch #118: Validation F1-score is 0.8608519269776878\n",
            "\n",
            "Epoch #119: Batch 313/313 -- Loss = 0.0065478491596877575\n",
            "Epoch #119: Average loss is 0.0044741411483019055\n",
            "Epoch #119: Validation F1-score is 0.8616789045445524\n",
            "\n",
            "Epoch #120: Batch 313/313 -- Loss = 0.00010446312080603093\n",
            "Epoch #120: Average loss is 0.004906769040449892\n",
            "Epoch #120: Validation F1-score is 0.8628696169874975\n",
            "\n",
            "Epoch #121: Batch 313/313 -- Loss = 0.000928808469325304\n",
            "Epoch #121: Average loss is 0.0029105714825730503\n",
            "Epoch #121: Validation F1-score is 0.8682658277624853\n",
            "\n",
            "Epoch #122: Batch 313/313 -- Loss = 0.00015196905587799847\n",
            "Epoch #122: Average loss is 0.0011366129158018091\n",
            "Epoch #122: Validation F1-score is 0.862551764937882\n",
            "\n",
            "Epoch #123: Batch 313/313 -- Loss = 0.002589987125247717\n",
            "Epoch #123: Average loss is 0.013231435234875374\n",
            "Epoch #123: Validation F1-score is 0.8634165995165188\n",
            "\n",
            "Epoch #124: Batch 313/313 -- Loss = 0.0003665077092591673\n",
            "Epoch #124: Average loss is 0.0037689480192673107\n",
            "Epoch #124: Validation F1-score is 0.8630988173982761\n",
            "\n",
            "Epoch #125: Batch 313/313 -- Loss = 0.0003621657087933272\n",
            "Epoch #125: Average loss is 0.0019688280910497976\n",
            "Epoch #125: Validation F1-score is 0.8636911942098915\n",
            "\n",
            "Epoch #126: Batch 313/313 -- Loss = 7.872702553868294e-05\n",
            "Epoch #126: Average loss is 0.002761531550974517\n",
            "Epoch #126: Validation F1-score is 0.8636273143539717\n",
            "\n",
            "Epoch #127: Batch 313/313 -- Loss = 0.0019112627487629652\n",
            "Epoch #127: Average loss is 0.009991030669453132\n",
            "Epoch #127: Validation F1-score is 0.8649193548387096\n",
            "\n",
            "Epoch #128: Batch 313/313 -- Loss = 0.0059474618174135685\n",
            "Epoch #128: Average loss is 0.004112852424848174\n",
            "Epoch #128: Validation F1-score is 0.864098982239074\n",
            "\n",
            "Epoch #129: Batch 313/313 -- Loss = 0.00024778538499958813\n",
            "Epoch #129: Average loss is 0.003200320733415715\n",
            "Epoch #129: Validation F1-score is 0.8649830980314178\n",
            "\n",
            "Epoch #130: Batch 313/313 -- Loss = 0.00010032164573203772\n",
            "Epoch #130: Average loss is 0.0022311870001338104\n",
            "Epoch #130: Validation F1-score is 0.8647476340694006\n",
            "\n",
            "Epoch #131: Batch 313/313 -- Loss = 0.00031366938492283225\n",
            "Epoch #131: Average loss is 0.005280555932635537\n",
            "Epoch #131: Validation F1-score is 0.8646838684262359\n",
            "\n",
            "Epoch #132: Batch 313/313 -- Loss = 4.6243821998359635e-05\n",
            "Epoch #132: Average loss is 0.004608446277497439\n",
            "Epoch #132: Validation F1-score is 0.8552147239263804\n",
            "\n",
            "Epoch #133: Batch 313/313 -- Loss = 0.0005164992762729526\n",
            "Epoch #133: Average loss is 0.006128766948805219\n",
            "Epoch #133: Validation F1-score is 0.8710546574287913\n",
            "\n",
            "Epoch #134: Batch 313/313 -- Loss = 0.000631459872238338\n",
            "Epoch #134: Average loss is 0.002202348162808021\n",
            "Epoch #134: Validation F1-score is 0.8643495531281032\n",
            "\n",
            "Epoch #135: Batch 313/313 -- Loss = 0.00012680666986852884\n",
            "Epoch #135: Average loss is 0.00238374256698295\n",
            "Epoch #135: Validation F1-score is 0.8689973107952363\n",
            "\n",
            "Epoch #136: Batch 313/313 -- Loss = 4.973720933776349e-05\n",
            "Epoch #136: Average loss is 0.0005760083296770235\n",
            "Epoch #136: Validation F1-score is 0.8639713204540929\n",
            "\n",
            "Epoch #137: Batch 313/313 -- Loss = 1.3776261766906828e-05\n",
            "Epoch #137: Average loss is 6.921404526572576e-05\n",
            "Epoch #137: Validation F1-score is 0.8666137356093688\n",
            "\n",
            "Epoch #138: Batch 313/313 -- Loss = 3.505937638692558e-05\n",
            "Epoch #138: Average loss is 0.00021917989319751306\n",
            "Epoch #138: Validation F1-score is 0.8682385575589459\n",
            "\n",
            "Epoch #139: Batch 313/313 -- Loss = 3.965522409998812e-06\n",
            "Epoch #139: Average loss is 3.755616146558017e-05\n",
            "Epoch #139: Validation F1-score is 0.8669579030976967\n",
            "\n",
            "Epoch #140: Batch 313/313 -- Loss = 6.578484317287803e-06\n",
            "Epoch #140: Average loss is 2.9111825750560777e-05\n",
            "Epoch #140: Validation F1-score is 0.8664546899841018\n",
            "\n",
            "Epoch #141: Batch 313/313 -- Loss = 1.56557925947709e-05\n",
            "Epoch #141: Average loss is 2.0339185627966882e-05\n",
            "Epoch #141: Validation F1-score is 0.8664546899841018\n",
            "\n",
            "Epoch #142: Batch 313/313 -- Loss = 1.386327130603604e-05\n",
            "Epoch #142: Average loss is 1.9282153446804555e-05\n",
            "Epoch #142: Validation F1-score is 0.868738863591368\n",
            "\n",
            "Epoch #143: Batch 313/313 -- Loss = 1.2116909601900261e-05\n",
            "Epoch #143: Average loss is 1.9614338132890054e-05\n",
            "Epoch #143: Validation F1-score is 0.8692900929404785\n",
            "\n",
            "Epoch #144: Batch 313/313 -- Loss = 1.4527132407238241e-05\n",
            "Epoch #144: Average loss is 2.855019775284083e-05\n",
            "Epoch #144: Validation F1-score is 0.8715127701375246\n",
            "\n",
            "Epoch #145: Batch 313/313 -- Loss = 5.846557542099617e-06\n",
            "Epoch #145: Average loss is 7.999130372199826e-06\n",
            "Epoch #145: Validation F1-score is 0.8709169618260528\n",
            "\n",
            "Epoch #146: Batch 313/313 -- Loss = 0.019359270110726357\n",
            "Epoch #146: Average loss is 0.018728607920341384\n",
            "Epoch #146: Validation F1-score is 0.8608608608608609\n",
            "\n",
            "Epoch #147: Batch 313/313 -- Loss = 0.0007517942576669157\n",
            "Epoch #147: Average loss is 0.015581011157395458\n",
            "Epoch #147: Validation F1-score is 0.8689870942510753\n",
            "\n",
            "Epoch #148: Batch 313/313 -- Loss = 0.00017507598386146128\n",
            "Epoch #148: Average loss is 0.00403945862299602\n",
            "Epoch #148: Validation F1-score is 0.8669815033451397\n",
            "\n",
            "Epoch #149: Batch 313/313 -- Loss = 0.000148070088471286\n",
            "Epoch #149: Average loss is 0.0018773179244149264\n",
            "Epoch #149: Validation F1-score is 0.8646942800788955\n",
            "\n",
            "Epoch #150: Batch 313/313 -- Loss = 0.04174181446433067\n",
            "Epoch #150: Average loss is 0.0024390293444961422\n",
            "Epoch #150: Validation F1-score is 0.8660201860281022\n",
            "\n",
            "Epoch #151: Batch 313/313 -- Loss = 0.0008217893773689866\n",
            "Epoch #151: Average loss is 0.00412886282274979\n",
            "Epoch #151: Validation F1-score is 0.8603130572617397\n",
            "\n",
            "Epoch #152: Batch 313/313 -- Loss = 3.982420457759872e-05\n",
            "Epoch #152: Average loss is 0.0031222103055692633\n",
            "Epoch #152: Validation F1-score is 0.8651863600317207\n",
            "\n",
            "Epoch #153: Batch 313/313 -- Loss = 9.323526319349185e-05\n",
            "Epoch #153: Average loss is 0.004099833749537989\n",
            "Epoch #153: Validation F1-score is 0.8675121951219512\n",
            "\n",
            "Epoch #154: Batch 313/313 -- Loss = 0.0007165959686972201\n",
            "Epoch #154: Average loss is 0.005659023939481525\n",
            "Epoch #154: Validation F1-score is 0.855279249898084\n",
            "\n",
            "Epoch #155: Batch 313/313 -- Loss = 0.004495492670685053\n",
            "Epoch #155: Average loss is 0.00799261463676019\n",
            "Epoch #155: Validation F1-score is 0.8660079051383399\n",
            "\n",
            "Epoch #156: Batch 313/313 -- Loss = 4.8511428758502007e-05\n",
            "Epoch #156: Average loss is 0.0015083555934313354\n",
            "Epoch #156: Validation F1-score is 0.860469785183698\n",
            "\n",
            "Epoch #157: Batch 313/313 -- Loss = 0.00014938258391339332\n",
            "Epoch #157: Average loss is 0.0034554665827263684\n",
            "Epoch #157: Validation F1-score is 0.870611439842209\n",
            "\n",
            "Epoch #158: Batch 313/313 -- Loss = 6.988763925619423e-05\n",
            "Epoch #158: Average loss is 0.0024586369730178366\n",
            "Epoch #158: Validation F1-score is 0.8663503361012257\n",
            "\n",
            "Epoch #159: Batch 313/313 -- Loss = 0.06969679892063141\n",
            "Epoch #159: Average loss is 0.00965915897458458\n",
            "Epoch #159: Validation F1-score is 0.8663484486873508\n",
            "\n",
            "Epoch #160: Batch 313/313 -- Loss = 0.0003739668463822454\n",
            "Epoch #160: Average loss is 0.0035930609757863783\n",
            "Epoch #160: Validation F1-score is 0.8662095984329089\n",
            "\n",
            "Epoch #161: Batch 313/313 -- Loss = 2.1020918211434036e-05\n",
            "Epoch #161: Average loss is 0.0004530556540591821\n",
            "Epoch #161: Validation F1-score is 0.8698884758364313\n",
            "\n",
            "Epoch #162: Batch 313/313 -- Loss = 1.017353315546643e-05\n",
            "Epoch #162: Average loss is 0.000197281218580943\n",
            "Epoch #162: Validation F1-score is 0.8707964601769912\n",
            "\n",
            "Epoch #163: Batch 313/313 -- Loss = 5.177350976737216e-05\n",
            "Epoch #163: Average loss is 6.0851454625041054e-05\n",
            "Epoch #163: Validation F1-score is 0.8716840243662801\n",
            "\n",
            "Epoch #164: Batch 313/313 -- Loss = 1.0938887498923577e-05\n",
            "Epoch #164: Average loss is 3.2883018343963254e-05\n",
            "Epoch #164: Validation F1-score is 0.8715325595120991\n",
            "\n",
            "Epoch #165: Batch 313/313 -- Loss = 2.4475034479110036e-06\n",
            "Epoch #165: Average loss is 2.5401671723484584e-05\n",
            "Epoch #165: Validation F1-score is 0.8710883684314111\n",
            "\n",
            "Epoch #166: Batch 313/313 -- Loss = 4.445961894816719e-05\n",
            "Epoch #166: Average loss is 2.8230526631829073e-05\n",
            "Epoch #166: Validation F1-score is 0.8702290076335877\n",
            "\n",
            "Epoch #167: Batch 313/313 -- Loss = 4.747451384901069e-06\n",
            "Epoch #167: Average loss is 2.308323381877502e-05\n",
            "Epoch #167: Validation F1-score is 0.8693605335425657\n",
            "\n",
            "Epoch #168: Batch 313/313 -- Loss = 5.7423230828135274e-06\n",
            "Epoch #168: Average loss is 1.2353712728085906e-05\n",
            "Epoch #168: Validation F1-score is 0.8693773325476332\n",
            "\n",
            "Epoch #169: Batch 313/313 -- Loss = 5.047524609835818e-06\n",
            "Epoch #169: Average loss is 1.2593193832545192e-05\n",
            "Epoch #169: Validation F1-score is 0.8695993715632364\n",
            "\n",
            "Epoch #170: Batch 313/313 -- Loss = 2.2016195089236135e-06\n",
            "Epoch #170: Average loss is 1.3233972735136989e-05\n",
            "Epoch #170: Validation F1-score is 0.8703667385761914\n",
            "\n",
            "Epoch #171: Batch 313/313 -- Loss = 1.2069890544808004e-06\n",
            "Epoch #171: Average loss is 9.940274836094504e-06\n",
            "Epoch #171: Validation F1-score is 0.8697875688434303\n",
            "\n",
            "Epoch #172: Batch 313/313 -- Loss = 4.563327820505947e-06\n",
            "Epoch #172: Average loss is 1.8201155559216107e-05\n",
            "Epoch #172: Validation F1-score is 0.8669632025879498\n",
            "\n",
            "Epoch #173: Batch 313/313 -- Loss = 0.0025556664913892746\n",
            "Epoch #173: Average loss is 0.014208173879669488\n",
            "Epoch #173: Validation F1-score is 0.8673050615595076\n",
            "\n",
            "Epoch #174: Batch 313/313 -- Loss = 0.002880271291360259\n",
            "Epoch #174: Average loss is 0.018897272883163105\n",
            "Epoch #174: Validation F1-score is 0.8642462509865824\n",
            "\n",
            "Epoch #175: Batch 313/313 -- Loss = 0.00043984828516840935\n",
            "Epoch #175: Average loss is 0.00392749986637621\n",
            "Epoch #175: Validation F1-score is 0.8646616541353384\n",
            "\n",
            "Epoch #176: Batch 313/313 -- Loss = 0.0007064429228194058\n",
            "Epoch #176: Average loss is 0.0038549561886902202\n",
            "Epoch #176: Validation F1-score is 0.8706980361656621\n",
            "\n",
            "Epoch #177: Batch 313/313 -- Loss = 8.303824142785743e-05\n",
            "Epoch #177: Average loss is 0.00398548684183487\n",
            "Epoch #177: Validation F1-score is 0.8672389698542622\n",
            "\n",
            "Epoch #178: Batch 313/313 -- Loss = 0.008407738991081715\n",
            "Epoch #178: Average loss is 0.0027225881456110156\n",
            "Epoch #178: Validation F1-score is 0.8662420382165604\n",
            "\n",
            "Epoch #179: Batch 313/313 -- Loss = 0.0004941937513649464\n",
            "Epoch #179: Average loss is 0.0059620537723761614\n",
            "Epoch #179: Validation F1-score is 0.8659427443237907\n",
            "\n",
            "Epoch #180: Batch 313/313 -- Loss = 8.09236807981506e-05\n",
            "Epoch #180: Average loss is 0.0037102322860326346\n",
            "Epoch #180: Validation F1-score is 0.8689600318154702\n",
            "\n",
            "Epoch #181: Batch 313/313 -- Loss = 0.00219164090231061\n",
            "Epoch #181: Average loss is 0.0009042553665407607\n",
            "Epoch #181: Validation F1-score is 0.8714003944773175\n",
            "\n",
            "Epoch #182: Batch 313/313 -- Loss = 0.005235426593571901\n",
            "Epoch #182: Average loss is 0.0011488351709517018\n",
            "Epoch #182: Validation F1-score is 0.8694656488549618\n",
            "\n",
            "Epoch #183: Batch 313/313 -- Loss = 0.0032134063076227903\n",
            "Epoch #183: Average loss is 0.004579808150711047\n",
            "Epoch #183: Validation F1-score is 0.8717156105100463\n",
            "\n",
            "Epoch #184: Batch 313/313 -- Loss = 1.136942591983825e-05\n",
            "Epoch #184: Average loss is 0.003171383707582389\n",
            "Epoch #184: Validation F1-score is 0.8633726745349071\n",
            "\n",
            "Epoch #185: Batch 313/313 -- Loss = 9.009178029373288e-06\n",
            "Epoch #185: Average loss is 0.0023590304270084406\n",
            "Epoch #185: Validation F1-score is 0.8682078640584864\n",
            "\n",
            "Epoch #186: Batch 313/313 -- Loss = 0.00013771990779787302\n",
            "Epoch #186: Average loss is 0.005474826412443455\n",
            "Epoch #186: Validation F1-score is 0.8573711546144627\n",
            "\n",
            "Epoch #187: Batch 313/313 -- Loss = 0.000883102766238153\n",
            "Epoch #187: Average loss is 0.0066005842541438375\n",
            "Epoch #187: Validation F1-score is 0.8594712780759293\n",
            "\n",
            "Epoch #188: Batch 313/313 -- Loss = 2.202239556936547e-05\n",
            "Epoch #188: Average loss is 0.0008017069982442787\n",
            "Epoch #188: Validation F1-score is 0.8662698412698412\n",
            "\n",
            "Epoch #189: Batch 313/313 -- Loss = 2.1569190721493214e-05\n",
            "Epoch #189: Average loss is 0.00012713978525972052\n",
            "Epoch #189: Validation F1-score is 0.8655956112852665\n",
            "\n",
            "Epoch #190: Batch 313/313 -- Loss = 6.487402970378753e-06\n",
            "Epoch #190: Average loss is 4.731282026982873e-05\n",
            "Epoch #190: Validation F1-score is 0.8658777120315582\n",
            "\n",
            "Epoch #191: Batch 313/313 -- Loss = 1.4326335985970218e-05\n",
            "Epoch #191: Average loss is 2.6548656696483353e-05\n",
            "Epoch #191: Validation F1-score is 0.8647686832740213\n",
            "\n",
            "Epoch #192: Batch 313/313 -- Loss = 1.9123808669974096e-05\n",
            "Epoch #192: Average loss is 2.878830189013079e-05\n",
            "Epoch #192: Validation F1-score is 0.8647898493259318\n",
            "\n",
            "Epoch #193: Batch 313/313 -- Loss = 1.6819190932437778e-05\n",
            "Epoch #193: Average loss is 1.5620349379935704e-05\n",
            "Epoch #193: Validation F1-score is 0.8652931854199684\n",
            "\n",
            "Epoch #194: Batch 313/313 -- Loss = 5.4982588153507095e-06\n",
            "Epoch #194: Average loss is 1.3614965325087976e-05\n",
            "Epoch #194: Validation F1-score is 0.8653351789598576\n",
            "\n",
            "Epoch #195: Batch 313/313 -- Loss = 1.2256164154678117e-06\n",
            "Epoch #195: Average loss is 1.4617212049068448e-05\n",
            "Epoch #195: Validation F1-score is 0.8662698412698412\n",
            "\n",
            "Epoch #196: Batch 313/313 -- Loss = 4.279970198695082e-06\n",
            "Epoch #196: Average loss is 1.0296985514390524e-05\n",
            "Epoch #196: Validation F1-score is 0.8664947431065264\n",
            "\n",
            "Epoch #197: Batch 313/313 -- Loss = 3.1645481612940785e-06\n",
            "Epoch #197: Average loss is 8.452858141478651e-06\n",
            "Epoch #197: Validation F1-score is 0.866891489783773\n",
            "\n",
            "Epoch #198: Batch 313/313 -- Loss = 1.5161804185481742e-06\n",
            "Epoch #198: Average loss is 8.080348052477092e-06\n",
            "Epoch #198: Validation F1-score is 0.8662167526796348\n",
            "\n",
            "Epoch #199: Batch 313/313 -- Loss = 2.6113052626897115e-06\n",
            "Epoch #199: Average loss is 7.805969344556938e-06\n",
            "Epoch #199: Validation F1-score is 0.8670772898867475\n",
            "\n",
            "Epoch #200: Batch 313/313 -- Loss = 8.623988492217904e-07\n",
            "Epoch #200: Average loss is 5.829019242403879e-06\n",
            "Epoch #200: Validation F1-score is 0.866838658463981\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-dd414f8f4f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    \u001b[0mloss_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                    valid_loader=valid_iterator)\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv6IGFO6iIik",
        "colab_type": "text"
      },
      "source": [
        "#### performance on validation & test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgjbKce2ruVO",
        "colab_type": "code",
        "outputId": "438d900c-0f6a-4b10-8537-94d65bb32493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "valid_score = evaluate_sentiment(modelSA, valid_iterator, device)\n",
        "print('Validation performance: %.4f' % valid_score)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix: \n",
            " [[2145  319]\n",
            " [ 352 2184]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.87      0.86      2464\n",
            "         1.0       0.87      0.86      0.87      2536\n",
            "\n",
            "    accuracy                           0.87      5000\n",
            "   macro avg       0.87      0.87      0.87      5000\n",
            "weighted avg       0.87      0.87      0.87      5000\n",
            "\n",
            "Validation performance: 0.8668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Zx-ZCf3Yrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "696616c8-56a6-4b4b-a320-4b23d8cebbfd"
      },
      "source": [
        "test_score = evaluate_sentiment(modelSA, test_iterator, device)\n",
        "print('Test performance: %.4f' % test_score)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix: \n",
            " [[10743  1757]\n",
            " [ 1614 10886]]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.86      0.86     12500\n",
            "         1.0       0.86      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "Test performance: 0.8659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pdl3c2viZGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}