{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCld3t9r8VMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as tdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9dEFUgB948F"
   },
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \"\"\"A vanilla RNN cell\n",
    "    \n",
    "    An RNN cell which takes two inputs: x and previous hidden state\n",
    "    and outputs the current hidden state and output.\n",
    "    \n",
    "    The RNN cell can be used as components for the following designs:\n",
    "    * One-to-one: https://stanford.edu/~shervine/images/rnn-one-to-one.png\n",
    "    * One-to-many: https://stanford.edu/~shervine/images/rnn-one-to-many.png\n",
    "    * Many-to-one: https://stanford.edu/~shervine/images/rnn-many-to-one.png\n",
    "    * Many-to-many\n",
    "        * Same: https://stanford.edu/~shervine/images/rnn-many-to-many-same.png\n",
    "        * Different: https://stanford.edu/~shervine/images/rnn-many-to-many-different.png\n",
    "    * Bidirectional RNNs: https://stanford.edu/~shervine/images/bidirectional-rnn.png\n",
    "    * Deep RNNs: https://stanford.edu/~shervine/images/deep-rnn.png\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: Dimension of input data\n",
    "    output_dim: Dimension of outputs for each input\n",
    "    hidden_dim: Size of hidden state\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, xavier_init=False):\n",
    "        super().__init__()\n",
    "        self.weights_hidden = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.weights_input = nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.weights_output = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.bias_state = nn.Parameter(torch.randn(hidden_dim))\n",
    "        self.bias_output = nn.Parameter(torch.randn(output_dim))\n",
    "        self.g1 = nn.Sigmoid()\n",
    "        self.g2 = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # Design from: \n",
    "        #    https://stanford.edu/~shervine/images/description-block-rnn.png\n",
    "        #    https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "        \n",
    "        # W_{ax}x^t\n",
    "        input_actv = torch.matmul(x, self.weights_input)\n",
    "        # W_{aa}a^{t-1}\n",
    "        old_state_actv = torch.matmul(hidden, self.weights_hidden)\n",
    "        # updated hidden state (new state)\n",
    "        # g_1(W_{aa}a^{t-1} + W_{ax}x^t + b_a)\n",
    "        hidden = self.g1(old_state_actv + input_actv + self.bias_state)\n",
    "        \n",
    "        # W_{ya}a^t + b_y\n",
    "        output_actv = torch.matmul(hidden, self.weights_output)\n",
    "        # g_2(W_{ya}a^t + b_y)\n",
    "        output = self.g2(output_actv + self.bias_output)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNCell(512, 2, 256)\n",
    "inputs = torch.randn(3,5,512)\n",
    "hidden = torch.randn(1, 5, 256)\n",
    "o, h = rnn(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 512])\n",
      "torch.Size([3, 5, 2])\n",
      "torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(o.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"A vanilla LSTM cell       \n",
    "    \n",
    "    An LSTM cell which takes two inputs: x and previous hidden state\n",
    "    and outputs the current cell state and hidden state.\n",
    "    Conditionally, an output cell can also be learnt where the final\n",
    "    hidden state is reprojected to the required dimension.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: Dimension of input data\n",
    "    hidden_dim: Size of hidden state\n",
    "    output_dim: Dimension of outputs for each input\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=None, xavier_init=False):\n",
    "        super().__init__()\n",
    "        dim_size = input_dim + hidden_dim\n",
    "        if xavier_init:\n",
    "            n = (dim_size + hidden_dim) / 2\n",
    "            self.initialize_with_Xavier(n, dim_size, hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.weights_forget = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
    "            self.bias_forget = nn.Parameter(torch.randn(hidden_dim))\n",
    "            self.weights_input = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
    "            self.bias_input = nn.Parameter(torch.randn(hidden_dim))\n",
    "            self.weights_candidate = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
    "            self.bias_candidate = nn.Parameter(torch.randn(hidden_dim))\n",
    "            self.weights_output = nn.Parameter(torch.randn(dim_size, hidden_dim))\n",
    "            self.bias_output = nn.Parameter(torch.randn(hidden_dim))\n",
    "            if output_dim is not None:\n",
    "                self.output_projection = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "                self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        self.g1 = nn.Sigmoid()\n",
    "        self.g2 = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, hidden_state, old_state):\n",
    "        # Design and notations from:\n",
    "        #    https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "        \n",
    "        # h^{t-1}x^t\n",
    "        concat_inputs = torch.cat((x, hidden_state), dim=2)\n",
    "        \n",
    "        # Forget gate: $\\Gamma_f = \\sigma(W_f.[h_{t-1}, x_t] + b_f)$\n",
    "        # Determines what(or how much) to throw away from old state\n",
    "        forget_g = self.g1(torch.matmul(concat_inputs, self.weights_forget) + \n",
    "                           self.bias_forget)\n",
    "        \n",
    "        # Input gate: $\\Gamma_i = \\sigma(W_i.[h_{t-1}, x_t] + b_i)$\n",
    "        # Decides which part of input is to be added\n",
    "        input_g = self.g1(torch.matmul(concat_inputs, self.weights_input) + \n",
    "                          self.bias_input)\n",
    "        \n",
    "        # Current candidate: $\\widetilde{C_t} = tanh(W_C.[h_{t-1}, x_t] + b_C)$\n",
    "        # Potential candidate to update the state with\n",
    "        candidate_new = self.g2(torch.matmul(concat_inputs, self.weights_candidate) + \n",
    "                                self.bias_candidate)\n",
    "        \n",
    "        # New state: $C_t = f_t * C_{t-1} + i_t * \\widetilde{C_t}$\n",
    "        new_state = torch.mul(forget_g, old_state) + \\\n",
    "                    torch.mul(input_g, candidate_new)\n",
    "        \n",
    "        # Output gate: $\\Gamma_o = \\sigma(W_o.[h_{t-1}, x_t] + b_o)$\n",
    "        # Determines what to output based on current input and\n",
    "        #    previous hidden state\n",
    "        output_g = self.g1(torch.matmul(concat_inputs, self.weights_output) + \n",
    "                           self.bias_output)\n",
    "        # New hidden state/output: $h_t = \\Gamma_o * tanh(C_t)$\n",
    "        hidden_state = torch.mul(output_g, self.g2(new_state))\n",
    "        \n",
    "        output = None\n",
    "        if hasattr(self, 'output_projection') and hasattr(self, 'softmax'):\n",
    "            output = self.softmax(torch.matmul(hidden_state, self.output_projection))\n",
    "        \n",
    "        return new_state, hidden_state, output\n",
    "    \n",
    "    def initialize_with_Xavier(self, n_var, dim_size, hidden_dim, output_dim):\n",
    "        gauss = tdist.Normal(0, 1 / np.sqrt(n_var))\n",
    "        self.weights_forget = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
    "        self.bias_forget = nn.Parameter(gauss.sample((hidden_dim,)))\n",
    "        self.weights_input = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
    "        self.bias_input = nn.Parameter(gauss.sample((hidden_dim,)))\n",
    "        self.weights_candidate = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
    "        self.bias_candidate = nn.Parameter(gauss.sample((hidden_dim,)))\n",
    "        self.weights_output = nn.Parameter(gauss.sample((dim_size, hidden_dim)))\n",
    "        self.bias_output = nn.Parameter(gauss.sample((hidden_dim,)))\n",
    "        if output_dim is not None:\n",
    "            self.output_projection = nn.Parameter(gauss.sample((hidden_dim, output_dim)))\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM without Output cell and Xavier initialization\n",
    "\n",
    "lstm = LSTMCell(512, 256)\n",
    "batch = 2\n",
    "inputs = torch.randn(3, batch, 512)\n",
    "hidden = torch.randn(1, batch, 256)\n",
    "state = torch.randn(256)\n",
    "new_state, h, _ = lstm(inputs[0].unsqueeze(0), hidden, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n",
      "torch.Size([1, 2, 256])\n",
      "torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].unsqueeze(0).shape)\n",
    "print(new_state.shape)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with Output cell and Xavier initialization\n",
    "\n",
    "lstm = LSTMCell(512, 256, 2, True)\n",
    "batch = 5\n",
    "inputs = torch.randn(3, batch, 512)\n",
    "hidden = torch.randn(1, batch, 256)\n",
    "state = torch.randn(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2])\n",
      "torch.Size([1, 5, 256])\n",
      "torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "new_state, h, o = lstm(inputs[0].unsqueeze(0), hidden, state)\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "print(new_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 0]])\n",
      "tensor([1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(o, dim=2))\n",
    "gt = (torch.ones(batch).long() - torch.argmax(o, dim=2)[0].long())\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7407665252685547\n"
     ]
    }
   ],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss = loss_criterion(o.view(batch, -1), gt)\n",
    "print(\"Loss: {}\".format(loss.item()))\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, ncell, input_dim, hidden_dim, output_dim, \n",
    "                 bidirectional=False, xavier_init=True):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Creating architecture for Left-to-Right Sequence\n",
    "        archi = []\n",
    "        for i in range(ncell):\n",
    "            archi.append(LSTMCell(input_dim, hidden_dim, \n",
    "                                  None, xavier_init))\n",
    "        self.archi = nn.ModuleList(archi)\n",
    "        self.final_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2) # use nn.LogSoftmax?\n",
    "        \n",
    "        if bidirectional:\n",
    "            # Creating architecture for Right-to-Left Sequence\n",
    "            archi_rev = []\n",
    "            for i in range(ncell):\n",
    "                archi_rev.append(LSTMCell(input_dim, hidden_dim, \n",
    "                                          None, xavier_init))\n",
    "            self.archi_rev = nn.ModuleList(archi_rev)\n",
    "            self.final_layer = nn.Linear(2 * hidden_dim, output_dim)               \n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        history = len(self.archi)\n",
    "        seq_length = x.shape[0]\n",
    "        hidden_state_rev = hidden_state.clone()\n",
    "        cell_state_rev = cell_state.clone()\n",
    "        assert(seq_length >= history)\n",
    "        # Forward pass for Left-to-Right sequence\n",
    "        for i in range(seq_length - history + 1):\n",
    "            for j in range(history):\n",
    "                inputs = x[i+j].unsqueeze(0)\n",
    "                cell_state, hidden_state, _ = self.archi[j](inputs, \n",
    "                                                            hidden_state, \n",
    "                                                            cell_state)\n",
    "        if self.bidirectional:\n",
    "            # Forward pass for Right-to-Left sequence\n",
    "            for i in range(seq_length - history + 1):\n",
    "                for j in range(history):\n",
    "                    inputs = x[i+j].unsqueeze(0)\n",
    "                    cell_state_rev, \n",
    "                    hidden_state_rev,\n",
    "                    _ = self.archi_rev[history-j-1](inputs, hidden_state_rev, cell_state_rev)\n",
    "            hidden_state = torch.cat((hidden_state, hidden_state_rev), dim=-1)\n",
    "        \n",
    "        output = self.softmax(self.final_layer(hidden_state))\n",
    "        return cell_state, hidden_state, output\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = 4\n",
    "# seq_length = 10\n",
    "# for i in range(seq_length - history + 1):\n",
    "#     for j in range(history):\n",
    "#         print(i+j, end=\" \")\n",
    "#     print()\n",
    "# print()\n",
    "# archi = [\"archi_1\", \"archi_2\", \"archi_3\", \"archi_4\"]\n",
    "# for i in range(0, seq_length - history + 1)[::-1]:\n",
    "#     for j in range(history)[::-1]:\n",
    "#         print(i+j, end=\" \")\n",
    "#         #print(\"{} {}\".format(history-j-1, i+j), end=\"\\t\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, batch_size, optimizer, epochs):\n",
    "    \"\"\"\n",
    "    Trains the model of class LSTM using data from data_loader passed as argument\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    model: object of class LSTM\n",
    "    data_loader: an object containing data and functions to sample batches\n",
    "        The sample() function of the object returns two tensors\n",
    "        x - which is a 3D tensor of [sequence, batch, dim]\n",
    "            Example-A batch of 10 sentences of 5 words each where each word has\n",
    "            an embedding vector of size 256, the 3D tensor shape will be [5, 10, 256]\n",
    "        y - a 1D tensor containing the classes as integers (torch.long())\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    Trained model object of class LSTM\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_list = []\n",
    "    for i in range(epochs):\n",
    "        loss_tracker = []\n",
    "        for i, x, y in enumerate(data_loader.sample_batch(batch_size)):\n",
    "            hidden_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
    "            cell_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
    "            cell_state, hidden_state, output = lstm(x=x_train, \n",
    "                                                    hidden_state=hidden_state, \n",
    "                                                    cell_state=cell_state)\n",
    "            loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
    "            print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, int(x_size/batch_size), \n",
    "                                                               loss.item()), end='\\r')\n",
    "            loss_tracker.append(loss.item())\n",
    "            # backward pass for the batch (+ weight updates)\n",
    "            loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_list.append(np.mean(loss_tracker))\n",
    "        loss_tracker = []\n",
    "        print(\"\\nEpoch #{}: Average loss is {}\".format(i+1, loss_list[-1]))\n",
    "        print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def evaluate(model, test_data, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model of class LSTM using test data passed\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    model: object of class LSTM\n",
    "    test_data: a tuple containing (x, y) \n",
    "        x - a 3D tensor of [sequence, len_test_data, dim]\n",
    "            Example-A batch of 10 sentences of 5 words each where each word has\n",
    "            an embedding vector of size 256, the 3D tensor shape will be [5, 10, 256]\n",
    "        y - a 1D tensor containing the classes as integers (torch.long())\n",
    "    verbose: prints the confusion matrix and F-score\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    F-score (float)\n",
    "    \n",
    "    \"\"\"\n",
    "    x, y = test_data\n",
    "    batch_size = test_data.shape[1]\n",
    "    hidden_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
    "    cell_state = torch.zeros(1, batch_size, model.hidden_dim)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, output = lstm(x=x_train, hidden_state=hidden_state, cell_state=cell_state)   \n",
    "    if verbose:\n",
    "        print(confusion_matrix(y, torch.argmax(output, 2)[0].numpy()))\n",
    "        print(f1_score(y, torch.argmax(output, 2)[0].numpy()))\n",
    "    return f1_score(y, torch.argmax(output, 2)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 2, (1000,100)).float()\n",
    "y = torch.sum(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "seq_length = 100\n",
    "hidden_dim = 50\n",
    "output_dim = 100\n",
    "lstm = LSTMCell(input_dim=input_dim, hidden_dim=hidden_dim, \n",
    "                output_dim=output_dim, xavier_init=True)\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "loss_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #100: Batch 100/100 -- Loss = 4.553095817565918\r"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "batch_size = 10\n",
    "# for i in range(10):\n",
    "for j in range(int(1000/batch_size)):\n",
    "    start = j * batch_size\n",
    "    end = min((j+1) * batch_size, 1000)\n",
    "    x_train = torch.tensor([]).float()\n",
    "    y_train = y[start:end]\n",
    "    # creating the 3D tensor for the batch: seq x batch x dim\n",
    "    for data in x[start:end]:\n",
    "        x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
    "    # initializing states\n",
    "    hidden_state = torch.zeros(1, batch_size, hidden_dim)\n",
    "    cell_state = torch.zeros(1, batch_size, hidden_dim)\n",
    "    # forward pass for the batch\n",
    "    for i, seq_i in enumerate(x_train):        \n",
    "        cell_state, hidden_state, output = lstm(x=seq_i.unsqueeze(0), \n",
    "                                                hidden_state=hidden_state, \n",
    "                                                old_state=cell_state)\n",
    "    loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
    "    print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, int(1000/batch_size), \n",
    "                                                       loss.item()), end='\\r')\n",
    "    # backward pass for the batch (+ weight updates)\n",
    "    loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[51, 51, 51, 51, 51, 51, 51, 51, 51, 51]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(output, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying product as positive/negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41702\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1\n",
    "seq_length = 10\n",
    "hidden_dim = 50\n",
    "output_dim = 2\n",
    "x_size = 1000\n",
    "\n",
    "x = torch.randn((x_size, seq_length)).float()\n",
    "y = torch.prod(x, 1)\n",
    "y[y >= 0] = 1\n",
    "y[y < 0] = 0\n",
    "# print(torch.histc(y, bins=2))\n",
    "\n",
    "lstm = LSTM(ncell=4, input_dim=input_dim, hidden_dim=hidden_dim, \n",
    "            output_dim=output_dim, bidirectional=False, xavier_init=True)\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "print(lstm.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of batch creating and forward pass\n",
    "\n",
    "# x_train = x\n",
    "# y_train = y\n",
    "\n",
    "# batch_size = 4\n",
    "# j = 0\n",
    "# start = j * batch_size\n",
    "# end = min((j+1) * batch_size, 1000)\n",
    "\n",
    "# x_train = torch.tensor([]).float()\n",
    "# y_train = y[start:end]\n",
    "# # creating the 3D tensor for the batch: seq x batch x dim\n",
    "# for data in x[start:end]:\n",
    "#     x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
    "\n",
    "# # initializing states\n",
    "# hidden_state = torch.zeros(1, batch_size, hidden_dim)\n",
    "# cell_state = torch.zeros(1, batch_size, hidden_dim)\n",
    "\n",
    "# print(x_train.shape, y_train.shape)\n",
    "# print(hidden_state.shape, cell_state.shape)\n",
    "\n",
    "# s,h,o = lstm(x_train, hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: Batch 250/250 -- Loss = 0.6912937760353088\n",
      "Epoch #1: Average loss is 0.6943509202003479\n",
      "\n",
      "Epoch #2: Batch 152/250 -- Loss = 0.7382816076278687\r"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "# Each batch passed to LSTM() is expected to be a 3D Tensor where\n",
    "# the dimensions describe [sequence_size, batch_size, dim_size]\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "for i in range(10):\n",
    "    loss_tracker = []\n",
    "    for j in range(int(x_size/batch_size)):\n",
    "        start = j * batch_size\n",
    "        end = min((j+1) * batch_size, x_size)\n",
    "        x_train = torch.tensor([]).float()\n",
    "        y_train = y[start:end]\n",
    "        # creating the 3D tensor for the batch: seq x batch x dim\n",
    "        for data in x[start:end]:\n",
    "            x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
    "        # initializing states\n",
    "        hidden_state = torch.zeros(1, batch_size, hidden_dim)\n",
    "        cell_state = torch.zeros(1, batch_size, hidden_dim)\n",
    "        cell_state, hidden_state, output = lstm(x=x_train, \n",
    "                                                hidden_state=hidden_state, \n",
    "                                                cell_state=cell_state)\n",
    "        loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
    "        print(\"Epoch #{}: Batch {}/{} -- Loss = {}\".format(i+1, j+1, int(x_size/batch_size), \n",
    "                                                           loss.item()), end='\\r')\n",
    "        loss_tracker.append(loss.item())\n",
    "        # backward pass for the batch (+ weight updates)\n",
    "        loss = loss_criterion(output.view(batch_size, -1), y_train.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"\\nEpoch #{}: Average loss is {}\".format(i+1, np.mean(loss_tracker)))\n",
    "    loss_tracker = []\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000, 1])\n",
      "torch.Size([1000])\n",
      "[[434  55]\n",
      " [ 72 439]]\n",
      "0.8736318407960199\n"
     ]
    }
   ],
   "source": [
    "# Validating on entire generated data\n",
    "\n",
    "x_train = torch.tensor([]).float()\n",
    "y_train = y\n",
    "# creating the 3D tensor for the batch: seq x batch x dim\n",
    "for data in x:\n",
    "    x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
    "print(x_train.shape)\n",
    "print(y.shape)\n",
    "hidden_state = torch.zeros(1, x_train.shape[1], hidden_dim)\n",
    "cell_state = torch.zeros(1, x_train.shape[1], hidden_dim)\n",
    "_, _, output = lstm(x=x_train, hidden_state=hidden_state, cell_state=cell_state)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, torch.argmax(output, 2)[0].numpy()))\n",
    "print(f1_score(y, torch.argmax(output, 2)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000, 1])\n",
      "torch.Size([1000])\n",
      "[[258 231]\n",
      " [254 257]]\n",
      "0.5145145145145146\n"
     ]
    }
   ],
   "source": [
    "# Validating on newly generated test data\n",
    "\n",
    "x_test = torch.randn((x_size, seq_length)).float()\n",
    "y_test = torch.prod(x_test, 1)\n",
    "y_test[y_test >= 0] = 1\n",
    "y_test[y_test < 0] = 0\n",
    "\n",
    "x_train = torch.tensor([]).float()\n",
    "y_train = y_test\n",
    "# creating the 3D tensor for the batch: seq x batch x dim\n",
    "for data in x_test:\n",
    "    x_train = torch.cat((x_train, data.view(seq_length, 1, 1)), dim=1)\n",
    "print(x_train.shape)\n",
    "print(y.shape)\n",
    "hidden_state = torch.zeros(1, x_train.shape[1], hidden_dim)\n",
    "cell_state = torch.zeros(1, x_train.shape[1], hidden_dim)\n",
    "_, _, output = lstm(x=x_train, hidden_state=hidden_state, cell_state=cell_state)\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(y, torch.argmax(output, 2)[0].numpy()))\n",
    "print(f1_score(y, torch.argmax(output, 2)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "#   author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "#   title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "#   booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "#   month     = {June},\n",
    "#   year      = {2011},\n",
    "#   address   = {Portland, Oregon, USA},\n",
    "#   publisher = {Association for Computational Linguistics},\n",
    "#   pages     = {142--150},\n",
    "#   url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../img/sentiment.png') \n",
    "# source:(https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
