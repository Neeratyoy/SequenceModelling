{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy(-reverse) dataset\n",
    "\n",
    "The entire dataset comprises of the binary representation of all numbers uptil a range defined. The binary sequence from left to right (most significant to least significant) is the input. The target is just the reverse sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Generating data\n",
    "state_size = 12\n",
    "data_x = []\n",
    "for i in range(pow(2, state_size)):\n",
    "    data_x.append([int(x) for x in list(np.binary_repr(i, width=state_size))])\n",
    "data_x = np.array(data_x)\n",
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 4096, 2]), torch.Size([12, 4096, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping for tensors\n",
    "data_x = np.transpose(data_x).reshape(state_size, pow(2, state_size), 1)\n",
    "data_x = torch.from_numpy(data_x).float()\n",
    "data_x = torch.zeros(data_x.shape[0], data_x.shape[1], 2).scatter_(2, data_x.long(), 1)\n",
    "data_y = data_x.clone()\n",
    "data_x.shape, data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute for reverse-copy (comment for copy-task)\n",
    "data_y = torch.flip(data_y, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3072, 2]) torch.Size([12, 3072, 2]) torch.Size([12, 1024, 2]) torch.Size([12, 1024, 2])\n"
     ]
    }
   ],
   "source": [
    "# Creating training and test sets\n",
    "train_size = 0.75\n",
    "ordering = torch.randperm(pow(2, state_size))\n",
    "data_x = data_x[:, ordering, :]\n",
    "data_y = data_y[:, ordering, :]\n",
    "train_x = data_x[:,:int(train_size * len(ordering)),:]\n",
    "train_y = data_y[:,:int(train_size * len(ordering)),:]\n",
    "test_x = data_x[:,int(train_size * len(ordering)):,:]\n",
    "test_y = data_y[:,int(train_size * len(ordering)):,:]\n",
    "\n",
    "# Creating training and validation sets\n",
    "## TODO\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dim\n",
    "input_dim = 2\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 16\n",
    "# Number of output nodes\n",
    "output_dim = 2\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = True\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False\n",
    "\n",
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if train_x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    layers = model.layers\n",
    "    hidden_dim = model.hidden_dim\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[:,ordering,:]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            if model.bidirectional:\n",
    "                hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            o = model(train_x[:,start:end,:], train_y[:,start:end,:], hidden_state, cell_state)\n",
    "            gt = torch.argmax(train_y[:,start:end,:], 2, keepdim=True).view(-1)\n",
    "            loss = loss_fn(o.view(-1, 2), gt)\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size), \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate(model, train_x, train_y)\n",
    "        f1_test = evaluate(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    layers = model.layers\n",
    "    hidden_dim = model.hidden_dim\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if model.bidirectional:\n",
    "            hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], y[:,start:end,:], hidden_state, cell_state)\n",
    "        pred = torch.argmax(o, 2, keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        label = torch.argmax(y[:,start:end,:], 2, \n",
    "                             keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        labels.extend(label)\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "\n",
    "class LSTMSeq2SeqDifferent(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-many-different)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers=1,\n",
    "                 bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.encoder = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = LSTM(input_dim=output_dim, hidden_dim=2 * hidden_dim, layers=layers,\n",
    "                                bidirectional=False, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = LSTM(input_dim=output_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                                bidirectional=False, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, target, hidden_state, cell_state, teacher_forcing=0.5):\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        # encoding\n",
    "        _, (hidden_state, cell_state) = self.encoder(x, hidden_state, cell_state)\n",
    "        batch_size = x.shape[1]\n",
    "        timesteps = x.shape[0]\n",
    "        x = torch.zeros(1, batch_size, self.output_dim).to(device)\n",
    "        output = torch.tensor([]).to(device)\n",
    "        if self.bidirectional:\n",
    "            # concatenating hidden states from two directions\n",
    "            hidden_state = torch.cat((hidden_state[:self.layers,:,:], \n",
    "                                      hidden_state[self.layers:,:,:]), dim=2)\n",
    "            cell_state = torch.cat((cell_state[:self.layers,:,:], \n",
    "                                    cell_state[self.layers:,:,:]), dim=2)\n",
    "        # decoding\n",
    "        for t in range(timesteps):           \n",
    "            x, (hidden_state, cell_state) = self.decoder(x, hidden_state, cell_state)            \n",
    "            x = self.softmax(self.fc(x))\n",
    "            output = torch.cat((output, x), dim=0)\n",
    "            choice = random.random() \n",
    "            if choice < teacher_forcing:\n",
    "                x = target[t].float().to(device)\n",
    "                x = x.unsqueeze(0)\n",
    "            else:\n",
    "                # converting x to a one-hot encoding\n",
    "                x = torch.zeros(x.shape).to(device).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6978\n"
     ]
    }
   ],
   "source": [
    "our = LSTMSeq2SeqDifferent(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(our.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(our.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.60726\n",
      "Average Loss: 0.647137\n",
      "Training F1: 0.7007\n",
      "Test F1: 0.6961\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.52643\n",
      "Average Loss: 0.566537\n",
      "Training F1: 0.7543\n",
      "Test F1: 0.7506\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.45041\n",
      "Average Loss: 0.526103\n",
      "Training F1: 0.7981\n",
      "Test F1: 0.7925\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.39401\n",
      "Average Loss: 0.468756\n",
      "Training F1: 0.9251\n",
      "Test F1: 0.9238\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.33727\n",
      "Average Loss: 0.378204\n",
      "Training F1: 0.9961\n",
      "Test F1: 0.9961\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.32612\n",
      "Average Loss: 0.332563\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9978\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.32172\n",
      "Average Loss: 0.32524\n",
      "Training F1: 0.9997\n",
      "Test F1: 0.9996\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.31742\n",
      "Average Loss: 0.321698\n",
      "Training F1: 0.9997\n",
      "Test F1: 0.9993\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.31459\n",
      "Average Loss: 0.318069\n",
      "Training F1: 0.9999\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.31796\n",
      "Average Loss: 0.32102\n",
      "Training F1: 0.9996\n",
      "Test F1: 0.9999\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.31465\n",
      "Average Loss: 0.322494\n",
      "Training F1: 0.9998\n",
      "Test F1: 0.9996\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.31521\n",
      "Average Loss: 0.314666\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.31399\n",
      "Average Loss: 0.314217\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.32897\n",
      "Average Loss: 0.328763\n",
      "Training F1: 0.9746\n",
      "Test F1: 0.9752\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.31401\n",
      "Average Loss: 0.322068\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.31368\n",
      "Average Loss: 0.313893\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.38329\n",
      "Average Loss: 0.315623\n",
      "Training F1: 0.9079\n",
      "Test F1: 0.9043\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.31364\n",
      "Average Loss: 0.347365\n",
      "Training F1: 0.9999\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.31355\n",
      "Average Loss: 0.313786\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.31359\n",
      "Average Loss: 0.313611\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.38337\n",
      "Average Loss: 0.319268\n",
      "Training F1: 0.9852\n",
      "Test F1: 0.9838\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.31354\n",
      "Average Loss: 0.320776\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.31344\n",
      "Average Loss: 0.313536\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.31359\n",
      "Average Loss: 0.313468\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.31333\n",
      "Average Loss: 0.313422\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.31336\n",
      "Average Loss: 0.313394\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.34564\n",
      "Average Loss: 0.347253\n",
      "Training F1: 0.9487\n",
      "Test F1: 0.948\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.31349\n",
      "Average Loss: 0.325951\n",
      "Training F1: 0.9999\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.31353\n",
      "Average Loss: 0.313503\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.31351\n",
      "Average Loss: 0.313424\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMSeq2SeqDifferent(\n",
       "  (encoder): LSTM(\n",
       "    (model): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (model_rev): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): LSTM(\n",
       "    (model): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(our, train_x, train_y, test_x, test_y, epochs=30, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchBaseline(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-many-different)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers=1,\n",
    "                 bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                         bidirectional=bidirectional) #, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.LSTM(input_size=output_dim, hidden_size=2 * hidden_dim, num_layers=layers,\n",
    "                                bidirectional=False) #, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(input_size=output_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                                bidirectional=False) #, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, target, hidden_state, cell_state, teacher_forcing=0.5):\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        # encoding\n",
    "        _, (hidden_state, cell_state) = self.encoder(x, (hidden_state, cell_state))\n",
    "        batch_size = x.shape[1]\n",
    "        timesteps = x.shape[0]\n",
    "        x = torch.zeros(1, batch_size, self.output_dim).to(device)\n",
    "        output = torch.tensor([]).to(device)\n",
    "        if self.bidirectional:\n",
    "            # concatenating hidden states from two directions\n",
    "            hidden_state = torch.cat((hidden_state[:self.layers,:,:], \n",
    "                                      hidden_state[self.layers:,:,:]), dim=2)\n",
    "            cell_state = torch.cat((cell_state[:self.layers,:,:], \n",
    "                                    cell_state[self.layers:,:,:]), dim=2)\n",
    "        # decoding\n",
    "        for t in range(timesteps):           \n",
    "            x, (hidden_state, cell_state) = self.decoder(x, (hidden_state, cell_state))\n",
    "            x = self.softmax(self.fc(x))\n",
    "            output = torch.cat((output, x), dim=0)\n",
    "            choice = random.random() \n",
    "            if choice < teacher_forcing:\n",
    "                x = target[t].float().to(device)\n",
    "                x = x.unsqueeze(0)\n",
    "            else:\n",
    "                # converting x to a one-hot encoding\n",
    "                x = torch.zeros(x.shape).to(device).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7234\n"
     ]
    }
   ],
   "source": [
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, layers, bidirectional).to(device)\n",
    "print(pytorch.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pytorch.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.54562\n",
      "Average Loss: 0.537935\n",
      "Training F1: 0.7931\n",
      "Test F1: 0.7901\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.49842\n",
      "Average Loss: 0.508572\n",
      "Training F1: 0.8193\n",
      "Test F1: 0.8151\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.42781\n",
      "Average Loss: 0.478206\n",
      "Training F1: 0.8622\n",
      "Test F1: 0.8674\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.41771\n",
      "Average Loss: 0.437634\n",
      "Training F1: 0.9015\n",
      "Test F1: 0.8992\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.33955\n",
      "Average Loss: 0.382576\n",
      "Training F1: 0.9926\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.32522\n",
      "Average Loss: 0.33769\n",
      "Training F1: 0.9976\n",
      "Test F1: 0.9974\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.32216\n",
      "Average Loss: 0.327912\n",
      "Training F1: 0.9971\n",
      "Test F1: 0.9974\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.31649\n",
      "Average Loss: 0.319896\n",
      "Training F1: 0.9998\n",
      "Test F1: 0.9998\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.31637\n",
      "Average Loss: 0.317914\n",
      "Training F1: 0.9998\n",
      "Test F1: 0.9998\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.32598\n",
      "Average Loss: 0.320436\n",
      "Training F1: 0.9913\n",
      "Test F1: 0.9891\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.31875\n",
      "Average Loss: 0.318973\n",
      "Training F1: 0.9991\n",
      "Test F1: 0.9992\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.31431\n",
      "Average Loss: 0.314791\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.31396\n",
      "Average Loss: 0.314212\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.31424\n",
      "Average Loss: 0.322698\n",
      "Training F1: 0.9998\n",
      "Test F1: 0.9997\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.31392\n",
      "Average Loss: 0.31882\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.31363\n",
      "Average Loss: 0.314309\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9998\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.31364\n",
      "Average Loss: 0.313664\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.31352\n",
      "Average Loss: 0.313591\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.31362\n",
      "Average Loss: 0.313513\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.31343\n",
      "Average Loss: 0.31347\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.31442\n",
      "Average Loss: 0.406838\n",
      "Training F1: 0.9945\n",
      "Test F1: 0.9938\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.31366\n",
      "Average Loss: 0.315172\n",
      "Training F1: 0.9999\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.31337\n",
      "Average Loss: 0.313872\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.31347\n",
      "Average Loss: 0.313503\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.31341\n",
      "Average Loss: 0.313454\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.31337\n",
      "Average Loss: 0.31342\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.31333\n",
      "Average Loss: 0.313397\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.31332\n",
      "Average Loss: 0.313377\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.31336\n",
      "Average Loss: 0.313364\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.33952\n",
      "Average Loss: 0.429226\n",
      "Training F1: 0.9802\n",
      "Test F1: 0.9771\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyTorchBaseline(\n",
       "  (encoder): LSTM(2, 16, bidirectional=True)\n",
       "  (decoder): LSTM(2, 32)\n",
       "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(pytorch, train_x, train_y, test_x, test_y, epochs=30, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "==================\n",
      "# of parameters: 6978\n",
      "encoder.model.0.weights  : torch.Size([18, 64])\n",
      "encoder.model.0.bias     : torch.Size([64])\n",
      "encoder.model_rev.0.weights: torch.Size([18, 64])\n",
      "encoder.model_rev.0.bias : torch.Size([64])\n",
      "decoder.model.0.weights  : torch.Size([34, 128])\n",
      "decoder.model.0.bias     : torch.Size([128])\n",
      "fc.weight                : torch.Size([2, 32])\n",
      "fc.bias                  : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\\n{}\".format(\"=\" * len(\"Our implementation\")))\n",
    "print(\"# of parameters: {}\".format(our.count_parameters()))\n",
    "for name, param in our.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch implementation\n",
      "======================\n",
      "# of parameters: 7234\n",
      "encoder.weight_ih_l0          : torch.Size([64, 2])\n",
      "encoder.weight_hh_l0          : torch.Size([64, 16])\n",
      "encoder.bias_ih_l0            : torch.Size([64])\n",
      "encoder.bias_hh_l0            : torch.Size([64])\n",
      "encoder.weight_ih_l0_reverse  : torch.Size([64, 2])\n",
      "encoder.weight_hh_l0_reverse  : torch.Size([64, 16])\n",
      "encoder.bias_ih_l0_reverse    : torch.Size([64])\n",
      "encoder.bias_hh_l0_reverse    : torch.Size([64])\n",
      "decoder.weight_ih_l0          : torch.Size([128, 2])\n",
      "decoder.weight_hh_l0          : torch.Size([128, 32])\n",
      "decoder.bias_ih_l0            : torch.Size([128])\n",
      "decoder.bias_hh_l0            : torch.Size([128])\n",
      "fc.weight                     : torch.Size([2, 32])\n",
      "fc.bias                       : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch implementation\\n{}\".format(\"=\" * len(\"PyTorch implementation\")))\n",
    "print(\"# of parameters: {}\".format(pytorch.count_parameters()))\n",
    "for name, param in pytorch.named_parameters():\n",
    "    print(\"{:<30}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses $Wh + b_h + Wx + b_x$ whereas we are using $Wx' + b$, where $x'$ is $h, x$ concatenated. Therefore PyTorch has an extra set of biases for each direction for the encoder and also for the decoder.\n",
    "\n",
    "For one direction - 64 <br>\n",
    "For reverse direction - 64 <br>\n",
    "For the decoder - 128 <br>\n",
    "\n",
    "Our model has $2465$ parameters while the PyTorch model has $2465 + 64 + 64 = 2593$ parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
