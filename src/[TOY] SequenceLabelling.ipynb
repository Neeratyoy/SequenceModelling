{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR task\n",
    "\n",
    "The entire dataset comprises of the binary representation of all numbers uptil a range defined. The binary sequence from left to right (most significant to least significant) is the input. While the y or the output for an input is calculated as: $a1 \\oplus a10 \\wedge a3 \\oplus a7$. Where, the most significant bit is a1, the least significant bit is a10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "state_size = 10\n",
    "data_x = []\n",
    "for i in range(pow(2, state_size)):\n",
    "    data_x.append([int(x) for x in list(np.binary_repr(i, width=state_size))])\n",
    "data_x = np.array(data_x)\n",
    "\n",
    "data_y = []\n",
    "for x in data_x:\n",
    "    # a1 xor a10 ^ a3 xor a7\n",
    "    data_y.append(np.bitwise_and(np.bitwise_xor(x[0], x[9]), \n",
    "                                 np.bitwise_xor(x[2], x[6])))\n",
    "data_y = np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping for tensors\n",
    "data_x = np.transpose(data_x).reshape(state_size, pow(2, state_size), 1)\n",
    "data_x = torch.from_numpy(data_x).float()\n",
    "data_y = torch.from_numpy(data_y).float()\n",
    "\n",
    "# Reshaping X to 2-input dimensions\n",
    "data_x = torch.zeros(data_x.shape[0], data_x.shape[1], 2).scatter_(2, data_x.long(), 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test sets\n",
    "train_size = 0.7\n",
    "ordering = torch.randperm(pow(2, state_size))\n",
    "data_x = data_x[:, ordering, :]\n",
    "data_y = data_y[ordering]\n",
    "train_x = data_x[:,:int(train_size * len(ordering)),:]\n",
    "train_y = data_y[:int(train_size * len(ordering))]\n",
    "test_x = data_x[:,int(train_size * len(ordering)):,:]\n",
    "test_y = data_y[int(train_size * len(ordering)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 716, 2]) torch.Size([716]) torch.Size([10, 308, 2]) torch.Size([308])\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dim\n",
    "input_dim = 2\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 16\n",
    "# Number of output nodes\n",
    "output_dim = 1\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = True\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False\n",
    "\n",
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[ordering]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            if model.bidirectional:\n",
    "                hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            o = model(train_x[:,start:end,:], hidden_state, cell_state)\n",
    "            loss = loss_fn(o.view(-1), train_y[start:end])\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size) + 1, \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate(model, train_x, train_y)\n",
    "        f1_test = evaluate(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if model.bidirectional:\n",
    "            hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], hidden_state, cell_state)\n",
    "        pred = torch.round(torch.sigmoid(o.view(-1))).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        labels.extend(y[start:end].int().detach().cpu().numpy())\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTMCell\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A complete LSTM architecture\n",
    "\n",
    "    Allows to stack multiple LSTM cells and also\n",
    "    create a bidirectional LSTM network.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: Dimension of input data\n",
    "    hidden_dim: Size of hidden state\n",
    "    layernorm: True/False\n",
    "    layers: Number of LSTM cells to stack\n",
    "    bidirectional: True/False\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        if self.layers < 1:\n",
    "            raise ValueError(\"layers need to be > 1\")\n",
    "        self.model = []\n",
    "        for i in range(self.layers):\n",
    "            self.model.append(LSTMCell(input_dim, hidden_dim, layernorm))\n",
    "        self.model = nn.ModuleList(self.model)\n",
    "        if self.bidirectional:\n",
    "            self.model_rev = []\n",
    "            for i in range(self.layers):\n",
    "                self.model_rev.append(LSTMCell(input_dim, hidden_dim, layernorm))\n",
    "            self.model_rev = nn.ModuleList(self.model_rev)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \"\"\"Forward pass for the LSTM network\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        x: [sequence_length, batch_size, input_dim]\n",
    "        hidden_state: [1, batch_size, hidden_dim]\n",
    "        cell_state: [1, batch_size, hidden_dim]\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        output, (hidden_state, cell_state)\n",
    "        If bidirectional=False\n",
    "            output: [sequence_length, batch_size, hidden_dim]\n",
    "                contains the output/hidden_state from all the timesteps\n",
    "                for the final layer in sequence 1...T\n",
    "            hidden_state: [1, batch_size, hidden_dim]\n",
    "                contains the hidden_state from the last timestep T\n",
    "                from all the layers\n",
    "            cell_state: [1, batch_size, hidden_dim]\n",
    "                contains the cell_state from the last timestep T\n",
    "                from all the layers\n",
    "        If bidirectional=True\n",
    "            output: [sequence_length, batch_size, 2, hidden_dim]\n",
    "                contains the output/hidden_state from all the timesteps\n",
    "                for the final layer in sequence 1...T\n",
    "                [:,:,0,:] contains output from Left-to-Right network\n",
    "                [:,:,1,:] contains output from to-Right-to-Left network\n",
    "            hidden_state: [layers, 2, batch_size, hidden_dim]\n",
    "            cell_state: [layers, 2, batch_size, hidden_dim]\n",
    "                [:,0,:,:] contains output from Left-to-Right network\n",
    "                    contains the output for the last timestep (t=T) with\n",
    "                    layer 1...layer N as index [0:N-1,:,:,:]\n",
    "                [:,1,:,:] contains output from to-Right-to-Left network\n",
    "                    contains the output for the last timestep (t=1) with\n",
    "                    layer 1...layer N as index [0:N-1,:,:,:]\n",
    "        \"\"\"\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        seq_length = x.shape[0]\n",
    "        # Left-to-right pass\n",
    "        # index of state is equivalent to index of layer in LSTM stack\n",
    "        hidden_states = hidden_state.view(hidden_state.shape[0], 1, \n",
    "                                           hidden_state.shape[1], hidden_state.shape[2])\n",
    "        cell_states = cell_state.view(cell_state.shape[0], 1, \n",
    "                                       cell_state.shape[1], cell_state.shape[2])\n",
    "        output = torch.tensor([], requires_grad=True).to(device)\n",
    "        # forward pass for one cell at a time\n",
    "        for j in range(self.layers):\n",
    "            output, (hidden_states[j], cell_states[j]) = self.model[j](x, hidden_states[j].clone(),\n",
    "                                                                  cell_states[j].clone(),\n",
    "                                                                  device)\n",
    "        hidden_states = hidden_states.squeeze(1)\n",
    "        cell_states = cell_states.squeeze(1) \n",
    "\n",
    "        # Right-to-left pass\n",
    "        if self.bidirectional:\n",
    "            # flipping inputs/rearranging x to be in reverse timestep order\n",
    "            x = torch.flip(x, [0])  # reversing only the sequence dimension\n",
    "            # index of state is equivalent to index of layer in LSTM stack\n",
    "            hidden_states_rev = hidden_state.view(hidden_state.shape[0], 1, \n",
    "                                               hidden_state.shape[1], hidden_state.shape[2])\n",
    "            cell_states_rev = cell_state.view(cell_state.shape[0], 1, \n",
    "                                           cell_state.shape[1], cell_state.shape[2])\n",
    "            output_rev = torch.tensor([], requires_grad=True).to(device)\n",
    "            # forward pass for one cell at a time\n",
    "            for j in range(self.layers):\n",
    "                output_rev, (hidden_states_rev[j], cell_states_rev[j]) = self.model_rev[j](x,\n",
    "                                                                        hidden_states_rev[j].clone(),\n",
    "                                                                        cell_states_rev[j].clone())\n",
    "            # flipping outputs to be in correct timestep order\n",
    "            output_rev = torch.flip(output_rev, [0]) # reversing only the sequence dimension\n",
    "            hidden_states_rev = hidden_states_rev.squeeze(1)\n",
    "            cell_states_rev = cell_states_rev.squeeze(1)\n",
    "            # concatenating tensors\n",
    "            ## creating tensors as expected in\n",
    "            ## here: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "            hidden_states = torch.cat((hidden_states,\n",
    "                                       hidden_states_rev), dim=0)\n",
    "            cell_states = torch.cat((cell_states,\n",
    "                                     cell_states_rev), dim=0)\n",
    "            output = torch.cat((output,\n",
    "                                output_rev), dim=2)\n",
    "\n",
    "        return output, (hidden_states, cell_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lstm import LSTM\n",
    "\n",
    "class LSTMSeqLabel(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-one)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "        \n",
    "        self.lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        output, (_, _) = self.lstm(x, hidden_state, cell_state)\n",
    "        output = output[-1].unsqueeze(0)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSeqLabel(input_dim, hidden_dim, output_dim, bidirectional=True, layers=layers).to(device)\n",
    "print(model.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.32766\n",
      "Average Loss: 0.617631\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.28181\n",
      "Average Loss: 0.576765\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.27816\n",
      "Average Loss: 0.57674\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.30521\n",
      "Average Loss: 0.575937\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.88851\n",
      "Average Loss: 0.579744\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.79432\n",
      "Average Loss: 0.578506\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  90/90  -- Loss: 0.32734\n",
      "Average Loss: 0.576033\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #8  : Batch  90/90  -- Loss: 0.33777\n",
      "Average Loss: 0.575859\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #9  : Batch  90/90  -- Loss: 0.56343\n",
      "Average Loss: 0.578558\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #10 : Batch  90/90  -- Loss: 0.28159\n",
      "Average Loss: 0.574664\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #11 : Batch  90/90  -- Loss: 0.60446\n",
      "Average Loss: 0.579632\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #12 : Batch  90/90  -- Loss: 0.57074\n",
      "Average Loss: 0.577355\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #13 : Batch  90/90  -- Loss: 1.00732\n",
      "Average Loss: 0.579143\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #14 : Batch  90/90  -- Loss: 0.56097\n",
      "Average Loss: 0.577647\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #15 : Batch  90/90  -- Loss: 0.55629\n",
      "Average Loss: 0.577023\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #16 : Batch  90/90  -- Loss: 0.85365\n",
      "Average Loss: 0.577935\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #17 : Batch  90/90  -- Loss: 0.80766\n",
      "Average Loss: 0.578342\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #18 : Batch  90/90  -- Loss: 0.84178\n",
      "Average Loss: 0.579349\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #19 : Batch  90/90  -- Loss: 0.60263\n",
      "Average Loss: 0.577376\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #20 : Batch  90/90  -- Loss: 0.53377\n",
      "Average Loss: 0.577102\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #21 : Batch  90/90  -- Loss: 0.76209\n",
      "Average Loss: 0.578262\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #22 : Batch  90/90  -- Loss: 0.57423\n",
      "Average Loss: 0.577984\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #23 : Batch  90/90  -- Loss: 0.53633\n",
      "Average Loss: 0.577539\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #24 : Batch  90/90  -- Loss: 0.78905\n",
      "Average Loss: 0.578533\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #25 : Batch  90/90  -- Loss: 0.55469\n",
      "Average Loss: 0.578535\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #26 : Batch  90/90  -- Loss: 0.54598\n",
      "Average Loss: 0.576191\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #27 : Batch  90/90  -- Loss: 0.32089\n",
      "Average Loss: 0.575474\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #28 : Batch  90/90  -- Loss: 0.30525\n",
      "Average Loss: 0.576363\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #29 : Batch  90/90  -- Loss: 0.28257\n",
      "Average Loss: 0.574997\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #30 : Batch  90/90  -- Loss: 0.57432\n",
      "Average Loss: 0.577406\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #31 : Batch  90/90  -- Loss: 0.58111\n",
      "Average Loss: 0.576617\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #32 : Batch  90/90  -- Loss: 0.29738\n",
      "Average Loss: 0.575841\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #33 : Batch  90/90  -- Loss: 0.82425\n",
      "Average Loss: 0.578394\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #34 : Batch  90/90  -- Loss: 0.30525\n",
      "Average Loss: 0.575256\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #35 : Batch  90/90  -- Loss: 0.53792\n",
      "Average Loss: 0.57631\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #36 : Batch  90/90  -- Loss: 0.55192\n",
      "Average Loss: 0.577453\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #37 : Batch  90/90  -- Loss: 0.28959\n",
      "Average Loss: 0.574812\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #38 : Batch  90/90  -- Loss: 0.79393\n",
      "Average Loss: 0.578026\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #39 : Batch  90/90  -- Loss: 0.53471\n",
      "Average Loss: 0.5769\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #40 : Batch  90/90  -- Loss: 0.32485\n",
      "Average Loss: 0.57602\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #41 : Batch  90/90  -- Loss: 0.54891\n",
      "Average Loss: 0.576499\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #42 : Batch  90/90  -- Loss: 0.28879\n",
      "Average Loss: 0.575575\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #43 : Batch  90/90  -- Loss: 0.30991\n",
      "Average Loss: 0.575337\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #44 : Batch  90/90  -- Loss: 0.87103\n",
      "Average Loss: 0.57819\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #45 : Batch  90/90  -- Loss: 0.84234\n",
      "Average Loss: 0.578184\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #46 : Batch  90/90  -- Loss: 0.52176\n",
      "Average Loss: 0.576113\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #47 : Batch  90/90  -- Loss: 0.54903\n",
      "Average Loss: 0.576637\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #48 : Batch  90/90  -- Loss: 0.33138\n",
      "Average Loss: 0.575367\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #49 : Batch  90/90  -- Loss: 0.33074\n",
      "Average Loss: 0.575378\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #50 : Batch  90/90  -- Loss: 0.88223\n",
      "Average Loss: 0.579055\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #51 : Batch  90/90  -- Loss: 1.14549\n",
      "Average Loss: 0.579659\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #52 : Batch  90/90  -- Loss: 1.08949\n",
      "Average Loss: 0.579405\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #53 : Batch  90/90  -- Loss: 0.81448\n",
      "Average Loss: 0.579511\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #54 : Batch  90/90  -- Loss: 0.55295\n",
      "Average Loss: 0.577331\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #55 : Batch  90/90  -- Loss: 0.84444\n",
      "Average Loss: 0.578159\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #56 : Batch  90/90  -- Loss: 0.31737\n",
      "Average Loss: 0.575157\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #57 : Batch  90/90  -- Loss: 0.81185\n",
      "Average Loss: 0.578871\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #58 : Batch  90/90  -- Loss: 0.27585\n",
      "Average Loss: 0.575107\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #59 : Batch  90/90  -- Loss: 0.83032\n",
      "Average Loss: 0.577714\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #60 : Batch  90/90  -- Loss: 0.57834\n",
      "Average Loss: 0.576625\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #61 : Batch  90/90  -- Loss: 0.75597\n",
      "Average Loss: 0.578901\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #62 : Batch  90/90  -- Loss: 0.30222\n",
      "Average Loss: 0.575703\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #63 : Batch  90/90  -- Loss: 0.56104\n",
      "Average Loss: 0.576499\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #64 : Batch  90/90  -- Loss: 0.53428\n",
      "Average Loss: 0.576122\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #65 : Batch  90/90  -- Loss: 0.85974\n",
      "Average Loss: 0.578113\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #66 : Batch  90/90  -- Loss: 0.54798\n",
      "Average Loss: 0.576097\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #67 : Batch  90/90  -- Loss: 0.31249\n",
      "Average Loss: 0.575906\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #68 : Batch  90/90  -- Loss: 0.59778\n",
      "Average Loss: 0.576482\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #69 : Batch  90/90  -- Loss: 0.28864\n",
      "Average Loss: 0.572374\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #70 : Batch  90/90  -- Loss: 0.62478\n",
      "Average Loss: 0.571526\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #71 : Batch  90/90  -- Loss: 0.74126\n",
      "Average Loss: 0.56665\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #72 : Batch  90/90  -- Loss: 0.53038\n",
      "Average Loss: 0.554694\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #73 : Batch  90/90  -- Loss: 1.02847\n",
      "Average Loss: 0.54179\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #74 : Batch  90/90  -- Loss: 0.37941\n",
      "Average Loss: 0.510682\n",
      "Training F1: 0.06897\n",
      "Test F1: 0.08451\n",
      "Epoch #75 : Batch  90/90  -- Loss: 0.15965\n",
      "Average Loss: 0.478353\n",
      "Training F1: 0.3576\n",
      "Test F1: 0.1633\n",
      "Epoch #76 : Batch  90/90  -- Loss: 0.395921\n",
      "Average Loss: 0.434843\n",
      "Training F1: 0.35\n",
      "Test F1: 0.1778\n",
      "Epoch #77 : Batch  90/90  -- Loss: 0.67945\n",
      "Average Loss: 0.393352\n",
      "Training F1: 0.396\n",
      "Test F1: 0.2105\n",
      "Epoch #78 : Batch  90/90  -- Loss: 0.054092\n",
      "Average Loss: 0.377804\n",
      "Training F1: 0.5574\n",
      "Test F1: 0.5677\n",
      "Epoch #79 : Batch  90/90  -- Loss: 0.361153\n",
      "Average Loss: 0.370517\n",
      "Training F1: 0.0\n",
      "Test F1: 0.02899\n",
      "Epoch #80 : Batch  90/90  -- Loss: 0.50853\n",
      "Average Loss: 0.367923\n",
      "Training F1: 0.5535\n",
      "Test F1: 0.3411\n",
      "Epoch #81 : Batch  90/90  -- Loss: 0.219285\n",
      "Average Loss: 0.363849\n",
      "Training F1: 0.5627\n",
      "Test F1: 0.3852\n",
      "Epoch #82 : Batch  90/90  -- Loss: 0.540215\n",
      "Average Loss: 0.3643\n",
      "Training F1: 0.6263\n",
      "Test F1: 0.5\n",
      "Epoch #83 : Batch  90/90  -- Loss: 0.360535\n",
      "Average Loss: 0.362429\n",
      "Training F1: 0.6381\n",
      "Test F1: 0.5465\n",
      "Epoch #84 : Batch  90/90  -- Loss: 0.216672\n",
      "Average Loss: 0.359945\n",
      "Training F1: 0.6283\n",
      "Test F1: 0.481\n",
      "Epoch #85 : Batch  90/90  -- Loss: 0.160522\n",
      "Average Loss: 0.357395\n",
      "Training F1: 0.6041\n",
      "Test F1: 0.4474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #86 : Batch  90/90  -- Loss: 0.524498\n",
      "Average Loss: 0.360302\n",
      "Training F1: 0.6393\n",
      "Test F1: 0.5349\n",
      "Epoch #87 : Batch  90/90  -- Loss: 0.5667565\n",
      "Average Loss: 0.359741\n",
      "Training F1: 0.6641\n",
      "Test F1: 0.6132\n",
      "Epoch #88 : Batch  90/90  -- Loss: 0.5048411\n",
      "Average Loss: 0.357354\n",
      "Training F1: 0.5561\n",
      "Test F1: 0.3817\n",
      "Epoch #89 : Batch  90/90  -- Loss: 0.716949\n",
      "Average Loss: 0.357729\n",
      "Training F1: 0.6143\n",
      "Test F1: 0.5333\n",
      "Epoch #90 : Batch  90/90  -- Loss: 0.363796\n",
      "Average Loss: 0.355099\n",
      "Training F1: 0.545\n",
      "Test F1: 0.4058\n",
      "Epoch #91 : Batch  90/90  -- Loss: 0.360429\n",
      "Average Loss: 0.356629\n",
      "Training F1: 0.5922\n",
      "Test F1: 0.472\n",
      "Epoch #92 : Batch  90/90  -- Loss: 0.175272\n",
      "Average Loss: 0.353627\n",
      "Training F1: 0.5237\n",
      "Test F1: 0.3492\n",
      "Epoch #93 : Batch  90/90  -- Loss: 0.3192449\n",
      "Average Loss: 0.358039\n",
      "Training F1: 0.58\n",
      "Test F1: 0.4414\n",
      "Epoch #94 : Batch  90/90  -- Loss: 0.6335727\n",
      "Average Loss: 0.35488\n",
      "Training F1: 0.5479\n",
      "Test F1: 0.3411\n",
      "Epoch #95 : Batch  90/90  -- Loss: 0.333491\n",
      "Average Loss: 0.367108\n",
      "Training F1: 0.5484\n",
      "Test F1: 0.3411\n",
      "Epoch #96 : Batch  90/90  -- Loss: 0.31021\n",
      "Average Loss: 0.353882\n",
      "Training F1: 0.621\n",
      "Test F1: 0.4487\n",
      "Epoch #97 : Batch  90/90  -- Loss: 0.358856\n",
      "Average Loss: 0.352801\n",
      "Training F1: 0.5806\n",
      "Test F1: 0.478\n",
      "Epoch #98 : Batch  90/90  -- Loss: 0.314275\n",
      "Average Loss: 0.353329\n",
      "Training F1: 0.5289\n",
      "Test F1: 0.3333\n",
      "Epoch #99 : Batch  90/90  -- Loss: 0.725918\n",
      "Average Loss: 0.354523\n",
      "Training F1: 0.5058\n",
      "Test F1: 0.3471\n",
      "Epoch #100: Batch  90/90  -- Loss: 0.5611138\n",
      "Average Loss: 0.354025\n",
      "Training F1: 0.6055\n",
      "Test F1: 0.4937\n",
      "Epoch #101: Batch  90/90  -- Loss: 0.184179\n",
      "Average Loss: 0.351206\n",
      "Training F1: 0.5894\n",
      "Test F1: 0.4805\n",
      "Epoch #102: Batch  90/90  -- Loss: 0.401677\n",
      "Average Loss: 0.353468\n",
      "Training F1: 0.5535\n",
      "Test F1: 0.3411\n",
      "Epoch #103: Batch  90/90  -- Loss: 0.20194\n",
      "Average Loss: 0.350609\n",
      "Training F1: 0.6874\n",
      "Test F1: 0.6175\n",
      "Epoch #104: Batch  90/90  -- Loss: 0.381847\n",
      "Average Loss: 0.352872\n",
      "Training F1: 0.6705\n",
      "Test F1: 0.6161\n",
      "Epoch #105: Batch  90/90  -- Loss: 0.421588\n",
      "Average Loss: 0.354578\n",
      "Training F1: 0.5435\n",
      "Test F1: 0.3411\n",
      "Epoch #106: Batch  90/90  -- Loss: 0.376334\n",
      "Average Loss: 0.353811\n",
      "Training F1: 0.6489\n",
      "Test F1: 0.5829\n",
      "Epoch #107: Batch  90/90  -- Loss: 0.1595836\n",
      "Average Loss: 0.351733\n",
      "Training F1: 0.5484\n",
      "Test F1: 0.3411\n",
      "Epoch #108: Batch  90/90  -- Loss: 0.550398\n",
      "Average Loss: 0.352501\n",
      "Training F1: 0.5\n",
      "Test F1: 0.3279\n",
      "Epoch #109: Batch  90/90  -- Loss: 0.60568\n",
      "Average Loss: 0.353419\n",
      "Training F1: 0.6742\n",
      "Test F1: 0.5882\n",
      "Epoch #110: Batch  90/90  -- Loss: 0.3576574\n",
      "Average Loss: 0.351603\n",
      "Training F1: 0.6109\n",
      "Test F1: 0.5445\n",
      "Epoch #111: Batch  90/90  -- Loss: 0.7199411\n",
      "Average Loss: 0.357372\n",
      "Training F1: 0.6164\n",
      "Test F1: 0.4944\n",
      "Epoch #112: Batch  90/90  -- Loss: 0.440534\n",
      "Average Loss: 0.353565\n",
      "Training F1: 0.6769\n",
      "Test F1: 0.6029\n",
      "Epoch #113: Batch  90/90  -- Loss: 0.5345985\n",
      "Average Loss: 0.352115\n",
      "Training F1: 0.6627\n",
      "Test F1: 0.5941\n",
      "Epoch #114: Batch  90/90  -- Loss: 0.2658773\n",
      "Average Loss: 0.352044\n",
      "Training F1: 0.6557\n",
      "Test F1: 0.5521\n",
      "Epoch #115: Batch  90/90  -- Loss: 0.3166881\n",
      "Average Loss: 0.351907\n",
      "Training F1: 0.6135\n",
      "Test F1: 0.5091\n",
      "Epoch #116: Batch  90/90  -- Loss: 0.211343\n",
      "Average Loss: 0.350874\n",
      "Training F1: 0.5535\n",
      "Test F1: 0.3411\n",
      "Epoch #117: Batch  90/90  -- Loss: 0.297128\n",
      "Average Loss: 0.35193\n",
      "Training F1: 0.6202\n",
      "Test F1: 0.5561\n",
      "Epoch #118: Batch  90/90  -- Loss: 0.382724\n",
      "Average Loss: 0.352828\n",
      "Training F1: 0.5395\n",
      "Test F1: 0.3281\n",
      "Epoch #119: Batch  90/90  -- Loss: 0.146655\n",
      "Average Loss: 0.350152\n",
      "Training F1: 0.625\n",
      "Test F1: 0.5246\n",
      "Epoch #120: Batch  90/90  -- Loss: 0.335684\n",
      "Average Loss: 0.349557\n",
      "Training F1: 0.5882\n",
      "Test F1: 0.3857\n",
      "Epoch #121: Batch  90/90  -- Loss: 0.3426914\n",
      "Average Loss: 0.352771\n",
      "Training F1: 0.6414\n",
      "Test F1: 0.5455\n",
      "Epoch #122: Batch  90/90  -- Loss: 0.165418\n",
      "Average Loss: 0.349433\n",
      "Training F1: 0.5275\n",
      "Test F1: 0.3307\n",
      "Epoch #123: Batch  90/90  -- Loss: 0.0026955\n",
      "Average Loss: 0.349861\n",
      "Training F1: 0.6667\n",
      "Test F1: 0.5414\n",
      "Epoch #124: Batch  90/90  -- Loss: 0.534549\n",
      "Average Loss: 0.351628\n",
      "Training F1: 0.4452\n",
      "Test F1: 0.2281\n",
      "Epoch #125: Batch  90/90  -- Loss: 0.1189597\n",
      "Average Loss: 0.34915\n",
      "Training F1: 0.6091\n",
      "Test F1: 0.45\n",
      "Epoch #126: Batch  90/90  -- Loss: 0.300461\n",
      "Average Loss: 0.350334\n",
      "Training F1: 0.3643\n",
      "Test F1: 0.1616\n",
      "Epoch #127: Batch  90/90  -- Loss: 0.29204\n",
      "Average Loss: 0.350651\n",
      "Training F1: 0.6309\n",
      "Test F1: 0.5532\n",
      "Epoch #128: Batch  90/90  -- Loss: 0.172228\n",
      "Average Loss: 0.349391\n",
      "Training F1: 0.5815\n",
      "Test F1: 0.3862\n",
      "Epoch #129: Batch  90/90  -- Loss: 0.356798\n",
      "Average Loss: 0.351315\n",
      "Training F1: 0.5405\n",
      "Test F1: 0.3411\n",
      "Epoch #130: Batch  90/90  -- Loss: 0.192363\n",
      "Average Loss: 0.351026\n",
      "Training F1: 0.6718\n",
      "Test F1: 0.5871\n",
      "Epoch #131: Batch  90/90  -- Loss: 0.3430584\n",
      "Average Loss: 0.349529\n",
      "Training F1: 0.4691\n",
      "Test F1: 0.2586\n",
      "Epoch #132: Batch  90/90  -- Loss: 0.325342\n",
      "Average Loss: 0.349052\n",
      "Training F1: 0.5842\n",
      "Test F1: 0.3974\n",
      "Epoch #133: Batch  90/90  -- Loss: 0.51453\n",
      "Average Loss: 0.351205\n",
      "Training F1: 0.4408\n",
      "Test F1: 0.2281\n",
      "Epoch #134: Batch  90/90  -- Loss: 0.3490356\n",
      "Average Loss: 0.349855\n",
      "Training F1: 0.5072\n",
      "Test F1: 0.336\n",
      "Epoch #135: Batch  90/90  -- Loss: 0.177569\n",
      "Average Loss: 0.348061\n",
      "Training F1: 0.6873\n",
      "Test F1: 0.5911\n",
      "Epoch #136: Batch  90/90  -- Loss: 0.569571\n",
      "Average Loss: 0.351402\n",
      "Training F1: 0.6337\n",
      "Test F1: 0.4912\n",
      "Epoch #137: Batch  90/90  -- Loss: 0.182898\n",
      "Average Loss: 0.348206\n",
      "Training F1: 0.6398\n",
      "Test F1: 0.5484\n",
      "Epoch #138: Batch  90/90  -- Loss: 0.3445398\n",
      "Average Loss: 0.349819\n",
      "Training F1: 0.6435\n",
      "Test F1: 0.5521\n",
      "Epoch #139: Batch  90/90  -- Loss: 0.364494\n",
      "Average Loss: 0.348725\n",
      "Training F1: 0.6893\n",
      "Test F1: 0.6058\n",
      "Epoch #140: Batch  90/90  -- Loss: 0.678214\n",
      "Average Loss: 0.351611\n",
      "Training F1: 0.677\n",
      "Test F1: 0.6154\n",
      "Epoch #141: Batch  90/90  -- Loss: 0.178197\n",
      "Average Loss: 0.346431\n",
      "Training F1: 0.471\n",
      "Test F1: 0.2542\n",
      "Epoch #142: Batch  90/90  -- Loss: 0.2988445\n",
      "Average Loss: 0.349402\n",
      "Training F1: 0.6943\n",
      "Test F1: 0.6124\n",
      "Epoch #143: Batch  90/90  -- Loss: 0.537786\n",
      "Average Loss: 0.350287\n",
      "Training F1: 0.6473\n",
      "Test F1: 0.5464\n",
      "Epoch #144: Batch  90/90  -- Loss: 0.2927871\n",
      "Average Loss: 0.349687\n",
      "Training F1: 0.6836\n",
      "Test F1: 0.6087\n",
      "Epoch #145: Batch  90/90  -- Loss: 0.312813\n",
      "Average Loss: 0.348219\n",
      "Training F1: 0.6983\n",
      "Test F1: 0.6019\n",
      "Epoch #146: Batch  90/90  -- Loss: 0.629896\n",
      "Average Loss: 0.352494\n",
      "Training F1: 0.5882\n",
      "Test F1: 0.3885\n",
      "Epoch #147: Batch  90/90  -- Loss: 0.348763\n",
      "Average Loss: 0.347554\n",
      "Training F1: 0.6495\n",
      "Test F1: 0.5684\n",
      "Epoch #148: Batch  90/90  -- Loss: 0.529091\n",
      "Average Loss: 0.34868\n",
      "Training F1: 0.5337\n",
      "Test F1: 0.3259\n",
      "Epoch #149: Batch  90/90  -- Loss: 0.507918\n",
      "Average Loss: 0.351084\n",
      "Training F1: 0.6584\n",
      "Test F1: 0.5911\n",
      "Epoch #150: Batch  90/90  -- Loss: 0.323351\n",
      "Average Loss: 0.346658\n",
      "Training F1: 0.6102\n",
      "Test F1: 0.4416\n",
      "Epoch #151: Batch  90/90  -- Loss: 0.182044\n",
      "Average Loss: 0.346612\n",
      "Training F1: 0.6065\n",
      "Test F1: 0.4211\n",
      "Epoch #152: Batch  90/90  -- Loss: 0.742556\n",
      "Average Loss: 0.348181\n",
      "Training F1: 0.6667\n",
      "Test F1: 0.5638\n",
      "Epoch #153: Batch  90/90  -- Loss: 0.712662\n",
      "Average Loss: 0.35226\n",
      "Training F1: 0.6393\n",
      "Test F1: 0.4568\n",
      "Epoch #154: Batch  90/90  -- Loss: 0.569247\n",
      "Average Loss: 0.345411\n",
      "Training F1: 0.6636\n",
      "Test F1: 0.5683\n",
      "Epoch #155: Batch  90/90  -- Loss: 0.66937\n",
      "Average Loss: 0.346355\n",
      "Training F1: 0.6652\n",
      "Test F1: 0.5029\n",
      "Epoch #156: Batch  90/90  -- Loss: 0.736214\n",
      "Average Loss: 0.350482\n",
      "Training F1: 0.6841\n",
      "Test F1: 0.5257\n",
      "Epoch #157: Batch  90/90  -- Loss: 0.519456\n",
      "Average Loss: 0.344026\n",
      "Training F1: 0.5579\n",
      "Test F1: 0.3622\n",
      "Epoch #158: Batch  90/90  -- Loss: 0.342722\n",
      "Average Loss: 0.342111\n",
      "Training F1: 0.6419\n",
      "Test F1: 0.4654\n",
      "Epoch #159: Batch  90/90  -- Loss: 0.2632127\n",
      "Average Loss: 0.344752\n",
      "Training F1: 0.6381\n",
      "Test F1: 0.4875\n",
      "Epoch #160: Batch  90/90  -- Loss: 0.345411\n",
      "Average Loss: 0.341819\n",
      "Training F1: 0.6359\n",
      "Test F1: 0.4774\n",
      "Epoch #161: Batch  90/90  -- Loss: 0.306044\n",
      "Average Loss: 0.341357\n",
      "Training F1: 0.5959\n",
      "Test F1: 0.4658\n",
      "Epoch #162: Batch  90/90  -- Loss: 0.119359\n",
      "Average Loss: 0.339706\n",
      "Training F1: 0.7061\n",
      "Test F1: 0.6231\n",
      "Epoch #163: Batch  90/90  -- Loss: 0.534692\n",
      "Average Loss: 0.342539\n",
      "Training F1: 0.6529\n",
      "Test F1: 0.5967\n",
      "Epoch #164: Batch  90/90  -- Loss: 0.324881\n",
      "Average Loss: 0.340511\n",
      "Training F1: 0.6527\n",
      "Test F1: 0.5965\n",
      "Epoch #165: Batch  90/90  -- Loss: 0.382077\n",
      "Average Loss: 0.339895\n",
      "Training F1: 0.6783\n",
      "Test F1: 0.6129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #166: Batch  90/90  -- Loss: 0.5307933\n",
      "Average Loss: 0.341013\n",
      "Training F1: 0.5905\n",
      "Test F1: 0.4094\n",
      "Epoch #167: Batch  90/90  -- Loss: 0.0016235\n",
      "Average Loss: 0.337921\n",
      "Training F1: 0.5995\n",
      "Test F1: 0.4604\n",
      "Epoch #168: Batch  90/90  -- Loss: 0.552672\n",
      "Average Loss: 0.340644\n",
      "Training F1: 0.654\n",
      "Test F1: 0.5542\n",
      "Epoch #169: Batch  90/90  -- Loss: 0.341618\n",
      "Average Loss: 0.33855\n",
      "Training F1: 0.6813\n",
      "Test F1: 0.5914\n",
      "Epoch #170: Batch  90/90  -- Loss: 0.3949449\n",
      "Average Loss: 0.339179\n",
      "Training F1: 0.6129\n",
      "Test F1: 0.4526\n",
      "Epoch #171: Batch  90/90  -- Loss: 0.2928178\n",
      "Average Loss: 0.338057\n",
      "Training F1: 0.6368\n",
      "Test F1: 0.5283\n",
      "Epoch #172: Batch  90/90  -- Loss: 0.3038718\n",
      "Average Loss: 0.337565\n",
      "Training F1: 0.6667\n",
      "Test F1: 0.5389\n",
      "Epoch #173: Batch  90/90  -- Loss: 0.0013661\n",
      "Average Loss: 0.337087\n",
      "Training F1: 0.6292\n",
      "Test F1: 0.4286\n",
      "Epoch #174: Batch  90/90  -- Loss: 0.346937\n",
      "Average Loss: 0.337836\n",
      "Training F1: 0.6193\n",
      "Test F1: 0.48\n",
      "Epoch #175: Batch  90/90  -- Loss: 0.0009979\n",
      "Average Loss: 0.336242\n",
      "Training F1: 0.6364\n",
      "Test F1: 0.4832\n",
      "Epoch #176: Batch  90/90  -- Loss: 0.342066\n",
      "Average Loss: 0.337887\n",
      "Training F1: 0.6429\n",
      "Test F1: 0.4306\n",
      "Epoch #177: Batch  90/90  -- Loss: 0.460663\n",
      "Average Loss: 0.335877\n",
      "Training F1: 0.6553\n",
      "Test F1: 0.52\n",
      "Epoch #178: Batch  90/90  -- Loss: 0.183491\n",
      "Average Loss: 0.333818\n",
      "Training F1: 0.6698\n",
      "Test F1: 0.5031\n",
      "Epoch #179: Batch  90/90  -- Loss: 0.228771\n",
      "Average Loss: 0.336611\n",
      "Training F1: 0.6332\n",
      "Test F1: 0.5101\n",
      "Epoch #180: Batch  90/90  -- Loss: 0.502458\n",
      "Average Loss: 0.337142\n",
      "Training F1: 0.6534\n",
      "Test F1: 0.4626\n",
      "Epoch #181: Batch  90/90  -- Loss: 0.648231\n",
      "Average Loss: 0.33514\n",
      "Training F1: 0.6218\n",
      "Test F1: 0.4895\n",
      "Epoch #182: Batch  90/90  -- Loss: 0.387333\n",
      "Average Loss: 0.337572\n",
      "Training F1: 0.5869\n",
      "Test F1: 0.4651\n",
      "Epoch #183: Batch  90/90  -- Loss: 0.555032\n",
      "Average Loss: 0.334102\n",
      "Training F1: 0.712\n",
      "Test F1: 0.6091\n",
      "Epoch #184: Batch  90/90  -- Loss: 0.1106617\n",
      "Average Loss: 0.334019\n",
      "Training F1: 0.657\n",
      "Test F1: 0.5385\n",
      "Epoch #185: Batch  90/90  -- Loss: 0.454686\n",
      "Average Loss: 0.334436\n",
      "Training F1: 0.6146\n",
      "Test F1: 0.4783\n",
      "Epoch #186: Batch  90/90  -- Loss: 0.30211\n",
      "Average Loss: 0.334043\n",
      "Training F1: 0.6802\n",
      "Test F1: 0.6102\n",
      "Epoch #187: Batch  90/90  -- Loss: 0.00141718\n",
      "Average Loss: 0.332187\n",
      "Training F1: 0.6851\n",
      "Test F1: 0.5412\n",
      "Epoch #188: Batch  90/90  -- Loss: 0.312999\n",
      "Average Loss: 0.331171\n",
      "Training F1: 0.6857\n",
      "Test F1: 0.5375\n",
      "Epoch #189: Batch  90/90  -- Loss: 0.357169\n",
      "Average Loss: 0.333421\n",
      "Training F1: 0.6166\n",
      "Test F1: 0.4818\n",
      "Epoch #190: Batch  90/90  -- Loss: 0.129782\n",
      "Average Loss: 0.331181\n",
      "Training F1: 0.7064\n",
      "Test F1: 0.6034\n",
      "Epoch #191: Batch  90/90  -- Loss: 0.448455\n",
      "Average Loss: 0.332107\n",
      "Training F1: 0.6732\n",
      "Test F1: 0.549\n",
      "Epoch #192: Batch  90/90  -- Loss: 0.221114\n",
      "Average Loss: 0.327438\n",
      "Training F1: 0.7178\n",
      "Test F1: 0.5977\n",
      "Epoch #193: Batch  90/90  -- Loss: 0.110331\n",
      "Average Loss: 0.32787\n",
      "Training F1: 0.6991\n",
      "Test F1: 0.5455\n",
      "Epoch #194: Batch  90/90  -- Loss: 0.722676\n",
      "Average Loss: 0.32959\n",
      "Training F1: 0.6062\n",
      "Test F1: 0.439\n",
      "Epoch #195: Batch  90/90  -- Loss: 0.526929\n",
      "Average Loss: 0.329954\n",
      "Training F1: 0.6514\n",
      "Test F1: 0.5205\n",
      "Epoch #196: Batch  90/90  -- Loss: 0.1195839\n",
      "Average Loss: 0.32427\n",
      "Training F1: 0.6538\n",
      "Test F1: 0.4062\n",
      "Epoch #197: Batch  90/90  -- Loss: 0.252478\n",
      "Average Loss: 0.323782\n",
      "Training F1: 0.6772\n",
      "Test F1: 0.4328\n",
      "Epoch #198: Batch  90/90  -- Loss: 0.4634699\n",
      "Average Loss: 0.323463\n",
      "Training F1: 0.6819\n",
      "Test F1: 0.4412\n",
      "Epoch #199: Batch  90/90  -- Loss: 0.181215\n",
      "Average Loss: 0.319186\n",
      "Training F1: 0.7095\n",
      "Test F1: 0.6036\n",
      "Epoch #200: Batch  90/90  -- Loss: 0.336643\n",
      "Average Loss: 0.323198\n",
      "Training F1: 0.7149\n",
      "Test F1: 0.5965\n",
      "Epoch #201: Batch  90/90  -- Loss: 0.173555\n",
      "Average Loss: 0.315526\n",
      "Training F1: 0.6268\n",
      "Test F1: 0.439\n",
      "Epoch #202: Batch  90/90  -- Loss: 0.332792\n",
      "Average Loss: 0.318035\n",
      "Training F1: 0.7222\n",
      "Test F1: 0.5802\n",
      "Epoch #203: Batch  90/90  -- Loss: 0.1689777\n",
      "Average Loss: 0.315117\n",
      "Training F1: 0.7176\n",
      "Test F1: 0.5926\n",
      "Epoch #204: Batch  90/90  -- Loss: 0.5344843\n",
      "Average Loss: 0.314206\n",
      "Training F1: 0.6948\n",
      "Test F1: 0.5419\n",
      "Epoch #205: Batch  90/90  -- Loss: 0.220775\n",
      "Average Loss: 0.309946\n",
      "Training F1: 0.73\n",
      "Test F1: 0.6484\n",
      "Epoch #206: Batch  90/90  -- Loss: 0.352885\n",
      "Average Loss: 0.309531\n",
      "Training F1: 0.6909\n",
      "Test F1: 0.4706\n",
      "Epoch #207: Batch  90/90  -- Loss: 0.18861759\n",
      "Average Loss: 0.302582\n",
      "Training F1: 0.7115\n",
      "Test F1: 0.5732\n",
      "Epoch #208: Batch  90/90  -- Loss: 0.4797538\n",
      "Average Loss: 0.306059\n",
      "Training F1: 0.6703\n",
      "Test F1: 0.4724\n",
      "Epoch #209: Batch  90/90  -- Loss: 0.1693519\n",
      "Average Loss: 0.298971\n",
      "Training F1: 0.7186\n",
      "Test F1: 0.5\n",
      "Epoch #210: Batch  90/90  -- Loss: 0.301837\n",
      "Average Loss: 0.296646\n",
      "Training F1: 0.7037\n",
      "Test F1: 0.4308\n",
      "Epoch #211: Batch  90/90  -- Loss: 0.718353\n",
      "Average Loss: 0.29591\n",
      "Training F1: 0.6891\n",
      "Test F1: 0.4655\n",
      "Epoch #212: Batch  90/90  -- Loss: 0.7029414\n",
      "Average Loss: 0.295153\n",
      "Training F1: 0.7318\n",
      "Test F1: 0.5175\n",
      "Epoch #213: Batch  90/90  -- Loss: 0.386244\n",
      "Average Loss: 0.285224\n",
      "Training F1: 0.7628\n",
      "Test F1: 0.6125\n",
      "Epoch #214: Batch  90/90  -- Loss: 0.242452\n",
      "Average Loss: 0.281789\n",
      "Training F1: 0.7358\n",
      "Test F1: 0.4662\n",
      "Epoch #215: Batch  90/90  -- Loss: 0.165352\n",
      "Average Loss: 0.272926\n",
      "Training F1: 0.7292\n",
      "Test F1: 0.5039\n",
      "Epoch #216: Batch  90/90  -- Loss: 0.171462\n",
      "Average Loss: 0.274075\n",
      "Training F1: 0.77\n",
      "Test F1: 0.6053\n",
      "Epoch #217: Batch  90/90  -- Loss: 0.196131\n",
      "Average Loss: 0.268672\n",
      "Training F1: 0.7677\n",
      "Test F1: 0.5915\n",
      "Epoch #218: Batch  90/90  -- Loss: 0.00045568\n",
      "Average Loss: 0.258126\n",
      "Training F1: 0.7283\n",
      "Test F1: 0.5366\n",
      "Epoch #219: Batch  90/90  -- Loss: 0.0018997\n",
      "Average Loss: 0.254497\n",
      "Training F1: 0.7627\n",
      "Test F1: 0.5581\n",
      "Epoch #220: Batch  90/90  -- Loss: 0.11128474\n",
      "Average Loss: 0.247938\n",
      "Training F1: 0.7962\n",
      "Test F1: 0.6709\n",
      "Epoch #221: Batch  90/90  -- Loss: 0.2484389\n",
      "Average Loss: 0.246215\n",
      "Training F1: 0.7813\n",
      "Test F1: 0.6494\n",
      "Epoch #222: Batch  90/90  -- Loss: 0.188711\n",
      "Average Loss: 0.229806\n",
      "Training F1: 0.7959\n",
      "Test F1: 0.6429\n",
      "Epoch #223: Batch  90/90  -- Loss: 0.0918536\n",
      "Average Loss: 0.221683\n",
      "Training F1: 0.8152\n",
      "Test F1: 0.6667\n",
      "Epoch #224: Batch  90/90  -- Loss: 0.304986\n",
      "Average Loss: 0.21098\n",
      "Training F1: 0.8107\n",
      "Test F1: 0.6423\n",
      "Epoch #225: Batch  90/90  -- Loss: 0.070944\n",
      "Average Loss: 0.206252\n",
      "Training F1: 0.8177\n",
      "Test F1: 0.6429\n",
      "Epoch #226: Batch  90/90  -- Loss: 0.020627\n",
      "Average Loss: 0.193512\n",
      "Training F1: 0.8249\n",
      "Test F1: 0.7654\n",
      "Epoch #227: Batch  90/90  -- Loss: 0.026619\n",
      "Average Loss: 0.18803\n",
      "Training F1: 0.8391\n",
      "Test F1: 0.7324\n",
      "Epoch #228: Batch  90/90  -- Loss: 0.149441\n",
      "Average Loss: 0.181246\n",
      "Training F1: 0.8259\n",
      "Test F1: 0.6711\n",
      "Epoch #229: Batch  90/90  -- Loss: 0.759899\n",
      "Average Loss: 0.190067\n",
      "Training F1: 0.8319\n",
      "Test F1: 0.7\n",
      "Epoch #230: Batch  90/90  -- Loss: 0.071959\n",
      "Average Loss: 0.171707\n",
      "Training F1: 0.8519\n",
      "Test F1: 0.7758\n",
      "Epoch #231: Batch  90/90  -- Loss: 0.4060665\n",
      "Average Loss: 0.167568\n",
      "Training F1: 0.8462\n",
      "Test F1: 0.6565\n",
      "Epoch #232: Batch  90/90  -- Loss: 0.043478\n",
      "Average Loss: 0.160697\n",
      "Training F1: 0.86\n",
      "Test F1: 0.7582\n",
      "Epoch #233: Batch  90/90  -- Loss: 0.2651852\n",
      "Average Loss: 0.158135\n",
      "Training F1: 0.8438\n",
      "Test F1: 0.7068\n",
      "Epoch #234: Batch  90/90  -- Loss: 0.2440798\n",
      "Average Loss: 0.157424\n",
      "Training F1: 0.8518\n",
      "Test F1: 0.7324\n",
      "Epoch #235: Batch  90/90  -- Loss: 0.0134324\n",
      "Average Loss: 0.153182\n",
      "Training F1: 0.8533\n",
      "Test F1: 0.7391\n",
      "Epoch #236: Batch  90/90  -- Loss: 0.2704934\n",
      "Average Loss: 0.151494\n",
      "Training F1: 0.8556\n",
      "Test F1: 0.7324\n",
      "Epoch #237: Batch  90/90  -- Loss: 0.0220662\n",
      "Average Loss: 0.146358\n",
      "Training F1: 0.8525\n",
      "Test F1: 0.7218\n",
      "Epoch #238: Batch  90/90  -- Loss: 0.0111821\n",
      "Average Loss: 0.145019\n",
      "Training F1: 0.8579\n",
      "Test F1: 0.7815\n",
      "Epoch #239: Batch  90/90  -- Loss: 0.0080966\n",
      "Average Loss: 0.143824\n",
      "Training F1: 0.8654\n",
      "Test F1: 0.7361\n",
      "Epoch #240: Batch  90/90  -- Loss: 0.1419193\n",
      "Average Loss: 0.141313\n",
      "Training F1: 0.8622\n",
      "Test F1: 0.7922\n",
      "Epoch #241: Batch  90/90  -- Loss: 0.01259219\n",
      "Average Loss: 0.135979\n",
      "Training F1: 0.8726\n",
      "Test F1: 0.7482\n",
      "Epoch #242: Batch  90/90  -- Loss: 0.1249516\n",
      "Average Loss: 0.135315\n",
      "Training F1: 0.8635\n",
      "Test F1: 0.746\n",
      "Epoch #243: Batch  90/90  -- Loss: 0.01276484\n",
      "Average Loss: 0.133184\n",
      "Training F1: 0.8747\n",
      "Test F1: 0.7692\n",
      "Epoch #244: Batch  90/90  -- Loss: 0.1081566\n",
      "Average Loss: 0.134549\n",
      "Training F1: 0.8668\n",
      "Test F1: 0.7862\n",
      "Epoch #245: Batch  90/90  -- Loss: 0.1332556\n",
      "Average Loss: 0.13514\n",
      "Training F1: 0.8726\n",
      "Test F1: 0.7647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #246: Batch  90/90  -- Loss: 0.0019791\n",
      "Average Loss: 0.128787\n",
      "Training F1: 0.8779\n",
      "Test F1: 0.8027\n",
      "Epoch #247: Batch  90/90  -- Loss: 0.43792791\n",
      "Average Loss: 0.163685\n",
      "Training F1: 0.8179\n",
      "Test F1: 0.7087\n",
      "Epoch #248: Batch  90/90  -- Loss: 0.00718799\n",
      "Average Loss: 0.137024\n",
      "Training F1: 0.8918\n",
      "Test F1: 0.8138\n",
      "Epoch #249: Batch  90/90  -- Loss: 0.00230159\n",
      "Average Loss: 0.140846\n",
      "Training F1: 0.8835\n",
      "Test F1: 0.791\n",
      "Epoch #250: Batch  90/90  -- Loss: 0.0847972\n",
      "Average Loss: 0.122335\n",
      "Training F1: 0.877\n",
      "Test F1: 0.8029\n",
      "Epoch #251: Batch  90/90  -- Loss: 0.00135468\n",
      "Average Loss: 0.15093\n",
      "Training F1: 0.7787\n",
      "Test F1: 0.6522\n",
      "Epoch #252: Batch  90/90  -- Loss: 0.09846629\n",
      "Average Loss: 0.190612\n",
      "Training F1: 0.7624\n",
      "Test F1: 0.5937\n",
      "Epoch #253: Batch  90/90  -- Loss: 0.00018368\n",
      "Average Loss: 0.179504\n",
      "Training F1: 0.7416\n",
      "Test F1: 0.541\n",
      "Epoch #254: Batch  90/90  -- Loss: 0.28273833\n",
      "Average Loss: 0.175793\n",
      "Training F1: 0.8421\n",
      "Test F1: 0.7619\n",
      "Epoch #255: Batch  90/90  -- Loss: 0.25879243\n",
      "Average Loss: 0.174191\n",
      "Training F1: 0.7775\n",
      "Test F1: 0.637\n",
      "Epoch #256: Batch  90/90  -- Loss: 0.15326529\n",
      "Average Loss: 0.171626\n",
      "Training F1: 0.8154\n",
      "Test F1: 0.75\n",
      "Epoch #257: Batch  90/90  -- Loss: 0.0034244\n",
      "Average Loss: 0.168571\n",
      "Training F1: 0.7414\n",
      "Test F1: 0.5128\n",
      "Epoch #258: Batch  90/90  -- Loss: 0.00015216\n",
      "Average Loss: 0.168409\n",
      "Training F1: 0.7937\n",
      "Test F1: 0.6861\n",
      "Epoch #259: Batch  90/90  -- Loss: 0.0049274\n",
      "Average Loss: 0.167801\n",
      "Training F1: 0.7717\n",
      "Test F1: 0.6202\n",
      "Epoch #260: Batch  90/90  -- Loss: 0.21692305\n",
      "Average Loss: 0.168868\n",
      "Training F1: 0.7738\n",
      "Test F1: 0.6412\n",
      "Epoch #261: Batch  90/90  -- Loss: 0.00017268\n",
      "Average Loss: 0.166243\n",
      "Training F1: 0.8392\n",
      "Test F1: 0.7755\n",
      "Epoch #262: Batch  90/90  -- Loss: 0.00073316\n",
      "Average Loss: 0.165742\n",
      "Training F1: 0.7937\n",
      "Test F1: 0.6765\n",
      "Epoch #263: Batch  90/90  -- Loss: 0.12166493\n",
      "Average Loss: 0.166972\n",
      "Training F1: 0.7667\n",
      "Test F1: 0.5574\n",
      "Epoch #264: Batch  90/90  -- Loss: 0.22688632\n",
      "Average Loss: 0.166045\n",
      "Training F1: 0.8214\n",
      "Test F1: 0.7324\n",
      "Epoch #265: Batch  90/90  -- Loss: 0.43803156\n",
      "Average Loss: 0.168005\n",
      "Training F1: 0.7434\n",
      "Test F1: 0.5088\n",
      "Epoch #266: Batch  90/90  -- Loss: 0.00296667\n",
      "Average Loss: 0.165536\n",
      "Training F1: 0.7807\n",
      "Test F1: 0.6617\n",
      "Epoch #267: Batch  90/90  -- Loss: 0.00013818\n",
      "Average Loss: 0.163534\n",
      "Training F1: 0.7793\n",
      "Test F1: 0.625\n",
      "Epoch #268: Batch  90/90  -- Loss: 0.15896642\n",
      "Average Loss: 0.165268\n",
      "Training F1: 0.7781\n",
      "Test F1: 0.6142\n",
      "Epoch #269: Batch  90/90  -- Loss: 0.22576181\n",
      "Average Loss: 0.165393\n",
      "Training F1: 0.7284\n",
      "Test F1: 0.5263\n",
      "Epoch #270: Batch  90/90  -- Loss: 0.42916436\n",
      "Average Loss: 0.167214\n",
      "Training F1: 0.7807\n",
      "Test F1: 0.6716\n",
      "Epoch #271: Batch  90/90  -- Loss: 0.16428962\n",
      "Average Loss: 0.163603\n",
      "Training F1: 0.7831\n",
      "Test F1: 0.6861\n",
      "Epoch #272: Batch  90/90  -- Loss: 0.37253137\n",
      "Average Loss: 0.165353\n",
      "Training F1: 0.7859\n",
      "Test F1: 0.6565\n",
      "Epoch #273: Batch  90/90  -- Loss: 0.00032348\n",
      "Average Loss: 0.162406\n",
      "Training F1: 0.7799\n",
      "Test F1: 0.592\n",
      "Epoch #274: Batch  90/90  -- Loss: 0.31280748\n",
      "Average Loss: 0.163044\n",
      "Training F1: 0.7817\n",
      "Test F1: 0.6716\n",
      "Epoch #275: Batch  90/90  -- Loss: 0.10324618\n",
      "Average Loss: 0.162129\n",
      "Training F1: 0.7635\n",
      "Test F1: 0.5424\n",
      "Epoch #276: Batch  90/90  -- Loss: 0.22375724\n",
      "Average Loss: 0.162932\n",
      "Training F1: 0.7381\n",
      "Test F1: 0.5\n",
      "Epoch #277: Batch  90/90  -- Loss: 0.23179676\n",
      "Average Loss: 0.16147\n",
      "Training F1: 0.7892\n",
      "Test F1: 0.6866\n",
      "Epoch #278: Batch  90/90  -- Loss: 0.13238343\n",
      "Average Loss: 0.159625\n",
      "Training F1: 0.7935\n",
      "Test F1: 0.6866\n",
      "Epoch #279: Batch  90/90  -- Loss: 0.00039994\n",
      "Average Loss: 0.157998\n",
      "Training F1: 0.7697\n",
      "Test F1: 0.6129\n",
      "Epoch #280: Batch  90/90  -- Loss: 0.35204185\n",
      "Average Loss: 0.158516\n",
      "Training F1: 0.8021\n",
      "Test F1: 0.7007\n",
      "Epoch #281: Batch  90/90  -- Loss: 0.00283337\n",
      "Average Loss: 0.15599\n",
      "Training F1: 0.8098\n",
      "Test F1: 0.697\n",
      "Epoch #282: Batch  90/90  -- Loss: 0.09963466\n",
      "Average Loss: 0.153038\n",
      "Training F1: 0.8205\n",
      "Test F1: 0.7143\n",
      "Epoch #283: Batch  90/90  -- Loss: 0.00069743\n",
      "Average Loss: 0.149403\n",
      "Training F1: 0.8127\n",
      "Test F1: 0.7101\n",
      "Epoch #284: Batch  90/90  -- Loss: 0.00053002\n",
      "Average Loss: 0.145335\n",
      "Training F1: 0.8165\n",
      "Test F1: 0.7143\n",
      "Epoch #285: Batch  90/90  -- Loss: 0.00057602\n",
      "Average Loss: 0.141754\n",
      "Training F1: 0.8649\n",
      "Test F1: 0.7755\n",
      "Epoch #286: Batch  90/90  -- Loss: 0.01037911\n",
      "Average Loss: 0.139315\n",
      "Training F1: 0.8894\n",
      "Test F1: 0.8387\n",
      "Epoch #287: Batch  90/90  -- Loss: 0.04847983\n",
      "Average Loss: 0.134821\n",
      "Training F1: 0.8947\n",
      "Test F1: 0.8535\n",
      "Epoch #288: Batch  90/90  -- Loss: 0.26871375\n",
      "Average Loss: 0.133199\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #289: Batch  90/90  -- Loss: 0.13942186\n",
      "Average Loss: 0.130272\n",
      "Training F1: 0.8841\n",
      "Test F1: 0.8235\n",
      "Epoch #290: Batch  90/90  -- Loss: 0.38017253\n",
      "Average Loss: 0.129059\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #291: Batch  90/90  -- Loss: 0.57206918\n",
      "Average Loss: 0.127685\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #292: Batch  90/90  -- Loss: 0.43848267\n",
      "Average Loss: 0.124176\n",
      "Training F1: 0.8947\n",
      "Test F1: 0.8535\n",
      "Epoch #293: Batch  90/90  -- Loss: 0.23553025\n",
      "Average Loss: 0.123633\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #294: Batch  90/90  -- Loss: 0.09644411\n",
      "Average Loss: 0.119924\n",
      "Training F1: 0.8974\n",
      "Test F1: 0.8535\n",
      "Epoch #295: Batch  90/90  -- Loss: 0.71058868\n",
      "Average Loss: 0.123932\n",
      "Training F1: 0.8964\n",
      "Test F1: 0.8235\n",
      "Epoch #296: Batch  90/90  -- Loss: 0.11131929\n",
      "Average Loss: 0.152538\n",
      "Training F1: 0.8957\n",
      "Test F1: 0.8535\n",
      "Epoch #297: Batch  90/90  -- Loss: 0.00021031\n",
      "Average Loss: 0.119116\n",
      "Training F1: 0.8974\n",
      "Test F1: 0.8535\n",
      "Epoch #298: Batch  90/90  -- Loss: 0.00536441\n",
      "Average Loss: 0.115455\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #299: Batch  90/90  -- Loss: 0.00023837\n",
      "Average Loss: 0.112791\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #300: Batch  90/90  -- Loss: 0.10217777\n",
      "Average Loss: 0.112939\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #301: Batch  90/90  -- Loss: 0.00185719\n",
      "Average Loss: 0.110295\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #302: Batch  90/90  -- Loss: 0.00014852\n",
      "Average Loss: 0.108671\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #303: Batch  90/90  -- Loss: 0.09121127\n",
      "Average Loss: 0.108147\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #304: Batch  90/90  -- Loss: 0.12683125\n",
      "Average Loss: 0.10729\n",
      "Training F1: 0.8995\n",
      "Test F1: 0.8645\n",
      "Epoch #305: Batch  90/90  -- Loss: 0.00018439\n",
      "Average Loss: 0.105907\n",
      "Training F1: 0.8974\n",
      "Test F1: 0.859\n",
      "Epoch #306: Batch  90/90  -- Loss: 0.23336098\n",
      "Average Loss: 0.103488\n",
      "Training F1: 0.9\n",
      "Test F1: 0.8535\n",
      "Epoch #307: Batch  90/90  -- Loss: 0.06321649\n",
      "Average Loss: 0.0994247\n",
      "Training F1: 0.91\n",
      "Test F1: 0.8874\n",
      "Epoch #308: Batch  90/90  -- Loss: 0.00131265\n",
      "Average Loss: 0.0946113\n",
      "Training F1: 0.9235\n",
      "Test F1: 0.8993\n",
      "Epoch #309: Batch  90/90  -- Loss: 0.00337533\n",
      "Average Loss: 0.089413\n",
      "Training F1: 0.9353\n",
      "Test F1: 0.9241\n",
      "Epoch #310: Batch  90/90  -- Loss: 0.27208379\n",
      "Average Loss: 0.0843152\n",
      "Training F1: 0.9356\n",
      "Test F1: 0.9241\n",
      "Epoch #311: Batch  90/90  -- Loss: 0.06859922\n",
      "Average Loss: 0.0763413\n",
      "Training F1: 0.9692\n",
      "Test F1: 0.9371\n",
      "Epoch #312: Batch  90/90  -- Loss: 0.18258683\n",
      "Average Loss: 0.0706568\n",
      "Training F1: 0.9767\n",
      "Test F1: 0.9437\n",
      "Epoch #313: Batch  90/90  -- Loss: 0.00435967\n",
      "Average Loss: 0.062482\n",
      "Training F1: 0.9947\n",
      "Test F1: 0.9926\n",
      "Epoch #314: Batch  90/90  -- Loss: 0.01989894\n",
      "Average Loss: 0.057102\n",
      "Training F1: 0.9717\n",
      "Test F1: 0.9371\n",
      "Epoch #315: Batch  90/90  -- Loss: 0.00010694\n",
      "Average Loss: 0.0515704\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9926\n",
      "Epoch #316: Batch  90/90  -- Loss: 0.00077402\n",
      "Average Loss: 0.0451834\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #317: Batch  90/90  -- Loss: 0.00163411\n",
      "Average Loss: 0.0378547\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #318: Batch  90/90  -- Loss: 0.01604178\n",
      "Average Loss: 0.0329759\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #319: Batch  90/90  -- Loss: 0.00029375\n",
      "Average Loss: 0.0287492\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #320: Batch  90/90  -- Loss: 0.01608242\n",
      "Average Loss: 0.0250706\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #321: Batch  90/90  -- Loss: 0.01491693\n",
      "Average Loss: 0.022023\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #322: Batch  90/90  -- Loss: 0.04549436\n",
      "Average Loss: 0.0194435\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #323: Batch  90/90  -- Loss: 0.05307327\n",
      "Average Loss: 0.0168778\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #324: Batch  90/90  -- Loss: 0.01318458\n",
      "Average Loss: 0.0142086\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #325: Batch  90/90  -- Loss: 0.03327974\n",
      "Average Loss: 0.0119746\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #326: Batch  90/90  -- Loss: 0.01036117\n",
      "Average Loss: 0.00972016\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #327: Batch  90/90  -- Loss: 0.00631843\n",
      "Average Loss: 0.0080781\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #328: Batch  90/90  -- Loss: 7.7569e-05\n",
      "Average Loss: 0.00686039\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #329: Batch  90/90  -- Loss: 0.00626395\n",
      "Average Loss: 0.00643535\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #330: Batch  90/90  -- Loss: 0.00476993\n",
      "Average Loss: 0.0053989\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #331: Batch  90/90  -- Loss: 0.00679585\n",
      "Average Loss: 0.00490524\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #332: Batch  90/90  -- Loss: 0.01178458\n",
      "Average Loss: 0.00448421\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #333: Batch  90/90  -- Loss: 5.674e-055\n",
      "Average Loss: 0.00408089\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #334: Batch  90/90  -- Loss: 0.00941173\n",
      "Average Loss: 0.00378631\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #335: Batch  90/90  -- Loss: 0.00206141\n",
      "Average Loss: 0.00344465\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #336: Batch  90/90  -- Loss: 0.00861195\n",
      "Average Loss: 0.00325801\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #337: Batch  90/90  -- Loss: 0.00200455\n",
      "Average Loss: 0.00296937\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #338: Batch  90/90  -- Loss: 0.00191481\n",
      "Average Loss: 0.00276489\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #339: Batch  90/90  -- Loss: 0.00319292\n",
      "Average Loss: 0.00260726\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #340: Batch  90/90  -- Loss: 0.00013882\n",
      "Average Loss: 0.00241803\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #341: Batch  90/90  -- Loss: 0.00489934\n",
      "Average Loss: 0.00228401\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #342: Batch  90/90  -- Loss: 0.00017909\n",
      "Average Loss: 0.00211959\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #343: Batch  90/90  -- Loss: 0.00391505\n",
      "Average Loss: 0.00200058\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #344: Batch  90/90  -- Loss: 6.8211e-05\n",
      "Average Loss: 0.00187363\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #345: Batch  90/90  -- Loss: 8.1675e-05\n",
      "Average Loss: 0.00176456\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #346: Batch  90/90  -- Loss: 0.00162096\n",
      "Average Loss: 0.00167547\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #347: Batch  90/90  -- Loss: 0.00147698\n",
      "Average Loss: 0.0015741\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #348: Batch  90/90  -- Loss: 7.1193e-05\n",
      "Average Loss: 0.0014778\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #349: Batch  90/90  -- Loss: 0.00050339\n",
      "Average Loss: 0.00139699\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #350: Batch  90/90  -- Loss: 0.00089369\n",
      "Average Loss: 0.00131501\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #351: Batch  90/90  -- Loss: 6.3502e-05\n",
      "Average Loss: 0.00124051\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #352: Batch  90/90  -- Loss: 4.9886e-05\n",
      "Average Loss: 0.00117603\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #353: Batch  90/90  -- Loss: 2.2083e-05\n",
      "Average Loss: 0.00111186\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #354: Batch  90/90  -- Loss: 0.00226052\n",
      "Average Loss: 0.00105626\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #355: Batch  90/90  -- Loss: 0.00240742\n",
      "Average Loss: 0.00100286\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #356: Batch  90/90  -- Loss: 6.0553e-05\n",
      "Average Loss: 0.000936679\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #357: Batch  90/90  -- Loss: 0.00127848\n",
      "Average Loss: 0.00089153\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #358: Batch  90/90  -- Loss: 0.00193491\n",
      "Average Loss: 0.000844178\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #359: Batch  90/90  -- Loss: 0.00150848\n",
      "Average Loss: 0.000796918\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #360: Batch  90/90  -- Loss: 0.00156733\n",
      "Average Loss: 0.00075601\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #361: Batch  90/90  -- Loss: 0.00029119\n",
      "Average Loss: 0.000708177\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #362: Batch  90/90  -- Loss: 1.9699e-05\n",
      "Average Loss: 0.000667026\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #363: Batch  90/90  -- Loss: 0.00027904\n",
      "Average Loss: 0.000632269\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #364: Batch  90/90  -- Loss: 0.00037113\n",
      "Average Loss: 0.000601462\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #365: Batch  90/90  -- Loss: 0.00046384\n",
      "Average Loss: 0.00056798\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #366: Batch  90/90  -- Loss: 0.00013487\n",
      "Average Loss: 0.00053591\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #367: Batch  90/90  -- Loss: 0.00179377\n",
      "Average Loss: 0.00051599\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #368: Batch  90/90  -- Loss: 0.00126655\n",
      "Average Loss: 0.000486427\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #369: Batch  90/90  -- Loss: 0.00029923\n",
      "Average Loss: 0.000453102\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #370: Batch  90/90  -- Loss: 0.00096853\n",
      "Average Loss: 0.00043303\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #371: Batch  90/90  -- Loss: 0.00029665\n",
      "Average Loss: 0.000405713\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #372: Batch  90/90  -- Loss: 0.00029945\n",
      "Average Loss: 0.000384144\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #373: Batch  90/90  -- Loss: 0.00028851\n",
      "Average Loss: 0.000362834\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #374: Batch  90/90  -- Loss: 0.00034813\n",
      "Average Loss: 0.00034349\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #375: Batch  90/90  -- Loss: 0.00041163\n",
      "Average Loss: 0.000326025\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #376: Batch  90/90  -- Loss: 0.00010754\n",
      "Average Loss: 0.000306129\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #377: Batch  90/90  -- Loss: 6.02e-0605\n",
      "Average Loss: 0.000288351\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #378: Batch  90/90  -- Loss: 9.6515e-05\n",
      "Average Loss: 0.000273272\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #379: Batch  90/90  -- Loss: 0.00082928\n",
      "Average Loss: 0.000263161\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #380: Batch  90/90  -- Loss: 1.2278e-05\n",
      "Average Loss: 0.000243699\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #381: Batch  90/90  -- Loss: 0.00033954\n",
      "Average Loss: 0.000231967\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #382: Batch  90/90  -- Loss: 0.00020461\n",
      "Average Loss: 0.000219145\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #383: Batch  90/90  -- Loss: 9.5367e-07\n",
      "Average Loss: 0.000205733\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #384: Batch  90/90  -- Loss: 4.7087e-06\n",
      "Average Loss: 0.000194885\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #385: Batch  90/90  -- Loss: 0.00060119\n",
      "Average Loss: 0.000187443\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #386: Batch  90/90  -- Loss: 6.9609e-05\n",
      "Average Loss: 0.000174906\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #387: Batch  90/90  -- Loss: 0.00035938\n",
      "Average Loss: 0.000166791\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #388: Batch  90/90  -- Loss: 0.00017733\n",
      "Average Loss: 0.000156297\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #389: Batch  90/90  -- Loss: 0.00033521\n",
      "Average Loss: 0.00014875\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #390: Batch  90/90  -- Loss: 3.8743e-06\n",
      "Average Loss: 0.00013882\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #391: Batch  90/90  -- Loss: 0.00011574\n",
      "Average Loss: 0.000132247\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #392: Batch  90/90  -- Loss: 9.1194e-06\n",
      "Average Loss: 0.000124628\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #393: Batch  90/90  -- Loss: 0.00040199\n",
      "Average Loss: 0.000119969\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #394: Batch  90/90  -- Loss: 0.00036238\n",
      "Average Loss: 0.000113513\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #395: Batch  90/90  -- Loss: 0.00021444\n",
      "Average Loss: 0.000106571\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #396: Batch  90/90  -- Loss: 3.6955e-06\n",
      "Average Loss: 9.98295e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #397: Batch  90/90  -- Loss: 1.1086e-05\n",
      "Average Loss: 9.47028e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #398: Batch  90/90  -- Loss: 7.7714e-05\n",
      "Average Loss: 8.99239e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #399: Batch  90/90  -- Loss: 5.3048e-06\n",
      "Average Loss: 8.49842e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #400: Batch  90/90  -- Loss: 1.4901e-07\n",
      "Average Loss: 8.06104e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #401: Batch  90/90  -- Loss: 0.00012787\n",
      "Average Loss: 7.70512e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #402: Batch  90/90  -- Loss: 2.4229e-05\n",
      "Average Loss: 7.25133e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #403: Batch  90/90  -- Loss: 8.5549e-05\n",
      "Average Loss: 6.90932e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #404: Batch  90/90  -- Loss: 0.00014869\n",
      "Average Loss: 6.61542e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #405: Batch  90/90  -- Loss: 0.00013719\n",
      "Average Loss: 6.26816e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #406: Batch  90/90  -- Loss: 3.5763e-07\n",
      "Average Loss: 5.86767e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #407: Batch  90/90  -- Loss: 0.00010688\n",
      "Average Loss: 5.64128e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #408: Batch  90/90  -- Loss: 0.00020811\n",
      "Average Loss: 5.40212e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #409: Batch  90/90  -- Loss: 5.4951e-05\n",
      "Average Loss: 5.05255e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #410: Batch  90/90  -- Loss: 4.1333e-05\n",
      "Average Loss: 4.79143e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #411: Batch  90/90  -- Loss: 4.6012e-05\n",
      "Average Loss: 4.55171e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #412: Batch  90/90  -- Loss: 5.4836e-06\n",
      "Average Loss: 4.31582e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #413: Batch  90/90  -- Loss: 1.8715e-05\n",
      "Average Loss: 4.10923e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #414: Batch  90/90  -- Loss: 1.3053e-05\n",
      "Average Loss: 3.91254e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #415: Batch  90/90  -- Loss: 4.5355e-05\n",
      "Average Loss: 3.73936e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #416: Batch  90/90  -- Loss: 1.5199e-06\n",
      "Average Loss: 3.53524e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #417: Batch  90/90  -- Loss: 7.0835e-05\n",
      "Average Loss: 3.39477e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #418: Batch  90/90  -- Loss: 7.9894e-05\n",
      "Average Loss: 3.24406e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #419: Batch  90/90  -- Loss: 4.6193e-06\n",
      "Average Loss: 3.04923e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #420: Batch  90/90  -- Loss: 4.1723e-07\n",
      "Average Loss: 2.89992e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #421: Batch  90/90  -- Loss: 5.0664e-07\n",
      "Average Loss: 2.75783e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #422: Batch  90/90  -- Loss: 1.3202e-05\n",
      "Average Loss: 2.6369e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #423: Batch  90/90  -- Loss: 5.1672e-05\n",
      "Average Loss: 2.53443e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #424: Batch  90/90  -- Loss: 3.1588e-05\n",
      "Average Loss: 2.40306e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #425: Batch  90/90  -- Loss: 3.7727e-05\n",
      "Average Loss: 2.29658e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #426: Batch  90/90  -- Loss: 6.3268e-05\n",
      "Average Loss: 2.19914e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #427: Batch  90/90  -- Loss: 3.3824e-05\n",
      "Average Loss: 2.08362e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #428: Batch  90/90  -- Loss: 4.5597e-06\n",
      "Average Loss: 1.96799e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #429: Batch  90/90  -- Loss: 3.6387e-05\n",
      "Average Loss: 1.89462e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #430: Batch  90/90  -- Loss: 2.3096e-05\n",
      "Average Loss: 1.80193e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #431: Batch  90/90  -- Loss: 0.0573e-05\n",
      "Average Loss: 1.70312e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #432: Batch  90/90  -- Loss: 3.3467e-05\n",
      "Average Loss: 1.64508e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #433: Batch  90/90  -- Loss: 1.8268e-05\n",
      "Average Loss: 1.55975e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #434: Batch  90/90  -- Loss: 2.241e-056\n",
      "Average Loss: 1.49033e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #435: Batch  90/90  -- Loss: 3.9516e-05\n",
      "Average Loss: 1.43658e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #436: Batch  90/90  -- Loss: 2.0564e-06\n",
      "Average Loss: 1.34588e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #437: Batch  90/90  -- Loss: 1.1563e-05\n",
      "Average Loss: 1.29054e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #438: Batch  90/90  -- Loss: 1.9639e-05\n",
      "Average Loss: 1.23804e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #439: Batch  90/90  -- Loss: 1.1325e-05\n",
      "Average Loss: 1.17479e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #440: Batch  90/90  -- Loss: 1.6004e-05\n",
      "Average Loss: 1.12554e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #441: Batch  90/90  -- Loss: 1.0073e-05\n",
      "Average Loss: 1.07026e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #442: Batch  90/90  -- Loss: 1.2099e-05\n",
      "Average Loss: 1.02279e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #443: Batch  90/90  -- Loss: 1.0311e-05\n",
      "Average Loss: 9.78224e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #444: Batch  90/90  -- Loss: 1.055e-055\n",
      "Average Loss: 9.31354e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #445: Batch  90/90  -- Loss: 3.3378e-06\n",
      "Average Loss: 8.85642e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #446: Batch  90/90  -- Loss: 1.0133e-05\n",
      "Average Loss: 8.49996e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #447: Batch  90/90  -- Loss: 1.0133e-06\n",
      "Average Loss: 8.06288e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #448: Batch  90/90  -- Loss: 6.5565e-07\n",
      "Average Loss: 7.69798e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #449: Batch  90/90  -- Loss: 0.0222e-05\n",
      "Average Loss: 7.35708e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #450: Batch  90/90  -- Loss: 8.7916e-06\n",
      "Average Loss: 7.06983e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #451: Batch  90/90  -- Loss: 1.4424e-05\n",
      "Average Loss: 6.78026e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #452: Batch  90/90  -- Loss: 1.4812e-05\n",
      "Average Loss: 6.4796e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #453: Batch  90/90  -- Loss: 1.4394e-05\n",
      "Average Loss: 6.19615e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #454: Batch  90/90  -- Loss: 8.0466e-07\n",
      "Average Loss: 5.83721e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #455: Batch  90/90  -- Loss: 7.0333e-06\n",
      "Average Loss: 5.6084e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #456: Batch  90/90  -- Loss: 1.4603e-06\n",
      "Average Loss: 5.33373e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #457: Batch  90/90  -- Loss: 1.6093e-06\n",
      "Average Loss: 5.09283e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #458: Batch  90/90  -- Loss: 5.3644e-07\n",
      "Average Loss: 4.85094e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #459: Batch  90/90  -- Loss: 1.4901e-07\n",
      "Average Loss: 4.63256e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #460: Batch  90/90  -- Loss: 1.3709e-06\n",
      "Average Loss: 4.4357e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #461: Batch  90/90  -- Loss: 9.9241e-06\n",
      "Average Loss: 4.28123e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #462: Batch  90/90  -- Loss: 5.9605e-08\n",
      "Average Loss: 4.03354e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #463: Batch  90/90  -- Loss: 1.8477e-06\n",
      "Average Loss: 3.85804e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #464: Batch  90/90  -- Loss: 1.7881e-07\n",
      "Average Loss: 3.6799e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #465: Batch  90/90  -- Loss: 1.4663e-05\n",
      "Average Loss: 3.58734e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #466: Batch  90/90  -- Loss: 6.9439e-06\n",
      "Average Loss: 3.37857e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #467: Batch  90/90  -- Loss: 6.6458e-06\n",
      "Average Loss: 3.21449e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #468: Batch  90/90  -- Loss: 4.0531e-06\n",
      "Average Loss: 3.05157e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #469: Batch  90/90  -- Loss: 1.7881e-07\n",
      "Average Loss: 2.89097e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #470: Batch  90/90  -- Loss: 2.9802e-08\n",
      "Average Loss: 2.74329e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #471: Batch  90/90  -- Loss: 3.0994e-06\n",
      "Average Loss: 2.62656e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #472: Batch  90/90  -- Loss: 4.3213e-06\n",
      "Average Loss: 2.50057e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #473: Batch  90/90  -- Loss: 2.563e-066\n",
      "Average Loss: 2.3797e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #474: Batch  90/90  -- Loss: 2.9504e-06\n",
      "Average Loss: 2.27225e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #475: Batch  90/90  -- Loss: 2.3842e-07\n",
      "Average Loss: 2.14609e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #476: Batch  90/90  -- Loss: 5.1558e-06\n",
      "Average Loss: 2.07208e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #477: Batch  90/90  -- Loss: 2.0862e-07\n",
      "Average Loss: 1.95088e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #478: Batch  90/90  -- Loss: 3.5167e-06\n",
      "Average Loss: 1.88234e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #479: Batch  90/90  -- Loss: 6.3181e-06\n",
      "Average Loss: 1.81247e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #480: Batch  90/90  -- Loss: 1.1921e-07\n",
      "Average Loss: 1.69757e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #481: Batch  90/90  -- Loss: 4.0531e-06\n",
      "Average Loss: 1.64425e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #482: Batch  90/90  -- Loss: 5.9605e-07\n",
      "Average Loss: 1.55187e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #483: Batch  90/90  -- Loss: 1.1325e-06\n",
      "Average Loss: 1.48548e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #484: Batch  90/90  -- Loss: 3.5763e-06\n",
      "Average Loss: 1.4315e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #485: Batch  90/90  -- Loss: 1.0729e-06\n",
      "Average Loss: 1.35335e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #486: Batch  90/90  -- Loss: 1.3709e-06\n",
      "Average Loss: 1.2964e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #487: Batch  90/90  -- Loss: 2.9802e-08\n",
      "Average Loss: 1.22885e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #488: Batch  90/90  -- Loss: 5.9605e-08\n",
      "Average Loss: 1.1747e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #489: Batch  90/90  -- Loss: 2.0862e-07\n",
      "Average Loss: 1.12603e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #490: Batch  90/90  -- Loss: 0.0838e-06\n",
      "Average Loss: 1.07305e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #491: Batch  90/90  -- Loss: 1.3709e-06\n",
      "Average Loss: 1.03414e-06\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #492: Batch  90/90  -- Loss: 2.116e-067\n",
      "Average Loss: 9.90925e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9926\n",
      "Epoch #493: Batch  90/90  -- Loss: 8.3446e-07\n",
      "Average Loss: 9.41586e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #494: Batch  90/90  -- Loss: 1.5795e-06\n",
      "Average Loss: 9.0483e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #495: Batch  90/90  -- Loss: 9.5367e-07\n",
      "Average Loss: 8.63769e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #496: Batch  90/90  -- Loss: 1.7881e-06\n",
      "Average Loss: 8.29496e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #497: Batch  90/90  -- Loss: 1.0729e-06\n",
      "Average Loss: 7.90422e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #498: Batch  90/90  -- Loss: 1.4901e-07\n",
      "Average Loss: 7.49527e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #499: Batch  90/90  -- Loss: 0.0007e-06\n",
      "Average Loss: 7.17573e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n",
      "Epoch #500: Batch  90/90  -- Loss: 1.7583e-06\n",
      "Average Loss: 6.9489e-07\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMSeqLabel(\n",
       "  (lstm): LSTM(\n",
       "    (model): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (model_rev): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(model, train_x, train_y, test_x, test_y, epochs=500, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PyTorch LSTM module\n",
    "class PyTorchBaseline(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_output, \n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = n_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layers = layers\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.lstm = nn.LSTM(n_input, n_hidden, bidirectional=self.bidirectional, num_layers=layers)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * n_hidden, n_output)\n",
    "        else:\n",
    "            self.fc = nn.Linear(n_hidden, n_output)\n",
    "        if self.layernorm and self.bidirectional:\n",
    "            self.ln = LayerNorm(2 * self.hidden_dim)\n",
    "        elif self.layernorm:\n",
    "            self.ln = LayerNorm(self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        o, (_, _) = self.lstm(x, (h, c))\n",
    "        o = o[-1].unsqueeze(0)\n",
    "        if self.layernorm:\n",
    "            output = self.fc(self.ln(o))\n",
    "        else:\n",
    "            output = self.fc(o)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2593\n"
     ]
    }
   ],
   "source": [
    "model = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(model.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.45927\n",
      "Average Loss: 0.678364\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.56302\n",
      "Average Loss: 0.58079\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.56522\n",
      "Average Loss: 0.578788\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.31799\n",
      "Average Loss: 0.577131\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.30906\n",
      "Average Loss: 0.576287\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.55948\n",
      "Average Loss: 0.577445\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  90/90  -- Loss: 0.57317\n",
      "Average Loss: 0.578531\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #8  : Batch  90/90  -- Loss: 0.81123\n",
      "Average Loss: 0.578521\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #9  : Batch  90/90  -- Loss: 0.30972\n",
      "Average Loss: 0.575707\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #10 : Batch  90/90  -- Loss: 0.55801\n",
      "Average Loss: 0.576991\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #11 : Batch  90/90  -- Loss: 0.31412\n",
      "Average Loss: 0.575952\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #12 : Batch  90/90  -- Loss: 0.30603\n",
      "Average Loss: 0.575915\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #13 : Batch  90/90  -- Loss: 0.78797\n",
      "Average Loss: 0.578051\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #14 : Batch  90/90  -- Loss: 1.03839\n",
      "Average Loss: 0.580424\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #15 : Batch  90/90  -- Loss: 0.55272\n",
      "Average Loss: 0.576645\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #16 : Batch  90/90  -- Loss: 0.55262\n",
      "Average Loss: 0.577597\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #17 : Batch  90/90  -- Loss: 0.52898\n",
      "Average Loss: 0.576444\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #18 : Batch  90/90  -- Loss: 0.84815\n",
      "Average Loss: 0.57811\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #19 : Batch  90/90  -- Loss: 0.80738\n",
      "Average Loss: 0.577701\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #20 : Batch  90/90  -- Loss: 0.57663\n",
      "Average Loss: 0.576937\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #21 : Batch  90/90  -- Loss: 0.81842\n",
      "Average Loss: 0.577756\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #22 : Batch  90/90  -- Loss: 1.03877\n",
      "Average Loss: 0.579362\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #23 : Batch  90/90  -- Loss: 0.55212\n",
      "Average Loss: 0.576739\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #24 : Batch  90/90  -- Loss: 0.81442\n",
      "Average Loss: 0.577783\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #25 : Batch  90/90  -- Loss: 0.60513\n",
      "Average Loss: 0.576617\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #26 : Batch  90/90  -- Loss: 0.30701\n",
      "Average Loss: 0.575646\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #27 : Batch  90/90  -- Loss: 0.56233\n",
      "Average Loss: 0.576914\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #28 : Batch  90/90  -- Loss: 0.85302\n",
      "Average Loss: 0.578225\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #29 : Batch  90/90  -- Loss: 0.57689\n",
      "Average Loss: 0.576876\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #30 : Batch  90/90  -- Loss: 0.82112\n",
      "Average Loss: 0.578053\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #31 : Batch  90/90  -- Loss: 0.31983\n",
      "Average Loss: 0.575321\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #32 : Batch  90/90  -- Loss: 0.29624\n",
      "Average Loss: 0.574995\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #33 : Batch  90/90  -- Loss: 0.79111\n",
      "Average Loss: 0.578239\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #34 : Batch  90/90  -- Loss: 0.29486\n",
      "Average Loss: 0.575172\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #35 : Batch  90/90  -- Loss: 0.78686\n",
      "Average Loss: 0.577653\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #36 : Batch  90/90  -- Loss: 0.54979\n",
      "Average Loss: 0.576256\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #37 : Batch  90/90  -- Loss: 0.29126\n",
      "Average Loss: 0.5754\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #38 : Batch  90/90  -- Loss: 0.81443\n",
      "Average Loss: 0.577713\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #39 : Batch  90/90  -- Loss: 0.85258\n",
      "Average Loss: 0.578138\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #40 : Batch  90/90  -- Loss: 0.29686\n",
      "Average Loss: 0.575455\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #41 : Batch  90/90  -- Loss: 0.77059\n",
      "Average Loss: 0.577748\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #42 : Batch  90/90  -- Loss: 0.31043\n",
      "Average Loss: 0.575604\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #43 : Batch  90/90  -- Loss: 0.53674\n",
      "Average Loss: 0.576645\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #44 : Batch  90/90  -- Loss: 0.82355\n",
      "Average Loss: 0.578025\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #45 : Batch  90/90  -- Loss: 0.30144\n",
      "Average Loss: 0.575158\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #46 : Batch  90/90  -- Loss: 0.31179\n",
      "Average Loss: 0.575078\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #47 : Batch  90/90  -- Loss: 0.78768\n",
      "Average Loss: 0.577787\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #48 : Batch  90/90  -- Loss: 0.60895\n",
      "Average Loss: 0.577026\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #49 : Batch  90/90  -- Loss: 0.55028\n",
      "Average Loss: 0.576564\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #50 : Batch  90/90  -- Loss: 0.57721\n",
      "Average Loss: 0.576698\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #51 : Batch  90/90  -- Loss: 0.59437\n",
      "Average Loss: 0.576484\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #52 : Batch  90/90  -- Loss: 0.78968\n",
      "Average Loss: 0.577843\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #53 : Batch  90/90  -- Loss: 0.32531\n",
      "Average Loss: 0.575598\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #54 : Batch  90/90  -- Loss: 0.30624\n",
      "Average Loss: 0.575284\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #55 : Batch  90/90  -- Loss: 0.57703\n",
      "Average Loss: 0.576677\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #56 : Batch  90/90  -- Loss: 0.29076\n",
      "Average Loss: 0.575066\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #57 : Batch  90/90  -- Loss: 0.56268\n",
      "Average Loss: 0.576645\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #58 : Batch  90/90  -- Loss: 0.82868\n",
      "Average Loss: 0.578071\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #59 : Batch  90/90  -- Loss: 1.06677\n",
      "Average Loss: 0.580144\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #60 : Batch  90/90  -- Loss: 0.59267\n",
      "Average Loss: 0.577225\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #61 : Batch  90/90  -- Loss: 0.57811\n",
      "Average Loss: 0.576979\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #62 : Batch  90/90  -- Loss: 0.31088\n",
      "Average Loss: 0.575052\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #63 : Batch  90/90  -- Loss: 0.59267\n",
      "Average Loss: 0.577113\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #64 : Batch  90/90  -- Loss: 0.56519\n",
      "Average Loss: 0.576575\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #65 : Batch  90/90  -- Loss: 0.53515\n",
      "Average Loss: 0.576316\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #66 : Batch  90/90  -- Loss: 0.30448\n",
      "Average Loss: 0.575217\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #67 : Batch  90/90  -- Loss: 0.53449\n",
      "Average Loss: 0.576738\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #68 : Batch  90/90  -- Loss: 0.82501\n",
      "Average Loss: 0.577752\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #69 : Batch  90/90  -- Loss: 0.87069\n",
      "Average Loss: 0.578326\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #70 : Batch  90/90  -- Loss: 0.83639\n",
      "Average Loss: 0.578353\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #71 : Batch  90/90  -- Loss: 0.54971\n",
      "Average Loss: 0.576354\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #72 : Batch  90/90  -- Loss: 0.32756\n",
      "Average Loss: 0.575456\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #73 : Batch  90/90  -- Loss: 0.33036\n",
      "Average Loss: 0.575903\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #74 : Batch  90/90  -- Loss: 0.55071\n",
      "Average Loss: 0.576138\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #75 : Batch  90/90  -- Loss: 0.88238\n",
      "Average Loss: 0.578452\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #76 : Batch  90/90  -- Loss: 0.60759\n",
      "Average Loss: 0.577019\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #77 : Batch  90/90  -- Loss: 0.80953\n",
      "Average Loss: 0.577588\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #78 : Batch  90/90  -- Loss: 0.28382\n",
      "Average Loss: 0.575224\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #79 : Batch  90/90  -- Loss: 0.28486\n",
      "Average Loss: 0.574833\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #80 : Batch  90/90  -- Loss: 0.31045\n",
      "Average Loss: 0.575473\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #81 : Batch  90/90  -- Loss: 0.57764\n",
      "Average Loss: 0.576739\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #82 : Batch  90/90  -- Loss: 1.03173\n",
      "Average Loss: 0.57934\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #83 : Batch  90/90  -- Loss: 0.60944\n",
      "Average Loss: 0.576751\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #84 : Batch  90/90  -- Loss: 0.82447\n",
      "Average Loss: 0.57863\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #85 : Batch  90/90  -- Loss: 0.81932\n",
      "Average Loss: 0.577923\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #86 : Batch  90/90  -- Loss: 0.79364\n",
      "Average Loss: 0.5783\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #87 : Batch  90/90  -- Loss: 0.55259\n",
      "Average Loss: 0.577766\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #88 : Batch  90/90  -- Loss: 0.33411\n",
      "Average Loss: 0.575577\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #89 : Batch  90/90  -- Loss: 0.59335\n",
      "Average Loss: 0.576981\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #90 : Batch  90/90  -- Loss: 0.30669\n",
      "Average Loss: 0.574895\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #91 : Batch  90/90  -- Loss: 0.59173\n",
      "Average Loss: 0.576351\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #92 : Batch  90/90  -- Loss: 0.30928\n",
      "Average Loss: 0.574995\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #93 : Batch  90/90  -- Loss: 0.57807\n",
      "Average Loss: 0.576923\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #94 : Batch  90/90  -- Loss: 0.30678\n",
      "Average Loss: 0.575203\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #95 : Batch  90/90  -- Loss: 0.31104\n",
      "Average Loss: 0.574977\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #96 : Batch  90/90  -- Loss: 0.32276\n",
      "Average Loss: 0.575155\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #97 : Batch  90/90  -- Loss: 0.59241\n",
      "Average Loss: 0.576546\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #98 : Batch  90/90  -- Loss: 0.53682\n",
      "Average Loss: 0.5762\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #99 : Batch  90/90  -- Loss: 0.56277\n",
      "Average Loss: 0.576585\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #100: Batch  90/90  -- Loss: 0.32389\n",
      "Average Loss: 0.575248\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #101: Batch  90/90  -- Loss: 1.05248\n",
      "Average Loss: 0.579145\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #102: Batch  90/90  -- Loss: 0.31289\n",
      "Average Loss: 0.575047\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #103: Batch  90/90  -- Loss: 0.28806\n",
      "Average Loss: 0.574576\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #104: Batch  90/90  -- Loss: 0.53605\n",
      "Average Loss: 0.576443\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #105: Batch  90/90  -- Loss: 0.55172\n",
      "Average Loss: 0.576508\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #106: Batch  90/90  -- Loss: 0.31959\n",
      "Average Loss: 0.574751\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #107: Batch  90/90  -- Loss: 0.28603\n",
      "Average Loss: 0.575097\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #108: Batch  90/90  -- Loss: 0.57803\n",
      "Average Loss: 0.576475\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #109: Batch  90/90  -- Loss: 0.53576\n",
      "Average Loss: 0.576159\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #110: Batch  90/90  -- Loss: 0.30682\n",
      "Average Loss: 0.574869\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #111: Batch  90/90  -- Loss: 0.60705\n",
      "Average Loss: 0.577329\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #112: Batch  90/90  -- Loss: 0.56655\n",
      "Average Loss: 0.576337\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #113: Batch  90/90  -- Loss: 0.31337\n",
      "Average Loss: 0.576508\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #114: Batch  90/90  -- Loss: 0.53476\n",
      "Average Loss: 0.576234\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #115: Batch  90/90  -- Loss: 0.56468\n",
      "Average Loss: 0.576969\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #116: Batch  90/90  -- Loss: 0.52078\n",
      "Average Loss: 0.576288\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #117: Batch  90/90  -- Loss: 0.54995\n",
      "Average Loss: 0.576355\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #118: Batch  90/90  -- Loss: 1.08526\n",
      "Average Loss: 0.579452\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #119: Batch  90/90  -- Loss: 0.30354\n",
      "Average Loss: 0.576374\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #120: Batch  90/90  -- Loss: 0.84873\n",
      "Average Loss: 0.578\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #121: Batch  90/90  -- Loss: 0.80719\n",
      "Average Loss: 0.577744\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #122: Batch  90/90  -- Loss: 0.83546\n",
      "Average Loss: 0.57792\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #123: Batch  90/90  -- Loss: 0.30688\n",
      "Average Loss: 0.575451\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #124: Batch  90/90  -- Loss: 0.32887\n",
      "Average Loss: 0.575465\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #125: Batch  90/90  -- Loss: 0.79735\n",
      "Average Loss: 0.577442\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #126: Batch  90/90  -- Loss: 0.80658\n",
      "Average Loss: 0.577664\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #127: Batch  90/90  -- Loss: 0.53628\n",
      "Average Loss: 0.57662\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #128: Batch  90/90  -- Loss: 0.57676\n",
      "Average Loss: 0.577025\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #129: Batch  90/90  -- Loss: 1.08639\n",
      "Average Loss: 0.579548\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #130: Batch  90/90  -- Loss: 0.32511\n",
      "Average Loss: 0.5751\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #131: Batch  90/90  -- Loss: 0.55045\n",
      "Average Loss: 0.576752\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #132: Batch  90/90  -- Loss: 0.59347\n",
      "Average Loss: 0.577001\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #133: Batch  90/90  -- Loss: 0.55145\n",
      "Average Loss: 0.576191\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #134: Batch  90/90  -- Loss: 0.30867\n",
      "Average Loss: 0.574749\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #135: Batch  90/90  -- Loss: 0.53787\n",
      "Average Loss: 0.576349\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #136: Batch  90/90  -- Loss: 0.60879\n",
      "Average Loss: 0.576752\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #137: Batch  90/90  -- Loss: 0.28999\n",
      "Average Loss: 0.575273\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #138: Batch  90/90  -- Loss: 1.08365\n",
      "Average Loss: 0.579642\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #139: Batch  90/90  -- Loss: 0.57787\n",
      "Average Loss: 0.576537\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #140: Batch  90/90  -- Loss: 0.77741\n",
      "Average Loss: 0.578219\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #141: Batch  90/90  -- Loss: 0.60732\n",
      "Average Loss: 0.576404\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #142: Batch  90/90  -- Loss: 0.55036\n",
      "Average Loss: 0.576588\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #143: Batch  90/90  -- Loss: 0.80656\n",
      "Average Loss: 0.578051\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #144: Batch  90/90  -- Loss: 1.28874\n",
      "Average Loss: 0.580484\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #145: Batch  90/90  -- Loss: 0.55079\n",
      "Average Loss: 0.576215\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #146: Batch  90/90  -- Loss: 0.82334\n",
      "Average Loss: 0.577994\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #147: Batch  90/90  -- Loss: 0.57744\n",
      "Average Loss: 0.576727\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #148: Batch  90/90  -- Loss: 0.78106\n",
      "Average Loss: 0.577806\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #149: Batch  90/90  -- Loss: 0.57806\n",
      "Average Loss: 0.576557\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #150: Batch  90/90  -- Loss: 0.27384\n",
      "Average Loss: 0.574735\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #151: Batch  90/90  -- Loss: 0.59398\n",
      "Average Loss: 0.576647\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #152: Batch  90/90  -- Loss: 0.56421\n",
      "Average Loss: 0.576396\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #153: Batch  90/90  -- Loss: 0.32662\n",
      "Average Loss: 0.575044\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #154: Batch  90/90  -- Loss: 0.79452\n",
      "Average Loss: 0.57768\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #155: Batch  90/90  -- Loss: 0.27547\n",
      "Average Loss: 0.574671\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #156: Batch  90/90  -- Loss: 0.30242\n",
      "Average Loss: 0.574782\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #157: Batch  90/90  -- Loss: 0.86214\n",
      "Average Loss: 0.578482\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #158: Batch  90/90  -- Loss: 0.33133\n",
      "Average Loss: 0.575152\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #159: Batch  90/90  -- Loss: 0.59264\n",
      "Average Loss: 0.576815\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #160: Batch  90/90  -- Loss: 0.59622\n",
      "Average Loss: 0.576329\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #161: Batch  90/90  -- Loss: 0.57592\n",
      "Average Loss: 0.576295\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #162: Batch  90/90  -- Loss: 0.84555\n",
      "Average Loss: 0.577745\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #163: Batch  90/90  -- Loss: 0.57952\n",
      "Average Loss: 0.576479\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #164: Batch  90/90  -- Loss: 0.28707\n",
      "Average Loss: 0.574879\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #165: Batch  90/90  -- Loss: 1.10898\n",
      "Average Loss: 0.580098\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #166: Batch  90/90  -- Loss: 0.30885\n",
      "Average Loss: 0.574789\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #167: Batch  90/90  -- Loss: 0.55586\n",
      "Average Loss: 0.576362\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #168: Batch  90/90  -- Loss: 0.81829\n",
      "Average Loss: 0.577371\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #169: Batch  90/90  -- Loss: 0.80596\n",
      "Average Loss: 0.578014\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #170: Batch  90/90  -- Loss: 0.82525\n",
      "Average Loss: 0.577695\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #171: Batch  90/90  -- Loss: 1.13886\n",
      "Average Loss: 0.579113\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #172: Batch  90/90  -- Loss: 0.58198\n",
      "Average Loss: 0.575588\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #173: Batch  90/90  -- Loss: 0.54034\n",
      "Average Loss: 0.576004\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #174: Batch  90/90  -- Loss: 0.34144\n",
      "Average Loss: 0.574643\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #175: Batch  90/90  -- Loss: 0.99764\n",
      "Average Loss: 0.577806\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #176: Batch  90/90  -- Loss: 0.52845\n",
      "Average Loss: 0.574447\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #177: Batch  90/90  -- Loss: 0.52833\n",
      "Average Loss: 0.574268\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #178: Batch  90/90  -- Loss: 0.30635\n",
      "Average Loss: 0.572904\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #179: Batch  90/90  -- Loss: 0.55328\n",
      "Average Loss: 0.57288\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #180: Batch  90/90  -- Loss: 0.30392\n",
      "Average Loss: 0.572101\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #181: Batch  90/90  -- Loss: 0.29576\n",
      "Average Loss: 0.572817\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #182: Batch  90/90  -- Loss: 1.26836\n",
      "Average Loss: 0.575598\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #183: Batch  90/90  -- Loss: 0.34463\n",
      "Average Loss: 0.569095\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #184: Batch  90/90  -- Loss: 0.80181\n",
      "Average Loss: 0.567524\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #185: Batch  90/90  -- Loss: 0.25349\n",
      "Average Loss: 0.552983\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #186: Batch  90/90  -- Loss: 0.47897\n",
      "Average Loss: 0.519954\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #187: Batch  90/90  -- Loss: 0.34075\n",
      "Average Loss: 0.486093\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #188: Batch  90/90  -- Loss: 0.42518\n",
      "Average Loss: 0.46203\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #189: Batch  90/90  -- Loss: 0.44721\n",
      "Average Loss: 0.444804\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #190: Batch  90/90  -- Loss: 0.33072\n",
      "Average Loss: 0.42704\n",
      "Training F1: 0.4161\n",
      "Test F1: 0.2549\n",
      "Epoch #191: Batch  90/90  -- Loss: 0.76393\n",
      "Average Loss: 0.417692\n",
      "Training F1: 0.07921\n",
      "Test F1: 0.0\n",
      "Epoch #192: Batch  90/90  -- Loss: 0.34681\n",
      "Average Loss: 0.406642\n",
      "Training F1: 0.5275\n",
      "Test F1: 0.3443\n",
      "Epoch #193: Batch  90/90  -- Loss: 0.607842\n",
      "Average Loss: 0.399176\n",
      "Training F1: 0.3016\n",
      "Test F1: 0.2022\n",
      "Epoch #194: Batch  90/90  -- Loss: 0.57284\n",
      "Average Loss: 0.394793\n",
      "Training F1: 0.4228\n",
      "Test F1: 0.2476\n",
      "Epoch #195: Batch  90/90  -- Loss: 0.296913\n",
      "Average Loss: 0.386983\n",
      "Training F1: 0.5146\n",
      "Test F1: 0.3103\n",
      "Epoch #196: Batch  90/90  -- Loss: 0.49348\n",
      "Average Loss: 0.384544\n",
      "Training F1: 0.5058\n",
      "Test F1: 0.3103\n",
      "Epoch #197: Batch  90/90  -- Loss: 0.473434\n",
      "Average Loss: 0.380217\n",
      "Training F1: 0.5239\n",
      "Test F1: 0.3051\n",
      "Epoch #198: Batch  90/90  -- Loss: 0.685132\n",
      "Average Loss: 0.377946\n",
      "Training F1: 0.5143\n",
      "Test F1: 0.3077\n",
      "Epoch #199: Batch  90/90  -- Loss: 0.43312\n",
      "Average Loss: 0.373952\n",
      "Training F1: 0.5315\n",
      "Test F1: 0.3471\n",
      "Epoch #200: Batch  90/90  -- Loss: 0.23692\n",
      "Average Loss: 0.371652\n",
      "Training F1: 0.5455\n",
      "Test F1: 0.371\n",
      "Epoch #201: Batch  90/90  -- Loss: 0.315255\n",
      "Average Loss: 0.369881\n",
      "Training F1: 0.5519\n",
      "Test F1: 0.336\n",
      "Epoch #202: Batch  90/90  -- Loss: 0.31131\n",
      "Average Loss: 0.367798\n",
      "Training F1: 0.5465\n",
      "Test F1: 0.3577\n",
      "Epoch #203: Batch  90/90  -- Loss: 0.30784\n",
      "Average Loss: 0.366618\n",
      "Training F1: 0.5501\n",
      "Test F1: 0.3252\n",
      "Epoch #204: Batch  90/90  -- Loss: 0.38354\n",
      "Average Loss: 0.365392\n",
      "Training F1: 0.5995\n",
      "Test F1: 0.4058\n",
      "Epoch #205: Batch  90/90  -- Loss: 0.286028\n",
      "Average Loss: 0.363489\n",
      "Training F1: 0.4894\n",
      "Test F1: 0.3158\n",
      "Epoch #206: Batch  90/90  -- Loss: 0.347914\n",
      "Average Loss: 0.363096\n",
      "Training F1: 0.5934\n",
      "Test F1: 0.4203\n",
      "Epoch #207: Batch  90/90  -- Loss: 0.38691\n",
      "Average Loss: 0.361223\n",
      "Training F1: 0.5913\n",
      "Test F1: 0.4203\n",
      "Epoch #208: Batch  90/90  -- Loss: 0.487752\n",
      "Average Loss: 0.359569\n",
      "Training F1: 0.5773\n",
      "Test F1: 0.4\n",
      "Epoch #209: Batch  90/90  -- Loss: 0.210425\n",
      "Average Loss: 0.360536\n",
      "Training F1: 0.5812\n",
      "Test F1: 0.4058\n",
      "Epoch #210: Batch  90/90  -- Loss: 0.173113\n",
      "Average Loss: 0.35612\n",
      "Training F1: 0.5656\n",
      "Test F1: 0.3971\n",
      "Epoch #211: Batch  90/90  -- Loss: 0.745484\n",
      "Average Loss: 0.361796\n",
      "Training F1: 0.5393\n",
      "Test F1: 0.4361\n",
      "Epoch #212: Batch  90/90  -- Loss: 0.327643\n",
      "Average Loss: 0.35686\n",
      "Training F1: 0.601\n",
      "Test F1: 0.4901\n",
      "Epoch #213: Batch  90/90  -- Loss: 0.20936\n",
      "Average Loss: 0.355843\n",
      "Training F1: 0.593\n",
      "Test F1: 0.4865\n",
      "Epoch #214: Batch  90/90  -- Loss: 0.201913\n",
      "Average Loss: 0.356098\n",
      "Training F1: 0.5419\n",
      "Test F1: 0.4559\n",
      "Epoch #215: Batch  90/90  -- Loss: 0.204334\n",
      "Average Loss: 0.353812\n",
      "Training F1: 0.6203\n",
      "Test F1: 0.5032\n",
      "Epoch #216: Batch  90/90  -- Loss: 0.164118\n",
      "Average Loss: 0.352612\n",
      "Training F1: 0.6158\n",
      "Test F1: 0.5256\n",
      "Epoch #217: Batch  90/90  -- Loss: 0.383764\n",
      "Average Loss: 0.351399\n",
      "Training F1: 0.6345\n",
      "Test F1: 0.561\n",
      "Epoch #218: Batch  90/90  -- Loss: 0.431129\n",
      "Average Loss: 0.351674\n",
      "Training F1: 0.6578\n",
      "Test F1: 0.5647\n",
      "Epoch #219: Batch  90/90  -- Loss: 0.33868\n",
      "Average Loss: 0.349563\n",
      "Training F1: 0.6395\n",
      "Test F1: 0.5488\n",
      "Epoch #220: Batch  90/90  -- Loss: 0.336775\n",
      "Average Loss: 0.349357\n",
      "Training F1: 0.643\n",
      "Test F1: 0.5283\n",
      "Epoch #221: Batch  90/90  -- Loss: 0.017085\n",
      "Average Loss: 0.346395\n",
      "Training F1: 0.6512\n",
      "Test F1: 0.5521\n",
      "Epoch #222: Batch  90/90  -- Loss: 0.326137\n",
      "Average Loss: 0.347576\n",
      "Training F1: 0.6697\n",
      "Test F1: 0.5663\n",
      "Epoch #223: Batch  90/90  -- Loss: 0.473556\n",
      "Average Loss: 0.347086\n",
      "Training F1: 0.6681\n",
      "Test F1: 0.5977\n",
      "Epoch #224: Batch  90/90  -- Loss: 0.214092\n",
      "Average Loss: 0.34372\n",
      "Training F1: 0.6525\n",
      "Test F1: 0.5283\n",
      "Epoch #225: Batch  90/90  -- Loss: 0.293255\n",
      "Average Loss: 0.344669\n",
      "Training F1: 0.6523\n",
      "Test F1: 0.5223\n",
      "Epoch #226: Batch  90/90  -- Loss: 0.225044\n",
      "Average Loss: 0.343523\n",
      "Training F1: 0.6559\n",
      "Test F1: 0.5521\n",
      "Epoch #227: Batch  90/90  -- Loss: 0.760319\n",
      "Average Loss: 0.344122\n",
      "Training F1: 0.674\n",
      "Test F1: 0.568\n",
      "Epoch #228: Batch  90/90  -- Loss: 0.365919\n",
      "Average Loss: 0.342931\n",
      "Training F1: 0.662\n",
      "Test F1: 0.5679\n",
      "Epoch #229: Batch  90/90  -- Loss: 0.488094\n",
      "Average Loss: 0.342176\n",
      "Training F1: 0.6635\n",
      "Test F1: 0.55\n",
      "Epoch #230: Batch  90/90  -- Loss: 0.591018\n",
      "Average Loss: 0.339733\n",
      "Training F1: 0.6953\n",
      "Test F1: 0.5967\n",
      "Epoch #231: Batch  90/90  -- Loss: 0.131591\n",
      "Average Loss: 0.337167\n",
      "Training F1: 0.6804\n",
      "Test F1: 0.5697\n",
      "Epoch #232: Batch  90/90  -- Loss: 0.656218\n",
      "Average Loss: 0.338277\n",
      "Training F1: 0.6835\n",
      "Test F1: 0.5697\n",
      "Epoch #233: Batch  90/90  -- Loss: 0.232496\n",
      "Average Loss: 0.334956\n",
      "Training F1: 0.6912\n",
      "Test F1: 0.5629\n",
      "Epoch #234: Batch  90/90  -- Loss: 0.50472\n",
      "Average Loss: 0.335593\n",
      "Training F1: 0.6569\n",
      "Test F1: 0.566\n",
      "Epoch #235: Batch  90/90  -- Loss: 0.238226\n",
      "Average Loss: 0.332618\n",
      "Training F1: 0.7032\n",
      "Test F1: 0.5679\n",
      "Epoch #236: Batch  90/90  -- Loss: 0.352728\n",
      "Average Loss: 0.330609\n",
      "Training F1: 0.7043\n",
      "Test F1: 0.6023\n",
      "Epoch #237: Batch  90/90  -- Loss: 0.384575\n",
      "Average Loss: 0.328521\n",
      "Training F1: 0.6983\n",
      "Test F1: 0.5641\n",
      "Epoch #238: Batch  90/90  -- Loss: 0.712637\n",
      "Average Loss: 0.330701\n",
      "Training F1: 0.7033\n",
      "Test F1: 0.5823\n",
      "Epoch #239: Batch  90/90  -- Loss: 0.142577\n",
      "Average Loss: 0.323907\n",
      "Training F1: 0.7217\n",
      "Test F1: 0.6057\n",
      "Epoch #240: Batch  90/90  -- Loss: 0.465488\n",
      "Average Loss: 0.321927\n",
      "Training F1: 0.6946\n",
      "Test F1: 0.5949\n",
      "Epoch #241: Batch  90/90  -- Loss: 0.395511\n",
      "Average Loss: 0.320024\n",
      "Training F1: 0.7187\n",
      "Test F1: 0.5963\n",
      "Epoch #242: Batch  90/90  -- Loss: 0.1385348\n",
      "Average Loss: 0.315126\n",
      "Training F1: 0.7209\n",
      "Test F1: 0.6049\n",
      "Epoch #243: Batch  90/90  -- Loss: 0.593487\n",
      "Average Loss: 0.312314\n",
      "Training F1: 0.7414\n",
      "Test F1: 0.6136\n",
      "Epoch #244: Batch  90/90  -- Loss: 0.3140858\n",
      "Average Loss: 0.311844\n",
      "Training F1: 0.7366\n",
      "Test F1: 0.6038\n",
      "Epoch #245: Batch  90/90  -- Loss: 0.010431\n",
      "Average Loss: 0.303164\n",
      "Training F1: 0.7229\n",
      "Test F1: 0.5987\n",
      "Epoch #246: Batch  90/90  -- Loss: 0.013512\n",
      "Average Loss: 0.29968\n",
      "Training F1: 0.7449\n",
      "Test F1: 0.6118\n",
      "Epoch #247: Batch  90/90  -- Loss: 0.448757\n",
      "Average Loss: 0.301209\n",
      "Training F1: 0.7562\n",
      "Test F1: 0.6316\n",
      "Epoch #248: Batch  90/90  -- Loss: 0.0341966\n",
      "Average Loss: 0.294493\n",
      "Training F1: 0.7651\n",
      "Test F1: 0.6364\n",
      "Epoch #249: Batch  90/90  -- Loss: 0.164716\n",
      "Average Loss: 0.292546\n",
      "Training F1: 0.7681\n",
      "Test F1: 0.65\n",
      "Epoch #250: Batch  90/90  -- Loss: 0.313728\n",
      "Average Loss: 0.289002\n",
      "Training F1: 0.7674\n",
      "Test F1: 0.642\n",
      "Epoch #251: Batch  90/90  -- Loss: 0.164576\n",
      "Average Loss: 0.283912\n",
      "Training F1: 0.7529\n",
      "Test F1: 0.6545\n",
      "Epoch #252: Batch  90/90  -- Loss: 0.478235\n",
      "Average Loss: 0.282682\n",
      "Training F1: 0.75\n",
      "Test F1: 0.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #253: Batch  90/90  -- Loss: 0.4906465\n",
      "Average Loss: 0.282823\n",
      "Training F1: 0.7533\n",
      "Test F1: 0.6533\n",
      "Epoch #254: Batch  90/90  -- Loss: 0.185746\n",
      "Average Loss: 0.279182\n",
      "Training F1: 0.7519\n",
      "Test F1: 0.6624\n",
      "Epoch #255: Batch  90/90  -- Loss: 0.212583\n",
      "Average Loss: 0.272331\n",
      "Training F1: 0.7586\n",
      "Test F1: 0.6497\n",
      "Epoch #256: Batch  90/90  -- Loss: 0.084369\n",
      "Average Loss: 0.271796\n",
      "Training F1: 0.7677\n",
      "Test F1: 0.6753\n",
      "Epoch #257: Batch  90/90  -- Loss: 0.209373\n",
      "Average Loss: 0.268731\n",
      "Training F1: 0.7912\n",
      "Test F1: 0.6879\n",
      "Epoch #258: Batch  90/90  -- Loss: 0.227871\n",
      "Average Loss: 0.262984\n",
      "Training F1: 0.7688\n",
      "Test F1: 0.6667\n",
      "Epoch #259: Batch  90/90  -- Loss: 0.547391\n",
      "Average Loss: 0.26169\n",
      "Training F1: 0.7828\n",
      "Test F1: 0.6974\n",
      "Epoch #260: Batch  90/90  -- Loss: 0.068621\n",
      "Average Loss: 0.25651\n",
      "Training F1: 0.797\n",
      "Test F1: 0.6711\n",
      "Epoch #261: Batch  90/90  -- Loss: 0.272975\n",
      "Average Loss: 0.260352\n",
      "Training F1: 0.7868\n",
      "Test F1: 0.68\n",
      "Epoch #262: Batch  90/90  -- Loss: 0.453091\n",
      "Average Loss: 0.252414\n",
      "Training F1: 0.796\n",
      "Test F1: 0.6667\n",
      "Epoch #263: Batch  90/90  -- Loss: 0.479012\n",
      "Average Loss: 0.251174\n",
      "Training F1: 0.799\n",
      "Test F1: 0.6711\n",
      "Epoch #264: Batch  90/90  -- Loss: 0.563014\n",
      "Average Loss: 0.250777\n",
      "Training F1: 0.7842\n",
      "Test F1: 0.6475\n",
      "Epoch #265: Batch  90/90  -- Loss: 0.2862708\n",
      "Average Loss: 0.243647\n",
      "Training F1: 0.8193\n",
      "Test F1: 0.6622\n",
      "Epoch #266: Batch  90/90  -- Loss: 0.2912459\n",
      "Average Loss: 0.239442\n",
      "Training F1: 0.799\n",
      "Test F1: 0.6331\n",
      "Epoch #267: Batch  90/90  -- Loss: 0.054434\n",
      "Average Loss: 0.235195\n",
      "Training F1: 0.8293\n",
      "Test F1: 0.6797\n",
      "Epoch #268: Batch  90/90  -- Loss: 0.271148\n",
      "Average Loss: 0.238215\n",
      "Training F1: 0.8163\n",
      "Test F1: 0.6525\n",
      "Epoch #269: Batch  90/90  -- Loss: 0.0025217\n",
      "Average Loss: 0.231349\n",
      "Training F1: 0.8074\n",
      "Test F1: 0.6377\n",
      "Epoch #270: Batch  90/90  -- Loss: 0.833514\n",
      "Average Loss: 0.230708\n",
      "Training F1: 0.825\n",
      "Test F1: 0.7027\n",
      "Epoch #271: Batch  90/90  -- Loss: 0.180625\n",
      "Average Loss: 0.225598\n",
      "Training F1: 0.8338\n",
      "Test F1: 0.6475\n",
      "Epoch #272: Batch  90/90  -- Loss: 0.215283\n",
      "Average Loss: 0.220924\n",
      "Training F1: 0.8135\n",
      "Test F1: 0.6939\n",
      "Epoch #273: Batch  90/90  -- Loss: 0.0051429\n",
      "Average Loss: 0.217893\n",
      "Training F1: 0.8128\n",
      "Test F1: 0.6667\n",
      "Epoch #274: Batch  90/90  -- Loss: 0.074322\n",
      "Average Loss: 0.210478\n",
      "Training F1: 0.8354\n",
      "Test F1: 0.7007\n",
      "Epoch #275: Batch  90/90  -- Loss: 0.549742\n",
      "Average Loss: 0.214947\n",
      "Training F1: 0.8329\n",
      "Test F1: 0.7172\n",
      "Epoch #276: Batch  90/90  -- Loss: 0.344341\n",
      "Average Loss: 0.20775\n",
      "Training F1: 0.8285\n",
      "Test F1: 0.7111\n",
      "Epoch #277: Batch  90/90  -- Loss: 0.0037241\n",
      "Average Loss: 0.20363\n",
      "Training F1: 0.8055\n",
      "Test F1: 0.7231\n",
      "Epoch #278: Batch  90/90  -- Loss: 0.321017\n",
      "Average Loss: 0.203959\n",
      "Training F1: 0.8564\n",
      "Test F1: 0.7194\n",
      "Epoch #279: Batch  90/90  -- Loss: 0.0585938\n",
      "Average Loss: 0.202188\n",
      "Training F1: 0.8491\n",
      "Test F1: 0.7482\n",
      "Epoch #280: Batch  90/90  -- Loss: 0.1974576\n",
      "Average Loss: 0.197956\n",
      "Training F1: 0.8665\n",
      "Test F1: 0.7376\n",
      "Epoch #281: Batch  90/90  -- Loss: 0.017172\n",
      "Average Loss: 0.19321\n",
      "Training F1: 0.8615\n",
      "Test F1: 0.7234\n",
      "Epoch #282: Batch  90/90  -- Loss: 0.613743\n",
      "Average Loss: 0.190845\n",
      "Training F1: 0.8635\n",
      "Test F1: 0.7429\n",
      "Epoch #283: Batch  90/90  -- Loss: 0.524391\n",
      "Average Loss: 0.193999\n",
      "Training F1: 0.8594\n",
      "Test F1: 0.7287\n",
      "Epoch #284: Batch  90/90  -- Loss: 0.107065\n",
      "Average Loss: 0.193577\n",
      "Training F1: 0.8693\n",
      "Test F1: 0.7391\n",
      "Epoch #285: Batch  90/90  -- Loss: 0.075241\n",
      "Average Loss: 0.179384\n",
      "Training F1: 0.8821\n",
      "Test F1: 0.7857\n",
      "Epoch #286: Batch  90/90  -- Loss: 0.0806419\n",
      "Average Loss: 0.18277\n",
      "Training F1: 0.8713\n",
      "Test F1: 0.75\n",
      "Epoch #287: Batch  90/90  -- Loss: 0.016555\n",
      "Average Loss: 0.174373\n",
      "Training F1: 0.8693\n",
      "Test F1: 0.7313\n",
      "Epoch #288: Batch  90/90  -- Loss: 0.325399\n",
      "Average Loss: 0.173111\n",
      "Training F1: 0.8872\n",
      "Test F1: 0.7794\n",
      "Epoch #289: Batch  90/90  -- Loss: 0.066815\n",
      "Average Loss: 0.169985\n",
      "Training F1: 0.8918\n",
      "Test F1: 0.7857\n",
      "Epoch #290: Batch  90/90  -- Loss: 0.129762\n",
      "Average Loss: 0.165834\n",
      "Training F1: 0.8953\n",
      "Test F1: 0.7852\n",
      "Epoch #291: Batch  90/90  -- Loss: 0.060091\n",
      "Average Loss: 0.160222\n",
      "Training F1: 0.8969\n",
      "Test F1: 0.7714\n",
      "Epoch #292: Batch  90/90  -- Loss: 0.097712\n",
      "Average Loss: 0.161426\n",
      "Training F1: 0.9081\n",
      "Test F1: 0.782\n",
      "Epoch #293: Batch  90/90  -- Loss: 0.074088\n",
      "Average Loss: 0.15681\n",
      "Training F1: 0.9153\n",
      "Test F1: 0.7727\n",
      "Epoch #294: Batch  90/90  -- Loss: 0.1353967\n",
      "Average Loss: 0.150745\n",
      "Training F1: 0.8951\n",
      "Test F1: 0.8029\n",
      "Epoch #295: Batch  90/90  -- Loss: 0.160075\n",
      "Average Loss: 0.14592\n",
      "Training F1: 0.9231\n",
      "Test F1: 0.8286\n",
      "Epoch #296: Batch  90/90  -- Loss: 0.0458312\n",
      "Average Loss: 0.143785\n",
      "Training F1: 0.9299\n",
      "Test F1: 0.7761\n",
      "Epoch #297: Batch  90/90  -- Loss: 0.0039137\n",
      "Average Loss: 0.140189\n",
      "Training F1: 0.9302\n",
      "Test F1: 0.8227\n",
      "Epoch #298: Batch  90/90  -- Loss: 0.036154\n",
      "Average Loss: 0.142432\n",
      "Training F1: 0.9167\n",
      "Test F1: 0.8\n",
      "Epoch #299: Batch  90/90  -- Loss: 0.0043699\n",
      "Average Loss: 0.132483\n",
      "Training F1: 0.9264\n",
      "Test F1: 0.8095\n",
      "Epoch #300: Batch  90/90  -- Loss: 0.1343611\n",
      "Average Loss: 0.13084\n",
      "Training F1: 0.9337\n",
      "Test F1: 0.8397\n",
      "Epoch #301: Batch  90/90  -- Loss: 0.0607193\n",
      "Average Loss: 0.124022\n",
      "Training F1: 0.9354\n",
      "Test F1: 0.8571\n",
      "Epoch #302: Batch  90/90  -- Loss: 0.1026147\n",
      "Average Loss: 0.124405\n",
      "Training F1: 0.929\n",
      "Test F1: 0.8095\n",
      "Epoch #303: Batch  90/90  -- Loss: 0.0688725\n",
      "Average Loss: 0.118207\n",
      "Training F1: 0.9446\n",
      "Test F1: 0.8485\n",
      "Epoch #304: Batch  90/90  -- Loss: 0.0584921\n",
      "Average Loss: 0.11464\n",
      "Training F1: 0.9496\n",
      "Test F1: 0.8722\n",
      "Epoch #305: Batch  90/90  -- Loss: 0.153069\n",
      "Average Loss: 0.111434\n",
      "Training F1: 0.9418\n",
      "Test F1: 0.8702\n",
      "Epoch #306: Batch  90/90  -- Loss: 0.0428314\n",
      "Average Loss: 0.104655\n",
      "Training F1: 0.937\n",
      "Test F1: 0.8099\n",
      "Epoch #307: Batch  90/90  -- Loss: 0.1332424\n",
      "Average Loss: 0.106758\n",
      "Training F1: 0.9337\n",
      "Test F1: 0.8189\n",
      "Epoch #308: Batch  90/90  -- Loss: 0.0851272\n",
      "Average Loss: 0.102198\n",
      "Training F1: 0.9534\n",
      "Test F1: 0.8759\n",
      "Epoch #309: Batch  90/90  -- Loss: 0.0436023\n",
      "Average Loss: 0.100705\n",
      "Training F1: 0.9399\n",
      "Test F1: 0.8759\n",
      "Epoch #310: Batch  90/90  -- Loss: 0.2896266\n",
      "Average Loss: 0.11509\n",
      "Training F1: 0.9551\n",
      "Test F1: 0.8769\n",
      "Epoch #311: Batch  90/90  -- Loss: 0.0633363\n",
      "Average Loss: 0.0921677\n",
      "Training F1: 0.9542\n",
      "Test F1: 0.8837\n",
      "Epoch #312: Batch  90/90  -- Loss: 0.1055975\n",
      "Average Loss: 0.0858934\n",
      "Training F1: 0.9641\n",
      "Test F1: 0.9028\n",
      "Epoch #313: Batch  90/90  -- Loss: 0.1006389\n",
      "Average Loss: 0.0883874\n",
      "Training F1: 0.9655\n",
      "Test F1: 0.8923\n",
      "Epoch #314: Batch  90/90  -- Loss: 0.1359325\n",
      "Average Loss: 0.0814356\n",
      "Training F1: 0.9661\n",
      "Test F1: 0.913\n",
      "Epoch #315: Batch  90/90  -- Loss: 0.0011736\n",
      "Average Loss: 0.0801752\n",
      "Training F1: 0.9689\n",
      "Test F1: 0.9078\n",
      "Epoch #316: Batch  90/90  -- Loss: 0.1403482\n",
      "Average Loss: 0.0756464\n",
      "Training F1: 0.9622\n",
      "Test F1: 0.8819\n",
      "Epoch #317: Batch  90/90  -- Loss: 0.0064095\n",
      "Average Loss: 0.0728902\n",
      "Training F1: 0.971\n",
      "Test F1: 0.9008\n",
      "Epoch #318: Batch  90/90  -- Loss: 0.0030764\n",
      "Average Loss: 0.0714478\n",
      "Training F1: 0.9708\n",
      "Test F1: 0.8992\n",
      "Epoch #319: Batch  90/90  -- Loss: 0.0397571\n",
      "Average Loss: 0.0709303\n",
      "Training F1: 0.9737\n",
      "Test F1: 0.9481\n",
      "Epoch #320: Batch  90/90  -- Loss: 0.0035245\n",
      "Average Loss: 0.0652595\n",
      "Training F1: 0.9791\n",
      "Test F1: 0.942\n",
      "Epoch #321: Batch  90/90  -- Loss: 0.1169343\n",
      "Average Loss: 0.0616399\n",
      "Training F1: 0.9793\n",
      "Test F1: 0.8986\n",
      "Epoch #322: Batch  90/90  -- Loss: 0.0202992\n",
      "Average Loss: 0.061952\n",
      "Training F1: 0.9813\n",
      "Test F1: 0.9385\n",
      "Epoch #323: Batch  90/90  -- Loss: 0.1891683\n",
      "Average Loss: 0.0618755\n",
      "Training F1: 0.9818\n",
      "Test F1: 0.9481\n",
      "Epoch #324: Batch  90/90  -- Loss: 0.0128312\n",
      "Average Loss: 0.0599394\n",
      "Training F1: 0.9728\n",
      "Test F1: 0.9194\n",
      "Epoch #325: Batch  90/90  -- Loss: 0.0470891\n",
      "Average Loss: 0.0559749\n",
      "Training F1: 0.9895\n",
      "Test F1: 0.9538\n",
      "Epoch #326: Batch  90/90  -- Loss: 0.0720143\n",
      "Average Loss: 0.0523784\n",
      "Training F1: 0.9868\n",
      "Test F1: 0.9701\n",
      "Epoch #327: Batch  90/90  -- Loss: 0.0331716\n",
      "Average Loss: 0.0494915\n",
      "Training F1: 0.9842\n",
      "Test F1: 0.9545\n",
      "Epoch #328: Batch  90/90  -- Loss: 0.0782123\n",
      "Average Loss: 0.0470284\n",
      "Training F1: 0.9895\n",
      "Test F1: 0.9545\n",
      "Epoch #329: Batch  90/90  -- Loss: 0.0169716\n",
      "Average Loss: 0.0442527\n",
      "Training F1: 0.9793\n",
      "Test F1: 0.9286\n",
      "Epoch #330: Batch  90/90  -- Loss: 0.02193847\n",
      "Average Loss: 0.0449976\n",
      "Training F1: 0.9869\n",
      "Test F1: 0.9618\n",
      "Epoch #331: Batch  90/90  -- Loss: 0.0058479\n",
      "Average Loss: 0.0436106\n",
      "Training F1: 0.9893\n",
      "Test F1: 0.9365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #332: Batch  90/90  -- Loss: 0.0523134\n",
      "Average Loss: 0.0408116\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9618\n",
      "Epoch #333: Batch  90/90  -- Loss: 0.2269419\n",
      "Average Loss: 0.0409718\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9618\n",
      "Epoch #334: Batch  90/90  -- Loss: 0.0012478\n",
      "Average Loss: 0.0392897\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9697\n",
      "Epoch #335: Batch  90/90  -- Loss: 0.1988451\n",
      "Average Loss: 0.0366872\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9692\n",
      "Epoch #336: Batch  90/90  -- Loss: 0.0276229\n",
      "Average Loss: 0.0336763\n",
      "Training F1: 0.9869\n",
      "Test F1: 0.9706\n",
      "Epoch #337: Batch  90/90  -- Loss: 0.0486611\n",
      "Average Loss: 0.0325859\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9612\n",
      "Epoch #338: Batch  90/90  -- Loss: 0.02148911\n",
      "Average Loss: 0.0297609\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9774\n",
      "Epoch #339: Batch  90/90  -- Loss: 0.00287683\n",
      "Average Loss: 0.0290608\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #340: Batch  90/90  -- Loss: 0.0006058\n",
      "Average Loss: 0.0262582\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #341: Batch  90/90  -- Loss: 0.0086941\n",
      "Average Loss: 0.0251354\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9771\n",
      "Epoch #342: Batch  90/90  -- Loss: 0.0166225\n",
      "Average Loss: 0.0242111\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #343: Batch  90/90  -- Loss: 0.0057896\n",
      "Average Loss: 0.0222957\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9612\n",
      "Epoch #344: Batch  90/90  -- Loss: 0.0107447\n",
      "Average Loss: 0.0218241\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #345: Batch  90/90  -- Loss: 0.00825576\n",
      "Average Loss: 0.0207489\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9692\n",
      "Epoch #346: Batch  90/90  -- Loss: 0.0029129\n",
      "Average Loss: 0.0210868\n",
      "Training F1: 0.992\n",
      "Test F1: 0.9692\n",
      "Epoch #347: Batch  90/90  -- Loss: 0.03213485\n",
      "Average Loss: 0.0740158\n",
      "Training F1: 0.956\n",
      "Test F1: 0.9466\n",
      "Epoch #348: Batch  90/90  -- Loss: 0.0385114\n",
      "Average Loss: 0.0398279\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9926\n",
      "Epoch #349: Batch  90/90  -- Loss: 0.0037771\n",
      "Average Loss: 0.0185662\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #350: Batch  90/90  -- Loss: 0.0362049\n",
      "Average Loss: 0.0174481\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #351: Batch  90/90  -- Loss: 0.0034516\n",
      "Average Loss: 0.0156608\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #352: Batch  90/90  -- Loss: 0.00412233\n",
      "Average Loss: 0.0148884\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #353: Batch  90/90  -- Loss: 0.03369416\n",
      "Average Loss: 0.0144365\n",
      "Training F1: 0.9947\n",
      "Test F1: 0.9531\n",
      "Epoch #354: Batch  90/90  -- Loss: 0.0022961\n",
      "Average Loss: 0.0148888\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #355: Batch  90/90  -- Loss: 0.00890769\n",
      "Average Loss: 0.0129601\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #356: Batch  90/90  -- Loss: 0.01510293\n",
      "Average Loss: 0.0121352\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9851\n",
      "Epoch #357: Batch  90/90  -- Loss: 0.02274586\n",
      "Average Loss: 0.0114218\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #358: Batch  90/90  -- Loss: 0.00901753\n",
      "Average Loss: 0.0135502\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #359: Batch  90/90  -- Loss: 6.4281e-05\n",
      "Average Loss: 0.0110327\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #360: Batch  90/90  -- Loss: 0.00312589\n",
      "Average Loss: 0.00990332\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #361: Batch  90/90  -- Loss: 0.00095786\n",
      "Average Loss: 0.00927853\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #362: Batch  90/90  -- Loss: 0.01099486\n",
      "Average Loss: 0.00888326\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #363: Batch  90/90  -- Loss: 0.00303829\n",
      "Average Loss: 0.0089906\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #364: Batch  90/90  -- Loss: 0.0099527\n",
      "Average Loss: 0.00816044\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #365: Batch  90/90  -- Loss: 0.0014529\n",
      "Average Loss: 0.00776721\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #366: Batch  90/90  -- Loss: 0.00278848\n",
      "Average Loss: 0.00720903\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #367: Batch  90/90  -- Loss: 0.00820991\n",
      "Average Loss: 0.00716309\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #368: Batch  90/90  -- Loss: 0.0267755\n",
      "Average Loss: 0.00789125\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #369: Batch  90/90  -- Loss: 0.00930363\n",
      "Average Loss: 0.00643806\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #370: Batch  90/90  -- Loss: 0.00266693\n",
      "Average Loss: 0.00580169\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #371: Batch  90/90  -- Loss: 0.02012639\n",
      "Average Loss: 0.00592866\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #372: Batch  90/90  -- Loss: 0.00738559\n",
      "Average Loss: 0.0053631\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #373: Batch  90/90  -- Loss: 0.0018199\n",
      "Average Loss: 0.00526658\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #374: Batch  90/90  -- Loss: 0.00014783\n",
      "Average Loss: 0.00465084\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #375: Batch  90/90  -- Loss: 0.00073779\n",
      "Average Loss: 0.00461501\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #376: Batch  90/90  -- Loss: 0.00067329\n",
      "Average Loss: 0.00442667\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #377: Batch  90/90  -- Loss: 0.00219088\n",
      "Average Loss: 0.00414558\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #378: Batch  90/90  -- Loss: 0.00425358\n",
      "Average Loss: 0.00376306\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #379: Batch  90/90  -- Loss: 0.00811695\n",
      "Average Loss: 0.00402846\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #380: Batch  90/90  -- Loss: 0.00108826\n",
      "Average Loss: 0.00351266\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #381: Batch  90/90  -- Loss: 0.00216214\n",
      "Average Loss: 0.00320803\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #382: Batch  90/90  -- Loss: 0.00203978\n",
      "Average Loss: 0.00326696\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #383: Batch  90/90  -- Loss: 0.00294215\n",
      "Average Loss: 0.00299771\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #384: Batch  90/90  -- Loss: 0.00648665\n",
      "Average Loss: 0.00363005\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #385: Batch  90/90  -- Loss: 0.00188547\n",
      "Average Loss: 0.0033199\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #386: Batch  90/90  -- Loss: 0.00147492\n",
      "Average Loss: 0.00264575\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #387: Batch  90/90  -- Loss: 0.00227937\n",
      "Average Loss: 0.00244385\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #388: Batch  90/90  -- Loss: 0.00274268\n",
      "Average Loss: 0.00234495\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #389: Batch  90/90  -- Loss: 0.00037343\n",
      "Average Loss: 0.00216162\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #390: Batch  90/90  -- Loss: 0.00285848\n",
      "Average Loss: 0.00209742\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #391: Batch  90/90  -- Loss: 0.00515598\n",
      "Average Loss: 0.00202426\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #392: Batch  90/90  -- Loss: 0.00104081\n",
      "Average Loss: 0.00191578\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #393: Batch  90/90  -- Loss: 0.00320495\n",
      "Average Loss: 0.00197139\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #394: Batch  90/90  -- Loss: 0.00105645\n",
      "Average Loss: 0.0021085\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #395: Batch  90/90  -- Loss: 0.00196093\n",
      "Average Loss: 0.00182498\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #396: Batch  90/90  -- Loss: 0.00061119\n",
      "Average Loss: 0.00161852\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #397: Batch  90/90  -- Loss: 0.00052641\n",
      "Average Loss: 0.00151159\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #398: Batch  90/90  -- Loss: 0.00101096\n",
      "Average Loss: 0.00143303\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #399: Batch  90/90  -- Loss: 0.00139188\n",
      "Average Loss: 0.00138341\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #400: Batch  90/90  -- Loss: 0.00081967\n",
      "Average Loss: 0.00131447\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #401: Batch  90/90  -- Loss: 0.00117086\n",
      "Average Loss: 0.00127057\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #402: Batch  90/90  -- Loss: 0.00066001\n",
      "Average Loss: 0.0012332\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #403: Batch  90/90  -- Loss: 0.00176388\n",
      "Average Loss: 0.0393712\n",
      "Training F1: 0.9815\n",
      "Test F1: 0.9559\n",
      "Epoch #404: Batch  90/90  -- Loss: 0.01675943\n",
      "Average Loss: 0.0358072\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #405: Batch  90/90  -- Loss: 0.00131617\n",
      "Average Loss: 0.00309385\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #406: Batch  90/90  -- Loss: 0.00077375\n",
      "Average Loss: 0.00235174\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #407: Batch  90/90  -- Loss: 0.00033909\n",
      "Average Loss: 0.00210852\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #408: Batch  90/90  -- Loss: 0.00441829\n",
      "Average Loss: 0.0019224\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #409: Batch  90/90  -- Loss: 0.00191881\n",
      "Average Loss: 0.00177458\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #410: Batch  90/90  -- Loss: 0.00060818\n",
      "Average Loss: 0.00166583\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #411: Batch  90/90  -- Loss: 0.00035372\n",
      "Average Loss: 0.00158268\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #412: Batch  90/90  -- Loss: 0.00099056\n",
      "Average Loss: 0.00150909\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #413: Batch  90/90  -- Loss: 0.00094177\n",
      "Average Loss: 0.00144537\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #414: Batch  90/90  -- Loss: 0.00148846\n",
      "Average Loss: 0.00139549\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #415: Batch  90/90  -- Loss: 0.00176659\n",
      "Average Loss: 0.00135028\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #416: Batch  90/90  -- Loss: 7.3909e-06\n",
      "Average Loss: 0.0012974\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #417: Batch  90/90  -- Loss: 0.00050857\n",
      "Average Loss: 0.00125849\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #418: Batch  90/90  -- Loss: 0.00036858\n",
      "Average Loss: 0.00122283\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #419: Batch  90/90  -- Loss: 0.00046539\n",
      "Average Loss: 0.0011922\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #420: Batch  90/90  -- Loss: 0.00057106\n",
      "Average Loss: 0.00115765\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #421: Batch  90/90  -- Loss: 0.00012959\n",
      "Average Loss: 0.00112106\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #422: Batch  90/90  -- Loss: 0.00011246\n",
      "Average Loss: 0.00109535\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #423: Batch  90/90  -- Loss: 0.00256052\n",
      "Average Loss: 0.00108\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #424: Batch  90/90  -- Loss: 0.00082016\n",
      "Average Loss: 0.00104003\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #425: Batch  90/90  -- Loss: 0.00164698\n",
      "Average Loss: 0.00102214\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #426: Batch  90/90  -- Loss: 8.3744e-06\n",
      "Average Loss: 0.00098882\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #427: Batch  90/90  -- Loss: 0.00232462\n",
      "Average Loss: 0.000975193\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #428: Batch  90/90  -- Loss: 0.00045281\n",
      "Average Loss: 0.000940117\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #429: Batch  90/90  -- Loss: 0.00082025\n",
      "Average Loss: 0.000918796\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #430: Batch  90/90  -- Loss: 0.00055077\n",
      "Average Loss: 0.000893008\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #431: Batch  90/90  -- Loss: 0.00033563\n",
      "Average Loss: 0.000871442\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #432: Batch  90/90  -- Loss: 0.00022471\n",
      "Average Loss: 0.000850701\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #433: Batch  90/90  -- Loss: 0.00050113\n",
      "Average Loss: 0.000829796\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #434: Batch  90/90  -- Loss: 0.00026437\n",
      "Average Loss: 0.000810732\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #435: Batch  90/90  -- Loss: 0.00010132\n",
      "Average Loss: 0.000791651\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #436: Batch  90/90  -- Loss: 0.00049911\n",
      "Average Loss: 0.00076199\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #437: Batch  90/90  -- Loss: 0.00043077\n",
      "Average Loss: 0.000753539\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #438: Batch  90/90  -- Loss: 0.00040615\n",
      "Average Loss: 0.000731613\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #439: Batch  90/90  -- Loss: 0.00141592\n",
      "Average Loss: 0.00071647\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #440: Batch  90/90  -- Loss: 0.00019599\n",
      "Average Loss: 0.000694037\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #441: Batch  90/90  -- Loss: 0.00024263\n",
      "Average Loss: 0.000676124\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #442: Batch  90/90  -- Loss: 0.00170125\n",
      "Average Loss: 0.000663865\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #443: Batch  90/90  -- Loss: 0.00122326\n",
      "Average Loss: 0.000640516\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #444: Batch  90/90  -- Loss: 0.00076925\n",
      "Average Loss: 0.000617683\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #445: Batch  90/90  -- Loss: 0.00147247\n",
      "Average Loss: 0.000608524\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #446: Batch  90/90  -- Loss: 0.00062083\n",
      "Average Loss: 0.000588783\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #447: Batch  90/90  -- Loss: 0.00034918\n",
      "Average Loss: 0.000568602\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #448: Batch  90/90  -- Loss: 0.00036653\n",
      "Average Loss: 0.000551375\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #449: Batch  90/90  -- Loss: 0.00030919\n",
      "Average Loss: 0.000533509\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #450: Batch  90/90  -- Loss: 0.00139559\n",
      "Average Loss: 0.000560352\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #451: Batch  90/90  -- Loss: 0.00041295\n",
      "Average Loss: 0.000504159\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #452: Batch  90/90  -- Loss: 0.00031582\n",
      "Average Loss: 0.000487923\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #453: Batch  90/90  -- Loss: 0.00039533\n",
      "Average Loss: 0.000480431\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #454: Batch  90/90  -- Loss: 0.00023625\n",
      "Average Loss: 0.000458136\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #455: Batch  90/90  -- Loss: 0.00056187\n",
      "Average Loss: 0.00044588\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #456: Batch  90/90  -- Loss: 0.00192437\n",
      "Average Loss: 0.000469979\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #457: Batch  90/90  -- Loss: 0.00049345\n",
      "Average Loss: 0.000452673\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #458: Batch  90/90  -- Loss: 0.00152298\n",
      "Average Loss: 0.000422947\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #459: Batch  90/90  -- Loss: 7.5035e-05\n",
      "Average Loss: 0.00039299\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #460: Batch  90/90  -- Loss: 0.00185969\n",
      "Average Loss: 0.000386763\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #461: Batch  90/90  -- Loss: 0.00059331\n",
      "Average Loss: 0.00036925\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #462: Batch  90/90  -- Loss: 0.00075182\n",
      "Average Loss: 0.000355045\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #463: Batch  90/90  -- Loss: 0.00032276\n",
      "Average Loss: 0.000338627\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #464: Batch  90/90  -- Loss: 0.00024727\n",
      "Average Loss: 0.000331434\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #465: Batch  90/90  -- Loss: 0.00017048\n",
      "Average Loss: 0.000318255\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #466: Batch  90/90  -- Loss: 6.9141e-06\n",
      "Average Loss: 0.000307636\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #467: Batch  90/90  -- Loss: 0.00058947\n",
      "Average Loss: 0.00029997\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #468: Batch  90/90  -- Loss: 0.00015576\n",
      "Average Loss: 0.000287427\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #469: Batch  90/90  -- Loss: 0.00056943\n",
      "Average Loss: 0.00027776\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #470: Batch  90/90  -- Loss: 0.00020045\n",
      "Average Loss: 0.000271096\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #471: Batch  90/90  -- Loss: 0.00021683\n",
      "Average Loss: 0.000259423\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #472: Batch  90/90  -- Loss: 3.457e-065\n",
      "Average Loss: 0.000245876\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #473: Batch  90/90  -- Loss: 0.00021855\n",
      "Average Loss: 0.0002394\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #474: Batch  90/90  -- Loss: 0.00017727\n",
      "Average Loss: 0.000229459\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #475: Batch  90/90  -- Loss: 0.00057748\n",
      "Average Loss: 0.000223779\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #476: Batch  90/90  -- Loss: 5.7007e-05\n",
      "Average Loss: 0.000212431\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #477: Batch  90/90  -- Loss: 8.2421e-05\n",
      "Average Loss: 0.000204228\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #478: Batch  90/90  -- Loss: 0.00014441\n",
      "Average Loss: 0.000197376\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #479: Batch  90/90  -- Loss: 0.00022492\n",
      "Average Loss: 0.000191035\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #480: Batch  90/90  -- Loss: 0.00012703\n",
      "Average Loss: 0.000181055\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #481: Batch  90/90  -- Loss: 5.826e-055\n",
      "Average Loss: 0.00017661\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #482: Batch  90/90  -- Loss: 0.00022032\n",
      "Average Loss: 0.000169447\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #483: Batch  90/90  -- Loss: 6.0198e-05\n",
      "Average Loss: 0.000166543\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #484: Batch  90/90  -- Loss: 8.4038e-05\n",
      "Average Loss: 0.00015677\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #485: Batch  90/90  -- Loss: 0.00013806\n",
      "Average Loss: 0.00014923\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #486: Batch  90/90  -- Loss: 0.00014002\n",
      "Average Loss: 0.000144549\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #487: Batch  90/90  -- Loss: 0.00012839\n",
      "Average Loss: 0.000139112\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #488: Batch  90/90  -- Loss: 0.00019187\n",
      "Average Loss: 0.000133494\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #489: Batch  90/90  -- Loss: 0.00011443\n",
      "Average Loss: 0.000128396\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #490: Batch  90/90  -- Loss: 0.00025536\n",
      "Average Loss: 0.000122462\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #491: Batch  90/90  -- Loss: 0.00019134\n",
      "Average Loss: 0.000120058\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #492: Batch  90/90  -- Loss: 6.0198e-05\n",
      "Average Loss: 0.000114907\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #493: Batch  90/90  -- Loss: 9.1492e-06\n",
      "Average Loss: 0.000108502\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #494: Batch  90/90  -- Loss: 2.8638e-05\n",
      "Average Loss: 0.000102724\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #495: Batch  90/90  -- Loss: 3.8861e-05\n",
      "Average Loss: 9.76e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #496: Batch  90/90  -- Loss: 5.2986e-05\n",
      "Average Loss: 9.69174e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #497: Batch  90/90  -- Loss: 5.239e-055\n",
      "Average Loss: 9.57881e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #498: Batch  90/90  -- Loss: 5.8021e-05\n",
      "Average Loss: 8.77893e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #499: Batch  90/90  -- Loss: 1.4633e-05\n",
      "Average Loss: 8.25765e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #500: Batch  90/90  -- Loss: 2.6404e-05\n",
      "Average Loss: 0.000100202\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyTorchBaseline(\n",
       "  (lstm): LSTM(2, 16, bidirectional=True)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(model, train_x, train_y, test_x, test_y, epochs=500, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "our = LSTMSeqLabel(input_dim, hidden_dim, output_dim, bidirectional=True, layers=layers).to(device)\n",
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "==================\n",
      "# of parameters: 2465\n",
      "lstm.model.0.weights     : torch.Size([18, 64])\n",
      "lstm.model.0.bias        : torch.Size([64])\n",
      "lstm.model_rev.0.weights : torch.Size([18, 64])\n",
      "lstm.model_rev.0.bias    : torch.Size([64])\n",
      "fc.weight                : torch.Size([1, 32])\n",
      "fc.bias                  : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\\n{}\".format(\"=\" * len(\"Our implementation\")))\n",
    "print(\"# of parameters: {}\".format(our.count_parameters()))\n",
    "for name, param in our.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch implementation\n",
      "======================\n",
      "# of parameters: 2593\n",
      "lstm.weight_ih_l0        : torch.Size([64, 2])\n",
      "lstm.weight_hh_l0        : torch.Size([64, 16])\n",
      "lstm.bias_ih_l0          : torch.Size([64])\n",
      "lstm.bias_hh_l0          : torch.Size([64])\n",
      "lstm.weight_ih_l0_reverse: torch.Size([64, 2])\n",
      "lstm.weight_hh_l0_reverse: torch.Size([64, 16])\n",
      "lstm.bias_ih_l0_reverse  : torch.Size([64])\n",
      "lstm.bias_hh_l0_reverse  : torch.Size([64])\n",
      "fc.weight                : torch.Size([1, 32])\n",
      "fc.bias                  : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch implementation\\n{}\".format(\"=\" * len(\"PyTorch implementation\")))\n",
    "print(\"# of parameters: {}\".format(pytorch.count_parameters()))\n",
    "for name, param in pytorch.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses $Wh + Wx$ whereas we are using $Wx'$, where $x'$ is $h, x$ concatenated. Therefore PyTorch has an extra set of biases for each direction.\n",
    "\n",
    "For one direction - 64 \\\\\n",
    "For reverse direction - 64 \\\\\n",
    "\n",
    "Our model has $2465$ parameters while PyTorch model has $2465 + 64 + 64 = 2593$ parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
