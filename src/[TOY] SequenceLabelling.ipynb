{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR task\n",
    "\n",
    "The entire dataset comprises of the binary representation of all numbers uptil a range defined. The binary sequence from left to right (most significant to least significant) is the input. While the y or the output for an input is calculated as: $a1 \\oplus a10 \\wedge a3 \\oplus a7$. Where, the most significant bit is a1, the least significant bit is a10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "state_size = 10\n",
    "data_x = []\n",
    "for i in range(pow(2, state_size)):\n",
    "    data_x.append([int(x) for x in list(np.binary_repr(i, width=state_size))])\n",
    "data_x = np.array(data_x)\n",
    "\n",
    "data_y = []\n",
    "for x in data_x:\n",
    "    # a1 xor a10 ^ a3 xor a7\n",
    "    data_y.append(np.bitwise_and(np.bitwise_xor(x[0], x[9]), \n",
    "                                 np.bitwise_xor(x[2], x[6])))\n",
    "data_y = np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping for tensors\n",
    "data_x = np.transpose(data_x).reshape(state_size, pow(2, state_size), 1)\n",
    "data_x = torch.from_numpy(data_x).float()\n",
    "data_y = torch.from_numpy(data_y).float()\n",
    "\n",
    "# Reshaping X to 2-input dimensions\n",
    "data_x = torch.zeros(data_x.shape[0], data_x.shape[1], 2).scatter_(2, data_x.long(), 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test sets\n",
    "train_size = 0.7\n",
    "ordering = torch.randperm(pow(2, state_size))\n",
    "data_x = data_x[:, ordering, :]\n",
    "data_y = data_y[ordering]\n",
    "train_x = data_x[:,:int(train_size * len(ordering)),:]\n",
    "train_y = data_y[:int(train_size * len(ordering))]\n",
    "test_x = data_x[:,int(train_size * len(ordering)):,:]\n",
    "test_y = data_y[int(train_size * len(ordering)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 716, 2]) torch.Size([716]) torch.Size([10, 308, 2]) torch.Size([308])\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dim\n",
    "input_dim = 2\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 16\n",
    "# Number of output nodes\n",
    "output_dim = 1\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = True\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False\n",
    "\n",
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[ordering]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            if model.bidirectional:\n",
    "                hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            o = model(train_x[:,start:end,:], hidden_state, cell_state)\n",
    "            loss = loss_fn(o.view(-1), train_y[start:end])\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size) + 1, \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate(model, train_x, train_y)\n",
    "        f1_test = evaluate(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if model.bidirectional:\n",
    "            hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], hidden_state, cell_state)\n",
    "        pred = torch.round(torch.sigmoid(o.view(-1))).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        labels.extend(y[start:end].int().detach().cpu().numpy())\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTMCell\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A complete LSTM architecture\n",
    "\n",
    "    Allows to stack multiple LSTM cells and also\n",
    "    create a bidirectional LSTM network.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: Dimension of input data\n",
    "    hidden_dim: Size of hidden state\n",
    "    layernorm: True/False\n",
    "    layers: Number of LSTM cells to stack\n",
    "    bidirectional: True/False\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        if self.layers < 1:\n",
    "            raise ValueError(\"layers need to be > 1\")\n",
    "        self.model = []\n",
    "        for i in range(self.layers):\n",
    "            self.model.append(LSTMCell(input_dim, hidden_dim, layernorm))\n",
    "        self.model = nn.ModuleList(self.model)\n",
    "        if self.bidirectional:\n",
    "            self.model_rev = []\n",
    "            for i in range(self.layers):\n",
    "                self.model_rev.append(LSTMCell(input_dim, hidden_dim, layernorm))\n",
    "            self.model_rev = nn.ModuleList(self.model_rev)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \"\"\"Forward pass for the LSTM network\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        x: [sequence_length, batch_size, input_dim]\n",
    "        hidden_state: [1, batch_size, hidden_dim]\n",
    "        cell_state: [1, batch_size, hidden_dim]\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        output, (hidden_state, cell_state)\n",
    "            output: [sequence_length, batch_size, hidden_dim]\n",
    "                contains the output/hidden_state from all the timesteps\n",
    "                for the final layer in sequence 1...T\n",
    "            hidden_state: [layers, batch_size, hidden_dim]\n",
    "                contains the hidden_state from the last timestep T\n",
    "                from all the layers\n",
    "            cell_state: [layers, batch_size, hidden_dim]\n",
    "                contains the cell_state from the last timestep T\n",
    "                from all the layers\n",
    "\n",
    "            If bidirectional=True\n",
    "                output: [sequence_length, batch_size, 2 * hidden_dim]\n",
    "                    [:,:,:hidden_dim] - for left-to-right\n",
    "                    [:,:,hidden_dim:] - for right-to-left\n",
    "                hidden_state: [2 * layers, batch_size, hidden_dim]\n",
    "                    [:layers,:,:] - for left-to-right\n",
    "                    [layers:,:,:] - for right-to-left\n",
    "                cell_state: [layers, batch_size, hidden_dim]\n",
    "                    [:layers,:,:] - for left-to-right\n",
    "                    [layers:,:,:] - for right-to-left\n",
    "        \"\"\"\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        seq_length = x.shape[0]\n",
    "        # Left-to-right pass\n",
    "        # index of states is equivalent to index of layer in LSTM stack\n",
    "        hidden_states = hidden_state[:self.layers,:,:].view(self.layers, 1,\n",
    "                                                            hidden_state.shape[1],\n",
    "                                                            hidden_state.shape[2])\n",
    "        cell_states = cell_state[:self.layers,:,:].view(self.layers, 1,\n",
    "                                                        cell_state.shape[1],\n",
    "                                                        cell_state.shape[2])\n",
    "        output = torch.tensor([], requires_grad=True).to(device)\n",
    "        # forward pass for one cell at a time along layers\n",
    "        for j in range(self.layers):\n",
    "            output, (hidden_states[j], cell_states[j]) = self.model[j](x, hidden_states[j].clone(),\n",
    "                                                                  cell_states[j].clone())\n",
    "        hidden_states = hidden_states.squeeze(1)\n",
    "        cell_states = cell_states.squeeze(1)\n",
    "\n",
    "        ## TODO:\n",
    "        ## The current code will work if bidirectional=False and\n",
    "        ## the hidden_state.shape[0] > self.layers owing to [:layers,:,:].\n",
    "        ## Maybe a warning should be raised without termination.\n",
    "\n",
    "        # Right-to-left pass\n",
    "        if self.bidirectional:\n",
    "            # flipping inputs/rearranging x to be in reverse timestep order\n",
    "            x = torch.flip(x, [0])  # reversing only the sequence dimension\n",
    "            # index of states is equivalent to index of layer in LSTM stack\n",
    "            hidden_states_rev = hidden_state[self.layers:,:,:].view(self.layers, 1,\n",
    "                                                                hidden_state.shape[1],\n",
    "                                                                hidden_state.shape[2])\n",
    "            cell_states_rev = cell_state[self.layers:,:,:].view(self.layers, 1,\n",
    "                                                             cell_state.shape[1],\n",
    "                                                             cell_state.shape[2])\n",
    "            output_rev = torch.tensor([], requires_grad=True).to(device)\n",
    "            # forward pass for one cell at a time along layers\n",
    "            for j in range(self.layers):\n",
    "                output_rev, (hidden_states_rev[j], cell_states_rev[j]) = self.model_rev[j](x,\n",
    "                                                                        hidden_states_rev[j].clone(),\n",
    "                                                                        cell_states_rev[j].clone())\n",
    "            # flipping outputs to be in correct timestep order\n",
    "            output_rev = torch.flip(output_rev, [0]) # reversing only the sequence dimension\n",
    "            hidden_states_rev = hidden_states_rev.squeeze(1)\n",
    "            cell_states_rev = cell_states_rev.squeeze(1)\n",
    "            # concatenating tensors\n",
    "            ## creating tensors as expected in\n",
    "            ## here: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "            hidden_states = torch.cat((hidden_states,\n",
    "                                       hidden_states_rev), dim=0)\n",
    "            cell_states = torch.cat((cell_states,\n",
    "                                     cell_states_rev), dim=0)\n",
    "            output = torch.cat((output,\n",
    "                                output_rev), dim=2)\n",
    "\n",
    "        return output, (hidden_states, cell_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lstm import LSTM\n",
    "\n",
    "class LSTMSeqLabel(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-one)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "        \n",
    "        self.lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        output, (_, _) = self.lstm(x, hidden_state, cell_state)\n",
    "        output = output[-1].unsqueeze(0)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSeqLabel(input_dim, hidden_dim, output_dim, bidirectional=True, layers=layers).to(device)\n",
    "print(model.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.58978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.667625\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.33324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c7a1d94a79ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a7baccf5d4bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mf1_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mf1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Loss: {:2.6}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training F1: {:3.4}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a7baccf5d4bb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(model, train_x, train_y, test_x, test_y, epochs=500, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PyTorch LSTM module\n",
    "class PyTorchBaseline(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_output, \n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = n_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layers = layers\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.lstm = nn.LSTM(n_input, n_hidden, bidirectional=self.bidirectional, num_layers=layers)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * n_hidden, n_output)\n",
    "        else:\n",
    "            self.fc = nn.Linear(n_hidden, n_output)\n",
    "        if self.layernorm and self.bidirectional:\n",
    "            self.ln = LayerNorm(2 * self.hidden_dim)\n",
    "        elif self.layernorm:\n",
    "            self.ln = LayerNorm(self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        o, (_, _) = self.lstm(x, (h, c))\n",
    "        o = o[-1].unsqueeze(0)\n",
    "        if self.layernorm:\n",
    "            output = self.fc(self.ln(o))\n",
    "        else:\n",
    "            output = self.fc(o)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2593\n"
     ]
    }
   ],
   "source": [
    "model = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(model.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.45927\n",
      "Average Loss: 0.678364\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.56302\n",
      "Average Loss: 0.58079\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.56522\n",
      "Average Loss: 0.578788\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.31799\n",
      "Average Loss: 0.577131\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.30906\n",
      "Average Loss: 0.576287\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.55948\n",
      "Average Loss: 0.577445\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  90/90  -- Loss: 0.57317\n",
      "Average Loss: 0.578531\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #8  : Batch  90/90  -- Loss: 0.81123\n",
      "Average Loss: 0.578521\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #9  : Batch  90/90  -- Loss: 0.30972\n",
      "Average Loss: 0.575707\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #10 : Batch  90/90  -- Loss: 0.55801\n",
      "Average Loss: 0.576991\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #11 : Batch  90/90  -- Loss: 0.31412\n",
      "Average Loss: 0.575952\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #12 : Batch  90/90  -- Loss: 0.30603\n",
      "Average Loss: 0.575915\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #13 : Batch  90/90  -- Loss: 0.78797\n",
      "Average Loss: 0.578051\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #14 : Batch  90/90  -- Loss: 1.03839\n",
      "Average Loss: 0.580424\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #15 : Batch  90/90  -- Loss: 0.55272\n",
      "Average Loss: 0.576645\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #16 : Batch  90/90  -- Loss: 0.55262\n",
      "Average Loss: 0.577597\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #17 : Batch  90/90  -- Loss: 0.52898\n",
      "Average Loss: 0.576444\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #18 : Batch  90/90  -- Loss: 0.84815\n",
      "Average Loss: 0.57811\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #19 : Batch  90/90  -- Loss: 0.80738\n",
      "Average Loss: 0.577701\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #20 : Batch  90/90  -- Loss: 0.57663\n",
      "Average Loss: 0.576937\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #21 : Batch  90/90  -- Loss: 0.81842\n",
      "Average Loss: 0.577756\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #22 : Batch  90/90  -- Loss: 1.03877\n",
      "Average Loss: 0.579362\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #23 : Batch  90/90  -- Loss: 0.55212\n",
      "Average Loss: 0.576739\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #24 : Batch  90/90  -- Loss: 0.81442\n",
      "Average Loss: 0.577783\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #25 : Batch  90/90  -- Loss: 0.60513\n",
      "Average Loss: 0.576617\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #26 : Batch  90/90  -- Loss: 0.30701\n",
      "Average Loss: 0.575646\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #27 : Batch  90/90  -- Loss: 0.56233\n",
      "Average Loss: 0.576914\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #28 : Batch  90/90  -- Loss: 0.85302\n",
      "Average Loss: 0.578225\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #29 : Batch  90/90  -- Loss: 0.57689\n",
      "Average Loss: 0.576876\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #30 : Batch  90/90  -- Loss: 0.82112\n",
      "Average Loss: 0.578053\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #31 : Batch  90/90  -- Loss: 0.31983\n",
      "Average Loss: 0.575321\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #32 : Batch  90/90  -- Loss: 0.29624\n",
      "Average Loss: 0.574995\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #33 : Batch  90/90  -- Loss: 0.79111\n",
      "Average Loss: 0.578239\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #34 : Batch  90/90  -- Loss: 0.29486\n",
      "Average Loss: 0.575172\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #35 : Batch  90/90  -- Loss: 0.78686\n",
      "Average Loss: 0.577653\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #36 : Batch  90/90  -- Loss: 0.54979\n",
      "Average Loss: 0.576256\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #37 : Batch  90/90  -- Loss: 0.29126\n",
      "Average Loss: 0.5754\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #38 : Batch  90/90  -- Loss: 0.81443\n",
      "Average Loss: 0.577713\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #39 : Batch  90/90  -- Loss: 0.85258\n",
      "Average Loss: 0.578138\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #40 : Batch  90/90  -- Loss: 0.29686\n",
      "Average Loss: 0.575455\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #41 : Batch  90/90  -- Loss: 0.77059\n",
      "Average Loss: 0.577748\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #42 : Batch  90/90  -- Loss: 0.31043\n",
      "Average Loss: 0.575604\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #43 : Batch  90/90  -- Loss: 0.53674\n",
      "Average Loss: 0.576645\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #44 : Batch  90/90  -- Loss: 0.82355\n",
      "Average Loss: 0.578025\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #45 : Batch  90/90  -- Loss: 0.30144\n",
      "Average Loss: 0.575158\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #46 : Batch  90/90  -- Loss: 0.31179\n",
      "Average Loss: 0.575078\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #47 : Batch  90/90  -- Loss: 0.78768\n",
      "Average Loss: 0.577787\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #48 : Batch  90/90  -- Loss: 0.60895\n",
      "Average Loss: 0.577026\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #49 : Batch  90/90  -- Loss: 0.55028\n",
      "Average Loss: 0.576564\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #50 : Batch  90/90  -- Loss: 0.57721\n",
      "Average Loss: 0.576698\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #51 : Batch  90/90  -- Loss: 0.59437\n",
      "Average Loss: 0.576484\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #52 : Batch  90/90  -- Loss: 0.78968\n",
      "Average Loss: 0.577843\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #53 : Batch  90/90  -- Loss: 0.32531\n",
      "Average Loss: 0.575598\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #54 : Batch  90/90  -- Loss: 0.30624\n",
      "Average Loss: 0.575284\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #55 : Batch  90/90  -- Loss: 0.57703\n",
      "Average Loss: 0.576677\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #56 : Batch  90/90  -- Loss: 0.29076\n",
      "Average Loss: 0.575066\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #57 : Batch  90/90  -- Loss: 0.56268\n",
      "Average Loss: 0.576645\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #58 : Batch  90/90  -- Loss: 0.82868\n",
      "Average Loss: 0.578071\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #59 : Batch  90/90  -- Loss: 1.06677\n",
      "Average Loss: 0.580144\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #60 : Batch  90/90  -- Loss: 0.59267\n",
      "Average Loss: 0.577225\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #61 : Batch  90/90  -- Loss: 0.57811\n",
      "Average Loss: 0.576979\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #62 : Batch  90/90  -- Loss: 0.31088\n",
      "Average Loss: 0.575052\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #63 : Batch  90/90  -- Loss: 0.59267\n",
      "Average Loss: 0.577113\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #64 : Batch  90/90  -- Loss: 0.56519\n",
      "Average Loss: 0.576575\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #65 : Batch  90/90  -- Loss: 0.53515\n",
      "Average Loss: 0.576316\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #66 : Batch  90/90  -- Loss: 0.30448\n",
      "Average Loss: 0.575217\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #67 : Batch  90/90  -- Loss: 0.53449\n",
      "Average Loss: 0.576738\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #68 : Batch  90/90  -- Loss: 0.82501\n",
      "Average Loss: 0.577752\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #69 : Batch  90/90  -- Loss: 0.87069\n",
      "Average Loss: 0.578326\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #70 : Batch  90/90  -- Loss: 0.83639\n",
      "Average Loss: 0.578353\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #71 : Batch  90/90  -- Loss: 0.54971\n",
      "Average Loss: 0.576354\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #72 : Batch  90/90  -- Loss: 0.32756\n",
      "Average Loss: 0.575456\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #73 : Batch  90/90  -- Loss: 0.33036\n",
      "Average Loss: 0.575903\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #74 : Batch  90/90  -- Loss: 0.55071\n",
      "Average Loss: 0.576138\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #75 : Batch  90/90  -- Loss: 0.88238\n",
      "Average Loss: 0.578452\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #76 : Batch  90/90  -- Loss: 0.60759\n",
      "Average Loss: 0.577019\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #77 : Batch  90/90  -- Loss: 0.80953\n",
      "Average Loss: 0.577588\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #78 : Batch  90/90  -- Loss: 0.28382\n",
      "Average Loss: 0.575224\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #79 : Batch  90/90  -- Loss: 0.28486\n",
      "Average Loss: 0.574833\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #80 : Batch  90/90  -- Loss: 0.31045\n",
      "Average Loss: 0.575473\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #81 : Batch  90/90  -- Loss: 0.57764\n",
      "Average Loss: 0.576739\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #82 : Batch  90/90  -- Loss: 1.03173\n",
      "Average Loss: 0.57934\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #83 : Batch  90/90  -- Loss: 0.60944\n",
      "Average Loss: 0.576751\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #84 : Batch  90/90  -- Loss: 0.82447\n",
      "Average Loss: 0.57863\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #85 : Batch  90/90  -- Loss: 0.81932\n",
      "Average Loss: 0.577923\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #86 : Batch  90/90  -- Loss: 0.79364\n",
      "Average Loss: 0.5783\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #87 : Batch  90/90  -- Loss: 0.55259\n",
      "Average Loss: 0.577766\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #88 : Batch  90/90  -- Loss: 0.33411\n",
      "Average Loss: 0.575577\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #89 : Batch  90/90  -- Loss: 0.59335\n",
      "Average Loss: 0.576981\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #90 : Batch  90/90  -- Loss: 0.30669\n",
      "Average Loss: 0.574895\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #91 : Batch  90/90  -- Loss: 0.59173\n",
      "Average Loss: 0.576351\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #92 : Batch  90/90  -- Loss: 0.30928\n",
      "Average Loss: 0.574995\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #93 : Batch  90/90  -- Loss: 0.57807\n",
      "Average Loss: 0.576923\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #94 : Batch  90/90  -- Loss: 0.30678\n",
      "Average Loss: 0.575203\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #95 : Batch  90/90  -- Loss: 0.31104\n",
      "Average Loss: 0.574977\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #96 : Batch  90/90  -- Loss: 0.32276\n",
      "Average Loss: 0.575155\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #97 : Batch  90/90  -- Loss: 0.59241\n",
      "Average Loss: 0.576546\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #98 : Batch  90/90  -- Loss: 0.53682\n",
      "Average Loss: 0.5762\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #99 : Batch  90/90  -- Loss: 0.56277\n",
      "Average Loss: 0.576585\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #100: Batch  90/90  -- Loss: 0.32389\n",
      "Average Loss: 0.575248\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #101: Batch  90/90  -- Loss: 1.05248\n",
      "Average Loss: 0.579145\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #102: Batch  90/90  -- Loss: 0.31289\n",
      "Average Loss: 0.575047\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #103: Batch  90/90  -- Loss: 0.28806\n",
      "Average Loss: 0.574576\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #104: Batch  90/90  -- Loss: 0.53605\n",
      "Average Loss: 0.576443\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #105: Batch  90/90  -- Loss: 0.55172\n",
      "Average Loss: 0.576508\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #106: Batch  90/90  -- Loss: 0.31959\n",
      "Average Loss: 0.574751\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #107: Batch  90/90  -- Loss: 0.28603\n",
      "Average Loss: 0.575097\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #108: Batch  90/90  -- Loss: 0.57803\n",
      "Average Loss: 0.576475\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #109: Batch  90/90  -- Loss: 0.53576\n",
      "Average Loss: 0.576159\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #110: Batch  90/90  -- Loss: 0.30682\n",
      "Average Loss: 0.574869\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #111: Batch  90/90  -- Loss: 0.60705\n",
      "Average Loss: 0.577329\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #112: Batch  90/90  -- Loss: 0.56655\n",
      "Average Loss: 0.576337\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #113: Batch  90/90  -- Loss: 0.31337\n",
      "Average Loss: 0.576508\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #114: Batch  90/90  -- Loss: 0.53476\n",
      "Average Loss: 0.576234\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #115: Batch  90/90  -- Loss: 0.56468\n",
      "Average Loss: 0.576969\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #116: Batch  90/90  -- Loss: 0.52078\n",
      "Average Loss: 0.576288\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #117: Batch  90/90  -- Loss: 0.54995\n",
      "Average Loss: 0.576355\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #118: Batch  90/90  -- Loss: 1.08526\n",
      "Average Loss: 0.579452\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #119: Batch  90/90  -- Loss: 0.30354\n",
      "Average Loss: 0.576374\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #120: Batch  90/90  -- Loss: 0.84873\n",
      "Average Loss: 0.578\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #121: Batch  90/90  -- Loss: 0.80719\n",
      "Average Loss: 0.577744\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #122: Batch  90/90  -- Loss: 0.83546\n",
      "Average Loss: 0.57792\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #123: Batch  90/90  -- Loss: 0.30688\n",
      "Average Loss: 0.575451\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #124: Batch  90/90  -- Loss: 0.32887\n",
      "Average Loss: 0.575465\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #125: Batch  90/90  -- Loss: 0.79735\n",
      "Average Loss: 0.577442\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #126: Batch  90/90  -- Loss: 0.80658\n",
      "Average Loss: 0.577664\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #127: Batch  90/90  -- Loss: 0.53628\n",
      "Average Loss: 0.57662\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #128: Batch  90/90  -- Loss: 0.57676\n",
      "Average Loss: 0.577025\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #129: Batch  90/90  -- Loss: 1.08639\n",
      "Average Loss: 0.579548\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #130: Batch  90/90  -- Loss: 0.32511\n",
      "Average Loss: 0.5751\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #131: Batch  90/90  -- Loss: 0.55045\n",
      "Average Loss: 0.576752\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #132: Batch  90/90  -- Loss: 0.59347\n",
      "Average Loss: 0.577001\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #133: Batch  90/90  -- Loss: 0.55145\n",
      "Average Loss: 0.576191\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #134: Batch  90/90  -- Loss: 0.30867\n",
      "Average Loss: 0.574749\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #135: Batch  90/90  -- Loss: 0.53787\n",
      "Average Loss: 0.576349\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #136: Batch  90/90  -- Loss: 0.60879\n",
      "Average Loss: 0.576752\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #137: Batch  90/90  -- Loss: 0.28999\n",
      "Average Loss: 0.575273\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #138: Batch  90/90  -- Loss: 1.08365\n",
      "Average Loss: 0.579642\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #139: Batch  90/90  -- Loss: 0.57787\n",
      "Average Loss: 0.576537\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #140: Batch  90/90  -- Loss: 0.77741\n",
      "Average Loss: 0.578219\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #141: Batch  90/90  -- Loss: 0.60732\n",
      "Average Loss: 0.576404\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #142: Batch  90/90  -- Loss: 0.55036\n",
      "Average Loss: 0.576588\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #143: Batch  90/90  -- Loss: 0.80656\n",
      "Average Loss: 0.578051\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #144: Batch  90/90  -- Loss: 1.28874\n",
      "Average Loss: 0.580484\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #145: Batch  90/90  -- Loss: 0.55079\n",
      "Average Loss: 0.576215\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #146: Batch  90/90  -- Loss: 0.82334\n",
      "Average Loss: 0.577994\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #147: Batch  90/90  -- Loss: 0.57744\n",
      "Average Loss: 0.576727\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #148: Batch  90/90  -- Loss: 0.78106\n",
      "Average Loss: 0.577806\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #149: Batch  90/90  -- Loss: 0.57806\n",
      "Average Loss: 0.576557\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #150: Batch  90/90  -- Loss: 0.27384\n",
      "Average Loss: 0.574735\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #151: Batch  90/90  -- Loss: 0.59398\n",
      "Average Loss: 0.576647\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #152: Batch  90/90  -- Loss: 0.56421\n",
      "Average Loss: 0.576396\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #153: Batch  90/90  -- Loss: 0.32662\n",
      "Average Loss: 0.575044\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #154: Batch  90/90  -- Loss: 0.79452\n",
      "Average Loss: 0.57768\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #155: Batch  90/90  -- Loss: 0.27547\n",
      "Average Loss: 0.574671\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #156: Batch  90/90  -- Loss: 0.30242\n",
      "Average Loss: 0.574782\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #157: Batch  90/90  -- Loss: 0.86214\n",
      "Average Loss: 0.578482\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #158: Batch  90/90  -- Loss: 0.33133\n",
      "Average Loss: 0.575152\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #159: Batch  90/90  -- Loss: 0.59264\n",
      "Average Loss: 0.576815\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #160: Batch  90/90  -- Loss: 0.59622\n",
      "Average Loss: 0.576329\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #161: Batch  90/90  -- Loss: 0.57592\n",
      "Average Loss: 0.576295\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #162: Batch  90/90  -- Loss: 0.84555\n",
      "Average Loss: 0.577745\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #163: Batch  90/90  -- Loss: 0.57952\n",
      "Average Loss: 0.576479\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #164: Batch  90/90  -- Loss: 0.28707\n",
      "Average Loss: 0.574879\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #165: Batch  90/90  -- Loss: 1.10898\n",
      "Average Loss: 0.580098\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #166: Batch  90/90  -- Loss: 0.30885\n",
      "Average Loss: 0.574789\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #167: Batch  90/90  -- Loss: 0.55586\n",
      "Average Loss: 0.576362\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #168: Batch  90/90  -- Loss: 0.81829\n",
      "Average Loss: 0.577371\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #169: Batch  90/90  -- Loss: 0.80596\n",
      "Average Loss: 0.578014\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #170: Batch  90/90  -- Loss: 0.82525\n",
      "Average Loss: 0.577695\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #171: Batch  90/90  -- Loss: 1.13886\n",
      "Average Loss: 0.579113\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #172: Batch  90/90  -- Loss: 0.58198\n",
      "Average Loss: 0.575588\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #173: Batch  90/90  -- Loss: 0.54034\n",
      "Average Loss: 0.576004\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #174: Batch  90/90  -- Loss: 0.34144\n",
      "Average Loss: 0.574643\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #175: Batch  90/90  -- Loss: 0.99764\n",
      "Average Loss: 0.577806\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #176: Batch  90/90  -- Loss: 0.52845\n",
      "Average Loss: 0.574447\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #177: Batch  90/90  -- Loss: 0.52833\n",
      "Average Loss: 0.574268\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #178: Batch  90/90  -- Loss: 0.30635\n",
      "Average Loss: 0.572904\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #179: Batch  90/90  -- Loss: 0.55328\n",
      "Average Loss: 0.57288\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #180: Batch  90/90  -- Loss: 0.30392\n",
      "Average Loss: 0.572101\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #181: Batch  90/90  -- Loss: 0.29576\n",
      "Average Loss: 0.572817\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #182: Batch  90/90  -- Loss: 1.26836\n",
      "Average Loss: 0.575598\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #183: Batch  90/90  -- Loss: 0.34463\n",
      "Average Loss: 0.569095\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #184: Batch  90/90  -- Loss: 0.80181\n",
      "Average Loss: 0.567524\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #185: Batch  90/90  -- Loss: 0.25349\n",
      "Average Loss: 0.552983\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #186: Batch  90/90  -- Loss: 0.47897\n",
      "Average Loss: 0.519954\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #187: Batch  90/90  -- Loss: 0.34075\n",
      "Average Loss: 0.486093\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #188: Batch  90/90  -- Loss: 0.42518\n",
      "Average Loss: 0.46203\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #189: Batch  90/90  -- Loss: 0.44721\n",
      "Average Loss: 0.444804\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #190: Batch  90/90  -- Loss: 0.33072\n",
      "Average Loss: 0.42704\n",
      "Training F1: 0.4161\n",
      "Test F1: 0.2549\n",
      "Epoch #191: Batch  90/90  -- Loss: 0.76393\n",
      "Average Loss: 0.417692\n",
      "Training F1: 0.07921\n",
      "Test F1: 0.0\n",
      "Epoch #192: Batch  90/90  -- Loss: 0.34681\n",
      "Average Loss: 0.406642\n",
      "Training F1: 0.5275\n",
      "Test F1: 0.3443\n",
      "Epoch #193: Batch  90/90  -- Loss: 0.607842\n",
      "Average Loss: 0.399176\n",
      "Training F1: 0.3016\n",
      "Test F1: 0.2022\n",
      "Epoch #194: Batch  90/90  -- Loss: 0.57284\n",
      "Average Loss: 0.394793\n",
      "Training F1: 0.4228\n",
      "Test F1: 0.2476\n",
      "Epoch #195: Batch  90/90  -- Loss: 0.296913\n",
      "Average Loss: 0.386983\n",
      "Training F1: 0.5146\n",
      "Test F1: 0.3103\n",
      "Epoch #196: Batch  90/90  -- Loss: 0.49348\n",
      "Average Loss: 0.384544\n",
      "Training F1: 0.5058\n",
      "Test F1: 0.3103\n",
      "Epoch #197: Batch  90/90  -- Loss: 0.473434\n",
      "Average Loss: 0.380217\n",
      "Training F1: 0.5239\n",
      "Test F1: 0.3051\n",
      "Epoch #198: Batch  90/90  -- Loss: 0.685132\n",
      "Average Loss: 0.377946\n",
      "Training F1: 0.5143\n",
      "Test F1: 0.3077\n",
      "Epoch #199: Batch  90/90  -- Loss: 0.43312\n",
      "Average Loss: 0.373952\n",
      "Training F1: 0.5315\n",
      "Test F1: 0.3471\n",
      "Epoch #200: Batch  90/90  -- Loss: 0.23692\n",
      "Average Loss: 0.371652\n",
      "Training F1: 0.5455\n",
      "Test F1: 0.371\n",
      "Epoch #201: Batch  90/90  -- Loss: 0.315255\n",
      "Average Loss: 0.369881\n",
      "Training F1: 0.5519\n",
      "Test F1: 0.336\n",
      "Epoch #202: Batch  90/90  -- Loss: 0.31131\n",
      "Average Loss: 0.367798\n",
      "Training F1: 0.5465\n",
      "Test F1: 0.3577\n",
      "Epoch #203: Batch  90/90  -- Loss: 0.30784\n",
      "Average Loss: 0.366618\n",
      "Training F1: 0.5501\n",
      "Test F1: 0.3252\n",
      "Epoch #204: Batch  90/90  -- Loss: 0.38354\n",
      "Average Loss: 0.365392\n",
      "Training F1: 0.5995\n",
      "Test F1: 0.4058\n",
      "Epoch #205: Batch  90/90  -- Loss: 0.286028\n",
      "Average Loss: 0.363489\n",
      "Training F1: 0.4894\n",
      "Test F1: 0.3158\n",
      "Epoch #206: Batch  90/90  -- Loss: 0.347914\n",
      "Average Loss: 0.363096\n",
      "Training F1: 0.5934\n",
      "Test F1: 0.4203\n",
      "Epoch #207: Batch  90/90  -- Loss: 0.38691\n",
      "Average Loss: 0.361223\n",
      "Training F1: 0.5913\n",
      "Test F1: 0.4203\n",
      "Epoch #208: Batch  90/90  -- Loss: 0.487752\n",
      "Average Loss: 0.359569\n",
      "Training F1: 0.5773\n",
      "Test F1: 0.4\n",
      "Epoch #209: Batch  90/90  -- Loss: 0.210425\n",
      "Average Loss: 0.360536\n",
      "Training F1: 0.5812\n",
      "Test F1: 0.4058\n",
      "Epoch #210: Batch  90/90  -- Loss: 0.173113\n",
      "Average Loss: 0.35612\n",
      "Training F1: 0.5656\n",
      "Test F1: 0.3971\n",
      "Epoch #211: Batch  90/90  -- Loss: 0.745484\n",
      "Average Loss: 0.361796\n",
      "Training F1: 0.5393\n",
      "Test F1: 0.4361\n",
      "Epoch #212: Batch  90/90  -- Loss: 0.327643\n",
      "Average Loss: 0.35686\n",
      "Training F1: 0.601\n",
      "Test F1: 0.4901\n",
      "Epoch #213: Batch  90/90  -- Loss: 0.20936\n",
      "Average Loss: 0.355843\n",
      "Training F1: 0.593\n",
      "Test F1: 0.4865\n",
      "Epoch #214: Batch  90/90  -- Loss: 0.201913\n",
      "Average Loss: 0.356098\n",
      "Training F1: 0.5419\n",
      "Test F1: 0.4559\n",
      "Epoch #215: Batch  90/90  -- Loss: 0.204334\n",
      "Average Loss: 0.353812\n",
      "Training F1: 0.6203\n",
      "Test F1: 0.5032\n",
      "Epoch #216: Batch  90/90  -- Loss: 0.164118\n",
      "Average Loss: 0.352612\n",
      "Training F1: 0.6158\n",
      "Test F1: 0.5256\n",
      "Epoch #217: Batch  90/90  -- Loss: 0.383764\n",
      "Average Loss: 0.351399\n",
      "Training F1: 0.6345\n",
      "Test F1: 0.561\n",
      "Epoch #218: Batch  90/90  -- Loss: 0.431129\n",
      "Average Loss: 0.351674\n",
      "Training F1: 0.6578\n",
      "Test F1: 0.5647\n",
      "Epoch #219: Batch  90/90  -- Loss: 0.33868\n",
      "Average Loss: 0.349563\n",
      "Training F1: 0.6395\n",
      "Test F1: 0.5488\n",
      "Epoch #220: Batch  90/90  -- Loss: 0.336775\n",
      "Average Loss: 0.349357\n",
      "Training F1: 0.643\n",
      "Test F1: 0.5283\n",
      "Epoch #221: Batch  90/90  -- Loss: 0.017085\n",
      "Average Loss: 0.346395\n",
      "Training F1: 0.6512\n",
      "Test F1: 0.5521\n",
      "Epoch #222: Batch  90/90  -- Loss: 0.326137\n",
      "Average Loss: 0.347576\n",
      "Training F1: 0.6697\n",
      "Test F1: 0.5663\n",
      "Epoch #223: Batch  90/90  -- Loss: 0.473556\n",
      "Average Loss: 0.347086\n",
      "Training F1: 0.6681\n",
      "Test F1: 0.5977\n",
      "Epoch #224: Batch  90/90  -- Loss: 0.214092\n",
      "Average Loss: 0.34372\n",
      "Training F1: 0.6525\n",
      "Test F1: 0.5283\n",
      "Epoch #225: Batch  90/90  -- Loss: 0.293255\n",
      "Average Loss: 0.344669\n",
      "Training F1: 0.6523\n",
      "Test F1: 0.5223\n",
      "Epoch #226: Batch  90/90  -- Loss: 0.225044\n",
      "Average Loss: 0.343523\n",
      "Training F1: 0.6559\n",
      "Test F1: 0.5521\n",
      "Epoch #227: Batch  90/90  -- Loss: 0.760319\n",
      "Average Loss: 0.344122\n",
      "Training F1: 0.674\n",
      "Test F1: 0.568\n",
      "Epoch #228: Batch  90/90  -- Loss: 0.365919\n",
      "Average Loss: 0.342931\n",
      "Training F1: 0.662\n",
      "Test F1: 0.5679\n",
      "Epoch #229: Batch  90/90  -- Loss: 0.488094\n",
      "Average Loss: 0.342176\n",
      "Training F1: 0.6635\n",
      "Test F1: 0.55\n",
      "Epoch #230: Batch  90/90  -- Loss: 0.591018\n",
      "Average Loss: 0.339733\n",
      "Training F1: 0.6953\n",
      "Test F1: 0.5967\n",
      "Epoch #231: Batch  90/90  -- Loss: 0.131591\n",
      "Average Loss: 0.337167\n",
      "Training F1: 0.6804\n",
      "Test F1: 0.5697\n",
      "Epoch #232: Batch  90/90  -- Loss: 0.656218\n",
      "Average Loss: 0.338277\n",
      "Training F1: 0.6835\n",
      "Test F1: 0.5697\n",
      "Epoch #233: Batch  90/90  -- Loss: 0.232496\n",
      "Average Loss: 0.334956\n",
      "Training F1: 0.6912\n",
      "Test F1: 0.5629\n",
      "Epoch #234: Batch  90/90  -- Loss: 0.50472\n",
      "Average Loss: 0.335593\n",
      "Training F1: 0.6569\n",
      "Test F1: 0.566\n",
      "Epoch #235: Batch  90/90  -- Loss: 0.238226\n",
      "Average Loss: 0.332618\n",
      "Training F1: 0.7032\n",
      "Test F1: 0.5679\n",
      "Epoch #236: Batch  90/90  -- Loss: 0.352728\n",
      "Average Loss: 0.330609\n",
      "Training F1: 0.7043\n",
      "Test F1: 0.6023\n",
      "Epoch #237: Batch  90/90  -- Loss: 0.384575\n",
      "Average Loss: 0.328521\n",
      "Training F1: 0.6983\n",
      "Test F1: 0.5641\n",
      "Epoch #238: Batch  90/90  -- Loss: 0.712637\n",
      "Average Loss: 0.330701\n",
      "Training F1: 0.7033\n",
      "Test F1: 0.5823\n",
      "Epoch #239: Batch  90/90  -- Loss: 0.142577\n",
      "Average Loss: 0.323907\n",
      "Training F1: 0.7217\n",
      "Test F1: 0.6057\n",
      "Epoch #240: Batch  90/90  -- Loss: 0.465488\n",
      "Average Loss: 0.321927\n",
      "Training F1: 0.6946\n",
      "Test F1: 0.5949\n",
      "Epoch #241: Batch  90/90  -- Loss: 0.395511\n",
      "Average Loss: 0.320024\n",
      "Training F1: 0.7187\n",
      "Test F1: 0.5963\n",
      "Epoch #242: Batch  90/90  -- Loss: 0.1385348\n",
      "Average Loss: 0.315126\n",
      "Training F1: 0.7209\n",
      "Test F1: 0.6049\n",
      "Epoch #243: Batch  90/90  -- Loss: 0.593487\n",
      "Average Loss: 0.312314\n",
      "Training F1: 0.7414\n",
      "Test F1: 0.6136\n",
      "Epoch #244: Batch  90/90  -- Loss: 0.3140858\n",
      "Average Loss: 0.311844\n",
      "Training F1: 0.7366\n",
      "Test F1: 0.6038\n",
      "Epoch #245: Batch  90/90  -- Loss: 0.010431\n",
      "Average Loss: 0.303164\n",
      "Training F1: 0.7229\n",
      "Test F1: 0.5987\n",
      "Epoch #246: Batch  90/90  -- Loss: 0.013512\n",
      "Average Loss: 0.29968\n",
      "Training F1: 0.7449\n",
      "Test F1: 0.6118\n",
      "Epoch #247: Batch  90/90  -- Loss: 0.448757\n",
      "Average Loss: 0.301209\n",
      "Training F1: 0.7562\n",
      "Test F1: 0.6316\n",
      "Epoch #248: Batch  90/90  -- Loss: 0.0341966\n",
      "Average Loss: 0.294493\n",
      "Training F1: 0.7651\n",
      "Test F1: 0.6364\n",
      "Epoch #249: Batch  90/90  -- Loss: 0.164716\n",
      "Average Loss: 0.292546\n",
      "Training F1: 0.7681\n",
      "Test F1: 0.65\n",
      "Epoch #250: Batch  90/90  -- Loss: 0.313728\n",
      "Average Loss: 0.289002\n",
      "Training F1: 0.7674\n",
      "Test F1: 0.642\n",
      "Epoch #251: Batch  90/90  -- Loss: 0.164576\n",
      "Average Loss: 0.283912\n",
      "Training F1: 0.7529\n",
      "Test F1: 0.6545\n",
      "Epoch #252: Batch  90/90  -- Loss: 0.478235\n",
      "Average Loss: 0.282682\n",
      "Training F1: 0.75\n",
      "Test F1: 0.6541\n",
      "Epoch #253: Batch  90/90  -- Loss: 0.4906465\n",
      "Average Loss: 0.282823\n",
      "Training F1: 0.7533\n",
      "Test F1: 0.6533\n",
      "Epoch #254: Batch  90/90  -- Loss: 0.185746\n",
      "Average Loss: 0.279182\n",
      "Training F1: 0.7519\n",
      "Test F1: 0.6624\n",
      "Epoch #255: Batch  90/90  -- Loss: 0.212583\n",
      "Average Loss: 0.272331\n",
      "Training F1: 0.7586\n",
      "Test F1: 0.6497\n",
      "Epoch #256: Batch  90/90  -- Loss: 0.084369\n",
      "Average Loss: 0.271796\n",
      "Training F1: 0.7677\n",
      "Test F1: 0.6753\n",
      "Epoch #257: Batch  90/90  -- Loss: 0.209373\n",
      "Average Loss: 0.268731\n",
      "Training F1: 0.7912\n",
      "Test F1: 0.6879\n",
      "Epoch #258: Batch  90/90  -- Loss: 0.227871\n",
      "Average Loss: 0.262984\n",
      "Training F1: 0.7688\n",
      "Test F1: 0.6667\n",
      "Epoch #259: Batch  90/90  -- Loss: 0.547391\n",
      "Average Loss: 0.26169\n",
      "Training F1: 0.7828\n",
      "Test F1: 0.6974\n",
      "Epoch #260: Batch  90/90  -- Loss: 0.068621\n",
      "Average Loss: 0.25651\n",
      "Training F1: 0.797\n",
      "Test F1: 0.6711\n",
      "Epoch #261: Batch  90/90  -- Loss: 0.272975\n",
      "Average Loss: 0.260352\n",
      "Training F1: 0.7868\n",
      "Test F1: 0.68\n",
      "Epoch #262: Batch  90/90  -- Loss: 0.453091\n",
      "Average Loss: 0.252414\n",
      "Training F1: 0.796\n",
      "Test F1: 0.6667\n",
      "Epoch #263: Batch  90/90  -- Loss: 0.479012\n",
      "Average Loss: 0.251174\n",
      "Training F1: 0.799\n",
      "Test F1: 0.6711\n",
      "Epoch #264: Batch  90/90  -- Loss: 0.563014\n",
      "Average Loss: 0.250777\n",
      "Training F1: 0.7842\n",
      "Test F1: 0.6475\n",
      "Epoch #265: Batch  90/90  -- Loss: 0.2862708\n",
      "Average Loss: 0.243647\n",
      "Training F1: 0.8193\n",
      "Test F1: 0.6622\n",
      "Epoch #266: Batch  90/90  -- Loss: 0.2912459\n",
      "Average Loss: 0.239442\n",
      "Training F1: 0.799\n",
      "Test F1: 0.6331\n",
      "Epoch #267: Batch  90/90  -- Loss: 0.054434\n",
      "Average Loss: 0.235195\n",
      "Training F1: 0.8293\n",
      "Test F1: 0.6797\n",
      "Epoch #268: Batch  90/90  -- Loss: 0.271148\n",
      "Average Loss: 0.238215\n",
      "Training F1: 0.8163\n",
      "Test F1: 0.6525\n",
      "Epoch #269: Batch  90/90  -- Loss: 0.0025217\n",
      "Average Loss: 0.231349\n",
      "Training F1: 0.8074\n",
      "Test F1: 0.6377\n",
      "Epoch #270: Batch  90/90  -- Loss: 0.833514\n",
      "Average Loss: 0.230708\n",
      "Training F1: 0.825\n",
      "Test F1: 0.7027\n",
      "Epoch #271: Batch  90/90  -- Loss: 0.180625\n",
      "Average Loss: 0.225598\n",
      "Training F1: 0.8338\n",
      "Test F1: 0.6475\n",
      "Epoch #272: Batch  90/90  -- Loss: 0.215283\n",
      "Average Loss: 0.220924\n",
      "Training F1: 0.8135\n",
      "Test F1: 0.6939\n",
      "Epoch #273: Batch  90/90  -- Loss: 0.0051429\n",
      "Average Loss: 0.217893\n",
      "Training F1: 0.8128\n",
      "Test F1: 0.6667\n",
      "Epoch #274: Batch  90/90  -- Loss: 0.074322\n",
      "Average Loss: 0.210478\n",
      "Training F1: 0.8354\n",
      "Test F1: 0.7007\n",
      "Epoch #275: Batch  90/90  -- Loss: 0.549742\n",
      "Average Loss: 0.214947\n",
      "Training F1: 0.8329\n",
      "Test F1: 0.7172\n",
      "Epoch #276: Batch  90/90  -- Loss: 0.344341\n",
      "Average Loss: 0.20775\n",
      "Training F1: 0.8285\n",
      "Test F1: 0.7111\n",
      "Epoch #277: Batch  90/90  -- Loss: 0.0037241\n",
      "Average Loss: 0.20363\n",
      "Training F1: 0.8055\n",
      "Test F1: 0.7231\n",
      "Epoch #278: Batch  90/90  -- Loss: 0.321017\n",
      "Average Loss: 0.203959\n",
      "Training F1: 0.8564\n",
      "Test F1: 0.7194\n",
      "Epoch #279: Batch  90/90  -- Loss: 0.0585938\n",
      "Average Loss: 0.202188\n",
      "Training F1: 0.8491\n",
      "Test F1: 0.7482\n",
      "Epoch #280: Batch  90/90  -- Loss: 0.1974576\n",
      "Average Loss: 0.197956\n",
      "Training F1: 0.8665\n",
      "Test F1: 0.7376\n",
      "Epoch #281: Batch  90/90  -- Loss: 0.017172\n",
      "Average Loss: 0.19321\n",
      "Training F1: 0.8615\n",
      "Test F1: 0.7234\n",
      "Epoch #282: Batch  90/90  -- Loss: 0.613743\n",
      "Average Loss: 0.190845\n",
      "Training F1: 0.8635\n",
      "Test F1: 0.7429\n",
      "Epoch #283: Batch  90/90  -- Loss: 0.524391\n",
      "Average Loss: 0.193999\n",
      "Training F1: 0.8594\n",
      "Test F1: 0.7287\n",
      "Epoch #284: Batch  90/90  -- Loss: 0.107065\n",
      "Average Loss: 0.193577\n",
      "Training F1: 0.8693\n",
      "Test F1: 0.7391\n",
      "Epoch #285: Batch  90/90  -- Loss: 0.075241\n",
      "Average Loss: 0.179384\n",
      "Training F1: 0.8821\n",
      "Test F1: 0.7857\n",
      "Epoch #286: Batch  90/90  -- Loss: 0.0806419\n",
      "Average Loss: 0.18277\n",
      "Training F1: 0.8713\n",
      "Test F1: 0.75\n",
      "Epoch #287: Batch  90/90  -- Loss: 0.016555\n",
      "Average Loss: 0.174373\n",
      "Training F1: 0.8693\n",
      "Test F1: 0.7313\n",
      "Epoch #288: Batch  90/90  -- Loss: 0.325399\n",
      "Average Loss: 0.173111\n",
      "Training F1: 0.8872\n",
      "Test F1: 0.7794\n",
      "Epoch #289: Batch  90/90  -- Loss: 0.066815\n",
      "Average Loss: 0.169985\n",
      "Training F1: 0.8918\n",
      "Test F1: 0.7857\n",
      "Epoch #290: Batch  90/90  -- Loss: 0.129762\n",
      "Average Loss: 0.165834\n",
      "Training F1: 0.8953\n",
      "Test F1: 0.7852\n",
      "Epoch #291: Batch  90/90  -- Loss: 0.060091\n",
      "Average Loss: 0.160222\n",
      "Training F1: 0.8969\n",
      "Test F1: 0.7714\n",
      "Epoch #292: Batch  90/90  -- Loss: 0.097712\n",
      "Average Loss: 0.161426\n",
      "Training F1: 0.9081\n",
      "Test F1: 0.782\n",
      "Epoch #293: Batch  90/90  -- Loss: 0.074088\n",
      "Average Loss: 0.15681\n",
      "Training F1: 0.9153\n",
      "Test F1: 0.7727\n",
      "Epoch #294: Batch  90/90  -- Loss: 0.1353967\n",
      "Average Loss: 0.150745\n",
      "Training F1: 0.8951\n",
      "Test F1: 0.8029\n",
      "Epoch #295: Batch  90/90  -- Loss: 0.160075\n",
      "Average Loss: 0.14592\n",
      "Training F1: 0.9231\n",
      "Test F1: 0.8286\n",
      "Epoch #296: Batch  90/90  -- Loss: 0.0458312\n",
      "Average Loss: 0.143785\n",
      "Training F1: 0.9299\n",
      "Test F1: 0.7761\n",
      "Epoch #297: Batch  90/90  -- Loss: 0.0039137\n",
      "Average Loss: 0.140189\n",
      "Training F1: 0.9302\n",
      "Test F1: 0.8227\n",
      "Epoch #298: Batch  90/90  -- Loss: 0.036154\n",
      "Average Loss: 0.142432\n",
      "Training F1: 0.9167\n",
      "Test F1: 0.8\n",
      "Epoch #299: Batch  90/90  -- Loss: 0.0043699\n",
      "Average Loss: 0.132483\n",
      "Training F1: 0.9264\n",
      "Test F1: 0.8095\n",
      "Epoch #300: Batch  90/90  -- Loss: 0.1343611\n",
      "Average Loss: 0.13084\n",
      "Training F1: 0.9337\n",
      "Test F1: 0.8397\n",
      "Epoch #301: Batch  90/90  -- Loss: 0.0607193\n",
      "Average Loss: 0.124022\n",
      "Training F1: 0.9354\n",
      "Test F1: 0.8571\n",
      "Epoch #302: Batch  90/90  -- Loss: 0.1026147\n",
      "Average Loss: 0.124405\n",
      "Training F1: 0.929\n",
      "Test F1: 0.8095\n",
      "Epoch #303: Batch  90/90  -- Loss: 0.0688725\n",
      "Average Loss: 0.118207\n",
      "Training F1: 0.9446\n",
      "Test F1: 0.8485\n",
      "Epoch #304: Batch  90/90  -- Loss: 0.0584921\n",
      "Average Loss: 0.11464\n",
      "Training F1: 0.9496\n",
      "Test F1: 0.8722\n",
      "Epoch #305: Batch  90/90  -- Loss: 0.153069\n",
      "Average Loss: 0.111434\n",
      "Training F1: 0.9418\n",
      "Test F1: 0.8702\n",
      "Epoch #306: Batch  90/90  -- Loss: 0.0428314\n",
      "Average Loss: 0.104655\n",
      "Training F1: 0.937\n",
      "Test F1: 0.8099\n",
      "Epoch #307: Batch  90/90  -- Loss: 0.1332424\n",
      "Average Loss: 0.106758\n",
      "Training F1: 0.9337\n",
      "Test F1: 0.8189\n",
      "Epoch #308: Batch  90/90  -- Loss: 0.0851272\n",
      "Average Loss: 0.102198\n",
      "Training F1: 0.9534\n",
      "Test F1: 0.8759\n",
      "Epoch #309: Batch  90/90  -- Loss: 0.0436023\n",
      "Average Loss: 0.100705\n",
      "Training F1: 0.9399\n",
      "Test F1: 0.8759\n",
      "Epoch #310: Batch  90/90  -- Loss: 0.2896266\n",
      "Average Loss: 0.11509\n",
      "Training F1: 0.9551\n",
      "Test F1: 0.8769\n",
      "Epoch #311: Batch  90/90  -- Loss: 0.0633363\n",
      "Average Loss: 0.0921677\n",
      "Training F1: 0.9542\n",
      "Test F1: 0.8837\n",
      "Epoch #312: Batch  90/90  -- Loss: 0.1055975\n",
      "Average Loss: 0.0858934\n",
      "Training F1: 0.9641\n",
      "Test F1: 0.9028\n",
      "Epoch #313: Batch  90/90  -- Loss: 0.1006389\n",
      "Average Loss: 0.0883874\n",
      "Training F1: 0.9655\n",
      "Test F1: 0.8923\n",
      "Epoch #314: Batch  90/90  -- Loss: 0.1359325\n",
      "Average Loss: 0.0814356\n",
      "Training F1: 0.9661\n",
      "Test F1: 0.913\n",
      "Epoch #315: Batch  90/90  -- Loss: 0.0011736\n",
      "Average Loss: 0.0801752\n",
      "Training F1: 0.9689\n",
      "Test F1: 0.9078\n",
      "Epoch #316: Batch  90/90  -- Loss: 0.1403482\n",
      "Average Loss: 0.0756464\n",
      "Training F1: 0.9622\n",
      "Test F1: 0.8819\n",
      "Epoch #317: Batch  90/90  -- Loss: 0.0064095\n",
      "Average Loss: 0.0728902\n",
      "Training F1: 0.971\n",
      "Test F1: 0.9008\n",
      "Epoch #318: Batch  90/90  -- Loss: 0.0030764\n",
      "Average Loss: 0.0714478\n",
      "Training F1: 0.9708\n",
      "Test F1: 0.8992\n",
      "Epoch #319: Batch  90/90  -- Loss: 0.0397571\n",
      "Average Loss: 0.0709303\n",
      "Training F1: 0.9737\n",
      "Test F1: 0.9481\n",
      "Epoch #320: Batch  90/90  -- Loss: 0.0035245\n",
      "Average Loss: 0.0652595\n",
      "Training F1: 0.9791\n",
      "Test F1: 0.942\n",
      "Epoch #321: Batch  90/90  -- Loss: 0.1169343\n",
      "Average Loss: 0.0616399\n",
      "Training F1: 0.9793\n",
      "Test F1: 0.8986\n",
      "Epoch #322: Batch  90/90  -- Loss: 0.0202992\n",
      "Average Loss: 0.061952\n",
      "Training F1: 0.9813\n",
      "Test F1: 0.9385\n",
      "Epoch #323: Batch  90/90  -- Loss: 0.1891683\n",
      "Average Loss: 0.0618755\n",
      "Training F1: 0.9818\n",
      "Test F1: 0.9481\n",
      "Epoch #324: Batch  90/90  -- Loss: 0.0128312\n",
      "Average Loss: 0.0599394\n",
      "Training F1: 0.9728\n",
      "Test F1: 0.9194\n",
      "Epoch #325: Batch  90/90  -- Loss: 0.0470891\n",
      "Average Loss: 0.0559749\n",
      "Training F1: 0.9895\n",
      "Test F1: 0.9538\n",
      "Epoch #326: Batch  90/90  -- Loss: 0.0720143\n",
      "Average Loss: 0.0523784\n",
      "Training F1: 0.9868\n",
      "Test F1: 0.9701\n",
      "Epoch #327: Batch  90/90  -- Loss: 0.0331716\n",
      "Average Loss: 0.0494915\n",
      "Training F1: 0.9842\n",
      "Test F1: 0.9545\n",
      "Epoch #328: Batch  90/90  -- Loss: 0.0782123\n",
      "Average Loss: 0.0470284\n",
      "Training F1: 0.9895\n",
      "Test F1: 0.9545\n",
      "Epoch #329: Batch  90/90  -- Loss: 0.0169716\n",
      "Average Loss: 0.0442527\n",
      "Training F1: 0.9793\n",
      "Test F1: 0.9286\n",
      "Epoch #330: Batch  90/90  -- Loss: 0.02193847\n",
      "Average Loss: 0.0449976\n",
      "Training F1: 0.9869\n",
      "Test F1: 0.9618\n",
      "Epoch #331: Batch  90/90  -- Loss: 0.0058479\n",
      "Average Loss: 0.0436106\n",
      "Training F1: 0.9893\n",
      "Test F1: 0.9365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #332: Batch  90/90  -- Loss: 0.0523134\n",
      "Average Loss: 0.0408116\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9618\n",
      "Epoch #333: Batch  90/90  -- Loss: 0.2269419\n",
      "Average Loss: 0.0409718\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9618\n",
      "Epoch #334: Batch  90/90  -- Loss: 0.0012478\n",
      "Average Loss: 0.0392897\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9697\n",
      "Epoch #335: Batch  90/90  -- Loss: 0.1988451\n",
      "Average Loss: 0.0366872\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9692\n",
      "Epoch #336: Batch  90/90  -- Loss: 0.0276229\n",
      "Average Loss: 0.0336763\n",
      "Training F1: 0.9869\n",
      "Test F1: 0.9706\n",
      "Epoch #337: Batch  90/90  -- Loss: 0.0486611\n",
      "Average Loss: 0.0325859\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9612\n",
      "Epoch #338: Batch  90/90  -- Loss: 0.02148911\n",
      "Average Loss: 0.0297609\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9774\n",
      "Epoch #339: Batch  90/90  -- Loss: 0.00287683\n",
      "Average Loss: 0.0290608\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #340: Batch  90/90  -- Loss: 0.0006058\n",
      "Average Loss: 0.0262582\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #341: Batch  90/90  -- Loss: 0.0086941\n",
      "Average Loss: 0.0251354\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9771\n",
      "Epoch #342: Batch  90/90  -- Loss: 0.0166225\n",
      "Average Loss: 0.0242111\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #343: Batch  90/90  -- Loss: 0.0057896\n",
      "Average Loss: 0.0222957\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9612\n",
      "Epoch #344: Batch  90/90  -- Loss: 0.0107447\n",
      "Average Loss: 0.0218241\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #345: Batch  90/90  -- Loss: 0.00825576\n",
      "Average Loss: 0.0207489\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9692\n",
      "Epoch #346: Batch  90/90  -- Loss: 0.0029129\n",
      "Average Loss: 0.0210868\n",
      "Training F1: 0.992\n",
      "Test F1: 0.9692\n",
      "Epoch #347: Batch  90/90  -- Loss: 0.03213485\n",
      "Average Loss: 0.0740158\n",
      "Training F1: 0.956\n",
      "Test F1: 0.9466\n",
      "Epoch #348: Batch  90/90  -- Loss: 0.0385114\n",
      "Average Loss: 0.0398279\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9926\n",
      "Epoch #349: Batch  90/90  -- Loss: 0.0037771\n",
      "Average Loss: 0.0185662\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #350: Batch  90/90  -- Loss: 0.0362049\n",
      "Average Loss: 0.0174481\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #351: Batch  90/90  -- Loss: 0.0034516\n",
      "Average Loss: 0.0156608\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #352: Batch  90/90  -- Loss: 0.00412233\n",
      "Average Loss: 0.0148884\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #353: Batch  90/90  -- Loss: 0.03369416\n",
      "Average Loss: 0.0144365\n",
      "Training F1: 0.9947\n",
      "Test F1: 0.9531\n",
      "Epoch #354: Batch  90/90  -- Loss: 0.0022961\n",
      "Average Loss: 0.0148888\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #355: Batch  90/90  -- Loss: 0.00890769\n",
      "Average Loss: 0.0129601\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #356: Batch  90/90  -- Loss: 0.01510293\n",
      "Average Loss: 0.0121352\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9851\n",
      "Epoch #357: Batch  90/90  -- Loss: 0.02274586\n",
      "Average Loss: 0.0114218\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #358: Batch  90/90  -- Loss: 0.00901753\n",
      "Average Loss: 0.0135502\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #359: Batch  90/90  -- Loss: 6.4281e-05\n",
      "Average Loss: 0.0110327\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #360: Batch  90/90  -- Loss: 0.00312589\n",
      "Average Loss: 0.00990332\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #361: Batch  90/90  -- Loss: 0.00095786\n",
      "Average Loss: 0.00927853\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #362: Batch  90/90  -- Loss: 0.01099486\n",
      "Average Loss: 0.00888326\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #363: Batch  90/90  -- Loss: 0.00303829\n",
      "Average Loss: 0.0089906\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #364: Batch  90/90  -- Loss: 0.0099527\n",
      "Average Loss: 0.00816044\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #365: Batch  90/90  -- Loss: 0.0014529\n",
      "Average Loss: 0.00776721\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #366: Batch  90/90  -- Loss: 0.00278848\n",
      "Average Loss: 0.00720903\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #367: Batch  90/90  -- Loss: 0.00820991\n",
      "Average Loss: 0.00716309\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #368: Batch  90/90  -- Loss: 0.0267755\n",
      "Average Loss: 0.00789125\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #369: Batch  90/90  -- Loss: 0.00930363\n",
      "Average Loss: 0.00643806\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #370: Batch  90/90  -- Loss: 0.00266693\n",
      "Average Loss: 0.00580169\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #371: Batch  90/90  -- Loss: 0.02012639\n",
      "Average Loss: 0.00592866\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #372: Batch  90/90  -- Loss: 0.00738559\n",
      "Average Loss: 0.0053631\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #373: Batch  90/90  -- Loss: 0.0018199\n",
      "Average Loss: 0.00526658\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #374: Batch  90/90  -- Loss: 0.00014783\n",
      "Average Loss: 0.00465084\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #375: Batch  90/90  -- Loss: 0.00073779\n",
      "Average Loss: 0.00461501\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #376: Batch  90/90  -- Loss: 0.00067329\n",
      "Average Loss: 0.00442667\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #377: Batch  90/90  -- Loss: 0.00219088\n",
      "Average Loss: 0.00414558\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #378: Batch  90/90  -- Loss: 0.00425358\n",
      "Average Loss: 0.00376306\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #379: Batch  90/90  -- Loss: 0.00811695\n",
      "Average Loss: 0.00402846\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #380: Batch  90/90  -- Loss: 0.00108826\n",
      "Average Loss: 0.00351266\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9774\n",
      "Epoch #381: Batch  90/90  -- Loss: 0.00216214\n",
      "Average Loss: 0.00320803\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #382: Batch  90/90  -- Loss: 0.00203978\n",
      "Average Loss: 0.00326696\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #383: Batch  90/90  -- Loss: 0.00294215\n",
      "Average Loss: 0.00299771\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #384: Batch  90/90  -- Loss: 0.00648665\n",
      "Average Loss: 0.00363005\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #385: Batch  90/90  -- Loss: 0.00188547\n",
      "Average Loss: 0.0033199\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #386: Batch  90/90  -- Loss: 0.00147492\n",
      "Average Loss: 0.00264575\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #387: Batch  90/90  -- Loss: 0.00227937\n",
      "Average Loss: 0.00244385\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #388: Batch  90/90  -- Loss: 0.00274268\n",
      "Average Loss: 0.00234495\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #389: Batch  90/90  -- Loss: 0.00037343\n",
      "Average Loss: 0.00216162\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #390: Batch  90/90  -- Loss: 0.00285848\n",
      "Average Loss: 0.00209742\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #391: Batch  90/90  -- Loss: 0.00515598\n",
      "Average Loss: 0.00202426\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #392: Batch  90/90  -- Loss: 0.00104081\n",
      "Average Loss: 0.00191578\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #393: Batch  90/90  -- Loss: 0.00320495\n",
      "Average Loss: 0.00197139\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #394: Batch  90/90  -- Loss: 0.00105645\n",
      "Average Loss: 0.0021085\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #395: Batch  90/90  -- Loss: 0.00196093\n",
      "Average Loss: 0.00182498\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #396: Batch  90/90  -- Loss: 0.00061119\n",
      "Average Loss: 0.00161852\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #397: Batch  90/90  -- Loss: 0.00052641\n",
      "Average Loss: 0.00151159\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #398: Batch  90/90  -- Loss: 0.00101096\n",
      "Average Loss: 0.00143303\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #399: Batch  90/90  -- Loss: 0.00139188\n",
      "Average Loss: 0.00138341\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #400: Batch  90/90  -- Loss: 0.00081967\n",
      "Average Loss: 0.00131447\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #401: Batch  90/90  -- Loss: 0.00117086\n",
      "Average Loss: 0.00127057\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #402: Batch  90/90  -- Loss: 0.00066001\n",
      "Average Loss: 0.0012332\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #403: Batch  90/90  -- Loss: 0.00176388\n",
      "Average Loss: 0.0393712\n",
      "Training F1: 0.9815\n",
      "Test F1: 0.9559\n",
      "Epoch #404: Batch  90/90  -- Loss: 0.01675943\n",
      "Average Loss: 0.0358072\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #405: Batch  90/90  -- Loss: 0.00131617\n",
      "Average Loss: 0.00309385\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #406: Batch  90/90  -- Loss: 0.00077375\n",
      "Average Loss: 0.00235174\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #407: Batch  90/90  -- Loss: 0.00033909\n",
      "Average Loss: 0.00210852\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #408: Batch  90/90  -- Loss: 0.00441829\n",
      "Average Loss: 0.0019224\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #409: Batch  90/90  -- Loss: 0.00191881\n",
      "Average Loss: 0.00177458\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #410: Batch  90/90  -- Loss: 0.00060818\n",
      "Average Loss: 0.00166583\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #411: Batch  90/90  -- Loss: 0.00035372\n",
      "Average Loss: 0.00158268\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #412: Batch  90/90  -- Loss: 0.00099056\n",
      "Average Loss: 0.00150909\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #413: Batch  90/90  -- Loss: 0.00094177\n",
      "Average Loss: 0.00144537\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #414: Batch  90/90  -- Loss: 0.00148846\n",
      "Average Loss: 0.00139549\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #415: Batch  90/90  -- Loss: 0.00176659\n",
      "Average Loss: 0.00135028\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #416: Batch  90/90  -- Loss: 7.3909e-06\n",
      "Average Loss: 0.0012974\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #417: Batch  90/90  -- Loss: 0.00050857\n",
      "Average Loss: 0.00125849\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #418: Batch  90/90  -- Loss: 0.00036858\n",
      "Average Loss: 0.00122283\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #419: Batch  90/90  -- Loss: 0.00046539\n",
      "Average Loss: 0.0011922\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #420: Batch  90/90  -- Loss: 0.00057106\n",
      "Average Loss: 0.00115765\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #421: Batch  90/90  -- Loss: 0.00012959\n",
      "Average Loss: 0.00112106\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #422: Batch  90/90  -- Loss: 0.00011246\n",
      "Average Loss: 0.00109535\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #423: Batch  90/90  -- Loss: 0.00256052\n",
      "Average Loss: 0.00108\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #424: Batch  90/90  -- Loss: 0.00082016\n",
      "Average Loss: 0.00104003\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #425: Batch  90/90  -- Loss: 0.00164698\n",
      "Average Loss: 0.00102214\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #426: Batch  90/90  -- Loss: 8.3744e-06\n",
      "Average Loss: 0.00098882\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #427: Batch  90/90  -- Loss: 0.00232462\n",
      "Average Loss: 0.000975193\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #428: Batch  90/90  -- Loss: 0.00045281\n",
      "Average Loss: 0.000940117\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #429: Batch  90/90  -- Loss: 0.00082025\n",
      "Average Loss: 0.000918796\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #430: Batch  90/90  -- Loss: 0.00055077\n",
      "Average Loss: 0.000893008\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #431: Batch  90/90  -- Loss: 0.00033563\n",
      "Average Loss: 0.000871442\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #432: Batch  90/90  -- Loss: 0.00022471\n",
      "Average Loss: 0.000850701\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #433: Batch  90/90  -- Loss: 0.00050113\n",
      "Average Loss: 0.000829796\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #434: Batch  90/90  -- Loss: 0.00026437\n",
      "Average Loss: 0.000810732\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #435: Batch  90/90  -- Loss: 0.00010132\n",
      "Average Loss: 0.000791651\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #436: Batch  90/90  -- Loss: 0.00049911\n",
      "Average Loss: 0.00076199\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #437: Batch  90/90  -- Loss: 0.00043077\n",
      "Average Loss: 0.000753539\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #438: Batch  90/90  -- Loss: 0.00040615\n",
      "Average Loss: 0.000731613\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #439: Batch  90/90  -- Loss: 0.00141592\n",
      "Average Loss: 0.00071647\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #440: Batch  90/90  -- Loss: 0.00019599\n",
      "Average Loss: 0.000694037\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #441: Batch  90/90  -- Loss: 0.00024263\n",
      "Average Loss: 0.000676124\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #442: Batch  90/90  -- Loss: 0.00170125\n",
      "Average Loss: 0.000663865\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #443: Batch  90/90  -- Loss: 0.00122326\n",
      "Average Loss: 0.000640516\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #444: Batch  90/90  -- Loss: 0.00076925\n",
      "Average Loss: 0.000617683\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #445: Batch  90/90  -- Loss: 0.00147247\n",
      "Average Loss: 0.000608524\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #446: Batch  90/90  -- Loss: 0.00062083\n",
      "Average Loss: 0.000588783\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #447: Batch  90/90  -- Loss: 0.00034918\n",
      "Average Loss: 0.000568602\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #448: Batch  90/90  -- Loss: 0.00036653\n",
      "Average Loss: 0.000551375\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #449: Batch  90/90  -- Loss: 0.00030919\n",
      "Average Loss: 0.000533509\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #450: Batch  90/90  -- Loss: 0.00139559\n",
      "Average Loss: 0.000560352\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #451: Batch  90/90  -- Loss: 0.00041295\n",
      "Average Loss: 0.000504159\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #452: Batch  90/90  -- Loss: 0.00031582\n",
      "Average Loss: 0.000487923\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #453: Batch  90/90  -- Loss: 0.00039533\n",
      "Average Loss: 0.000480431\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #454: Batch  90/90  -- Loss: 0.00023625\n",
      "Average Loss: 0.000458136\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #455: Batch  90/90  -- Loss: 0.00056187\n",
      "Average Loss: 0.00044588\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #456: Batch  90/90  -- Loss: 0.00192437\n",
      "Average Loss: 0.000469979\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #457: Batch  90/90  -- Loss: 0.00049345\n",
      "Average Loss: 0.000452673\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #458: Batch  90/90  -- Loss: 0.00152298\n",
      "Average Loss: 0.000422947\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #459: Batch  90/90  -- Loss: 7.5035e-05\n",
      "Average Loss: 0.00039299\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #460: Batch  90/90  -- Loss: 0.00185969\n",
      "Average Loss: 0.000386763\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #461: Batch  90/90  -- Loss: 0.00059331\n",
      "Average Loss: 0.00036925\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #462: Batch  90/90  -- Loss: 0.00075182\n",
      "Average Loss: 0.000355045\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #463: Batch  90/90  -- Loss: 0.00032276\n",
      "Average Loss: 0.000338627\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #464: Batch  90/90  -- Loss: 0.00024727\n",
      "Average Loss: 0.000331434\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #465: Batch  90/90  -- Loss: 0.00017048\n",
      "Average Loss: 0.000318255\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #466: Batch  90/90  -- Loss: 6.9141e-06\n",
      "Average Loss: 0.000307636\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #467: Batch  90/90  -- Loss: 0.00058947\n",
      "Average Loss: 0.00029997\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #468: Batch  90/90  -- Loss: 0.00015576\n",
      "Average Loss: 0.000287427\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #469: Batch  90/90  -- Loss: 0.00056943\n",
      "Average Loss: 0.00027776\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #470: Batch  90/90  -- Loss: 0.00020045\n",
      "Average Loss: 0.000271096\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #471: Batch  90/90  -- Loss: 0.00021683\n",
      "Average Loss: 0.000259423\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #472: Batch  90/90  -- Loss: 3.457e-065\n",
      "Average Loss: 0.000245876\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #473: Batch  90/90  -- Loss: 0.00021855\n",
      "Average Loss: 0.0002394\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #474: Batch  90/90  -- Loss: 0.00017727\n",
      "Average Loss: 0.000229459\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #475: Batch  90/90  -- Loss: 0.00057748\n",
      "Average Loss: 0.000223779\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #476: Batch  90/90  -- Loss: 5.7007e-05\n",
      "Average Loss: 0.000212431\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #477: Batch  90/90  -- Loss: 8.2421e-05\n",
      "Average Loss: 0.000204228\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #478: Batch  90/90  -- Loss: 0.00014441\n",
      "Average Loss: 0.000197376\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #479: Batch  90/90  -- Loss: 0.00022492\n",
      "Average Loss: 0.000191035\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #480: Batch  90/90  -- Loss: 0.00012703\n",
      "Average Loss: 0.000181055\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #481: Batch  90/90  -- Loss: 5.826e-055\n",
      "Average Loss: 0.00017661\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #482: Batch  90/90  -- Loss: 0.00022032\n",
      "Average Loss: 0.000169447\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #483: Batch  90/90  -- Loss: 6.0198e-05\n",
      "Average Loss: 0.000166543\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #484: Batch  90/90  -- Loss: 8.4038e-05\n",
      "Average Loss: 0.00015677\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #485: Batch  90/90  -- Loss: 0.00013806\n",
      "Average Loss: 0.00014923\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #486: Batch  90/90  -- Loss: 0.00014002\n",
      "Average Loss: 0.000144549\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #487: Batch  90/90  -- Loss: 0.00012839\n",
      "Average Loss: 0.000139112\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #488: Batch  90/90  -- Loss: 0.00019187\n",
      "Average Loss: 0.000133494\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #489: Batch  90/90  -- Loss: 0.00011443\n",
      "Average Loss: 0.000128396\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #490: Batch  90/90  -- Loss: 0.00025536\n",
      "Average Loss: 0.000122462\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #491: Batch  90/90  -- Loss: 0.00019134\n",
      "Average Loss: 0.000120058\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #492: Batch  90/90  -- Loss: 6.0198e-05\n",
      "Average Loss: 0.000114907\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #493: Batch  90/90  -- Loss: 9.1492e-06\n",
      "Average Loss: 0.000108502\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #494: Batch  90/90  -- Loss: 2.8638e-05\n",
      "Average Loss: 0.000102724\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #495: Batch  90/90  -- Loss: 3.8861e-05\n",
      "Average Loss: 9.76e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #496: Batch  90/90  -- Loss: 5.2986e-05\n",
      "Average Loss: 9.69174e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9925\n",
      "Epoch #497: Batch  90/90  -- Loss: 5.239e-055\n",
      "Average Loss: 9.57881e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #498: Batch  90/90  -- Loss: 5.8021e-05\n",
      "Average Loss: 8.77893e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #499: Batch  90/90  -- Loss: 1.4633e-05\n",
      "Average Loss: 8.25765e-05\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n",
      "Epoch #500: Batch  90/90  -- Loss: 2.6404e-05\n",
      "Average Loss: 0.000100202\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyTorchBaseline(\n",
       "  (lstm): LSTM(2, 16, bidirectional=True)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(model, train_x, train_y, test_x, test_y, epochs=500, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "our = LSTMSeqLabel(input_dim, hidden_dim, output_dim, bidirectional=True, layers=layers).to(device)\n",
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "==================\n",
      "# of parameters: 2465\n",
      "lstm.model.0.weights     : torch.Size([18, 64])\n",
      "lstm.model.0.bias        : torch.Size([64])\n",
      "lstm.model_rev.0.weights : torch.Size([18, 64])\n",
      "lstm.model_rev.0.bias    : torch.Size([64])\n",
      "fc.weight                : torch.Size([1, 32])\n",
      "fc.bias                  : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\\n{}\".format(\"=\" * len(\"Our implementation\")))\n",
    "print(\"# of parameters: {}\".format(our.count_parameters()))\n",
    "for name, param in our.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch implementation\n",
      "======================\n",
      "# of parameters: 2593\n",
      "lstm.weight_ih_l0        : torch.Size([64, 2])\n",
      "lstm.weight_hh_l0        : torch.Size([64, 16])\n",
      "lstm.bias_ih_l0          : torch.Size([64])\n",
      "lstm.bias_hh_l0          : torch.Size([64])\n",
      "lstm.weight_ih_l0_reverse: torch.Size([64, 2])\n",
      "lstm.weight_hh_l0_reverse: torch.Size([64, 16])\n",
      "lstm.bias_ih_l0_reverse  : torch.Size([64])\n",
      "lstm.bias_hh_l0_reverse  : torch.Size([64])\n",
      "fc.weight                : torch.Size([1, 32])\n",
      "fc.bias                  : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch implementation\\n{}\".format(\"=\" * len(\"PyTorch implementation\")))\n",
    "print(\"# of parameters: {}\".format(pytorch.count_parameters()))\n",
    "for name, param in pytorch.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses $Wh + b_h + Wx + b_x$ whereas we are using $Wx' + b$, where $x'$ is $h, x$ concatenated. Therefore PyTorch has an extra set of biases for each direction.\n",
    "\n",
    "For one direction - 64 <br>\n",
    "For reverse direction - 64 <br>\n",
    "\n",
    "Our model has $6978$ parameters while the PyTorch model has $6978 + 64 + 64 + 128 = 7234$ parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
