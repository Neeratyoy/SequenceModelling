{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse sequence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_size):\n",
    "    for j in range(sequence_length):\n",
    "        target[j, i, train_x[j, i,].item()] = 1\n",
    "# target = torch.flip(target, [0]).long()\n",
    "train_x = train_x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "train_size = 100\n",
    "train_x = torch.randint(0, 2, (sequence_length, train_size, input_dim))\n",
    "target = torch.zeros(sequence_length, train_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from lstm import LSTM\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "class LSTMSeq2SeqDifferent(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-many-different)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers=1,\n",
    "                 bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.encoder = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = LSTM(input_dim=output_dim, hidden_dim=2 * hidden_dim, layers=layers,\n",
    "                                bidirectional=False, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = LSTM(input_dim=output_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                                bidirectional=False, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, target, hidden_state, cell_state, teacher_forcing=0.5):\n",
    "        # encoding\n",
    "        _, (hidden_state, cell_state) = self.encoder(x, hidden_state, cell_state)\n",
    "        batch_size = x.shape[1]\n",
    "        timesteps = x.shape[0]\n",
    "        x = torch.zeros(1, batch_size, self.output_dim).to(device)\n",
    "        output = torch.tensor([]).to(device)\n",
    "        if self.bidirectional:\n",
    "            # concatenating hidden states from two directions\n",
    "            hidden_state = torch.cat((hidden_state[:,0,:,:], hidden_state[:,1,:,:]), dim=2)\n",
    "            cell_state = torch.cat((cell_state[:,0,:,:], cell_state[:,1,:,:]), dim=2)\n",
    "#         else:\n",
    "#             hidden_state = hidden_state[-1].unsqueeze(0)\n",
    "#             cell_state = cell_state[-1].unsqueeze(0)\n",
    "        # decoding\n",
    "        for t in range(timesteps):\n",
    "            # taking hidden state from last layer\n",
    "#             if self.bidirectional:\n",
    "#                 hidden_state = hidden_state[-1,:,:].unsqueeze(0)\n",
    "#                 cell_state = cell_state[-1,:,:].unsqueeze(0)\n",
    "#             else:\n",
    "            hidden_state = hidden_state[-1].unsqueeze(0)\n",
    "            cell_state = cell_state[-1].unsqueeze(0)            \n",
    "            x, (hidden_state, cell_state) = self.decoder(x, hidden_state, cell_state)            \n",
    "            x = self.softmax(self.fc(x))\n",
    "            output = torch.cat((output, x), dim=0)\n",
    "            choice = random.random() \n",
    "            if choice < teacher_forcing:\n",
    "                x = target[t].float().to(device)\n",
    "                x = x.unsqueeze(0)\n",
    "            else:\n",
    "                # converting x to a one-hot encoding\n",
    "                x = torch.zeros(x.shape).to(device).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1594370"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "batch_size = 8\n",
    "layers = 1\n",
    "bidirectional = True\n",
    "layernorm = True\n",
    "device = 'cuda'\n",
    "\n",
    "model = LSTMSeq2SeqDifferent(input_dim=input_dim, hidden_dim=hidden_dim, \n",
    "                             output_dim=output_dim, layers=layers,\n",
    "                             bidirectional=bidirectional, layernorm=layernorm).to(device)\n",
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_x.shape, target.shape)\n",
    "# print(train_x[:,0,:])\n",
    "# print(target[:,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "test_size = 50\n",
    "test_x = torch.randint(0, 2, (sequence_length, test_size, input_dim))\n",
    "test_target = torch.zeros(sequence_length, test_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_size):\n",
    "    for j in range(sequence_length):\n",
    "        test_target[j, i, test_x[j, i,].item()] = 1\n",
    "# target = torch.flip(target, [0]).long()\n",
    "test_x = test_x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_x.shape, test_target.shape)\n",
    "# print(test_x[:,0,:])\n",
    "# print(test_target[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flattening softmax output sample for preds in loss_fn\n",
    "# print(target.view(-1, target.shape[-1])[:20])\n",
    "## flattening softmax output sample for gt in loss_fn\n",
    "# print(torch.argmax(target, 2).view(-1)[:20])\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# loss = loss_fn(target.view(-1, target.shape[-1])[:20], torch.argmax(target, 2).view(-1)[:20])\n",
    "# loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = torch.zeros(1, batch_size, hidden_dim).to(device)\n",
    "# c = torch.zeros(1, batch_size, hidden_dim).to(device)\n",
    "# o = model(train_x[:,:batch_size,:].to(device), target[:,:batch_size,:].to(device), h, c, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40])\n",
      "torch.Size([40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 1061,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.argmax(o.view(-1, o.shape[-1]), 1).shape)\n",
    "print(torch.argmax(target[:,:batch_size,:], 2).view(-1).shape)\n",
    "f1_score(torch.argmax(o.view(-1, o.shape[-1]), 1).cpu(), \n",
    "         torch.argmax(target[:,:batch_size,:], 2).view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# print(target[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 256]), torch.Size([1, 8, 256]))"
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = torch.zeros(1, batch_size, hidden_dim).to(device)\n",
    "cell_state = torch.zeros(1, batch_size, hidden_dim).to(device)\n",
    "hidden_state.shape, cell_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, batch_size, epochs, loss_fn, optimizer, \n",
    "          teacher_forcing=0.5, test_x=None, test_y=None):\n",
    "    model.train()\n",
    "    stats = {'train':[], 'valid':[]}\n",
    "    train_size = x.shape[1]\n",
    "    for i in range(1, epochs + 1):\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        x = x[:, ordering, :]\n",
    "        y = y[:, ordering, :]\n",
    "        for j in range(int(train_size/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch == 0:\n",
    "                continue\n",
    "            hidden_state = torch.zeros(1, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(1, batch, hidden_dim).to(device)\n",
    "            o = model(x[:,start:end,:], y[:,start:end,:], hidden_state, cell_state, teacher_forcing)\n",
    "            loss = loss_fn(o.view(-1, o.shape[-1]), torch.argmax(y[:,start:end,:], 2).view(-1))\n",
    "            loss_tracker.append(loss.item())\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size) + 1, \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        f1_train = evaluate(model, x, y)\n",
    "        print(\"Training Acc.: {:3.4}\".format(f1_train))\n",
    "        if test_x is not None and test_y is not None:\n",
    "            f1_test = evaluate(model, test_x, test_y)\n",
    "            print(\"Test Acc.: {:3.4}\".format(f1_test))\n",
    "        print(\"=\"*42)\n",
    "    return model\n",
    "        \n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        hidden_state = torch.zeros(1, batch, hidden_dim).to(device)\n",
    "        cell_state = torch.zeros(1, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], y[:,start:end,:], hidden_state, cell_state, teacher_forcing=0)\n",
    "        preds.extend(torch.argmax(o.view(-1, o.shape[-1]), 1).cpu().numpy())\n",
    "        labels.extend(torch.argmax(y[:,start:end,:], 2).view(-1).cpu().numpy())\n",
    "    return accuracy_score(labels, preds)\n",
    "#         print(torch.argmax(o.view(-1, o.shape[-1]), 1).cpu().numpy().shape, \n",
    "#               torch.argmax(y[:,start:end,:], 2).view(-1).cpu().numpy().shape, len(preds), len(labels))\n",
    "#     from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "#     return confusion_matrix(labels, preds), accuracy_score(labels, preds), \\\n",
    "#            precision_score(labels, preds), recall_score(labels, preds)\n",
    "# #     return f1_score(labels, preds), labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  13/13  -- Loss: 0.68828\n",
      "Average Loss: 0.69258\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #2  : Batch  13/13  -- Loss: 0.69329\n",
      "Average Loss: 0.692773\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #3  : Batch  13/13  -- Loss: 0.69329\n",
      "Average Loss: 0.692773\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #4  : Batch  13/13  -- Loss: 0.69329\n",
      "Average Loss: 0.692773\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #5  : Batch  13/13  -- Loss: 0.68828\n",
      "Average Loss: 0.69258\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #6  : Batch  13/13  -- Loss: 0.69329\n",
      "Average Loss: 0.692773\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #7  : Batch  13/13  -- Loss: 0.69162\n",
      "Average Loss: 0.692708\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #8  : Batch  13/13  -- Loss: 0.69829\n",
      "Average Loss: 0.692965\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #9  : Batch  13/13  -- Loss: 0.68661\n",
      "Average Loss: 0.692516\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n",
      "Epoch #10 : Batch  13/13  -- Loss: 0.68995\n",
      "Average Loss: 0.692644\n",
      "Training Acc.: 0.516\n",
      "Test Acc.: 0.484\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_x.to(device), target.to(device), batch_size, 10, loss_fn, optimizer,\n",
    "              test_x=test_x.to(device), test_y=test_target.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.484"
      ]
     },
     "execution_count": 1106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_x.to(device), test_target.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchBaseline(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-many-different)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers=1,\n",
    "                 bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                         bidirectional=bidirectional) #, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.LSTM(input_size=output_dim, hidden_size=2 * hidden_dim, num_layers=layers,\n",
    "                                bidirectional=False) #, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(input_size=output_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                                bidirectional=False) #, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, x, target, hidden_state, cell_state, teacher_forcing=0.5):\n",
    "        # encoding\n",
    "        _, (hidden_state, cell_state) = self.encoder(x, (hidden_state, cell_state))\n",
    "        batch_size = x.shape[1]\n",
    "        timesteps = x.shape[0]\n",
    "        x = torch.zeros(1, batch_size, self.output_dim).to(device)\n",
    "        output = torch.tensor([]).to(device)\n",
    "        if self.bidirectional:\n",
    "            # concatenating hidden states from two directions\n",
    "            hidden_state = torch.cat((hidden_state[:self.layers,:,:], \n",
    "                                      hidden_state[self.layers:,:,:]), dim=2).to(device)\n",
    "            cell_state = torch.cat((cell_state[:self.layers,:,:], \n",
    "                                    cell_state[self.layers:,:,:]), dim=2).to(device)\n",
    "#         else:\n",
    "#             hidden_state = hidden_state[-1].unsqueeze(0)\n",
    "#             cell_state = cell_state[-1].unsqueeze(0)\n",
    "        # decoding\n",
    "        for t in range(timesteps):          \n",
    "            x, (hidden_state, cell_state) = self.decoder(x, (hidden_state, cell_state))            \n",
    "            x = self.softmax(self.fc(x))\n",
    "            output = torch.cat((output, x), dim=0)\n",
    "            choice = random.random() \n",
    "            if choice < teacher_forcing:\n",
    "                x = target[t].float().to(device)\n",
    "                x = x.unsqueeze(0)\n",
    "            else:\n",
    "                # converting x to a one-hot encoding\n",
    "                x = torch.zeros(x.shape).to(device).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 1\n",
    "bidirectional = True\n",
    "\n",
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, layers, bidirectional).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch(model, x, y, batch_size, epochs, loss_fn, optimizer, \n",
    "                  teacher_forcing=0.5, test_x=None, test_y=None):\n",
    "    model.train()\n",
    "    stats = {'train':[], 'valid':[]}\n",
    "    train_size = x.shape[1]\n",
    "    for i in range(1, epochs + 1):\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        x = x[:, ordering, :]\n",
    "        y = y[:, ordering, :]\n",
    "        for j in range(int(train_size/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch == 0:\n",
    "                continue\n",
    "            if bidirectional:\n",
    "                hidden_state = torch.zeros(2*layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2*layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "#             print(hidden_state.shape, cell_state.shape)\n",
    "            o = model(x[:,start:end,:], y[:,start:end,:], hidden_state, cell_state, teacher_forcing)\n",
    "            loss = loss_fn(o.view(-1, o.shape[-1]), torch.argmax(y[:,start:end,:], 2).view(-1))\n",
    "            loss_tracker.append(loss.item())\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size) + 1, loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        f1_train = evaluate_pytorch(model, x, y)\n",
    "        print(\"Training Acc.: {:3.4}\".format(f1_train))\n",
    "        if test_x is not None and test_y is not None:\n",
    "            f1_test = evaluate_pytorch(model, test_x, test_y)\n",
    "            print(\"Test Acc.: {:3.4}\".format(f1_test))\n",
    "        print(\"=\"*42)\n",
    "        print(\"=\"*42)\n",
    "    return model\n",
    "\n",
    "        \n",
    "def evaluate_pytorch(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if bidirectional:\n",
    "            hidden_state = torch.zeros(2*layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2*layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], y[:,start:end,:], hidden_state, cell_state, teacher_forcing=0)\n",
    "        preds.extend(torch.argmax(o.view(-1, o.shape[-1]), 1).cpu().numpy())\n",
    "        labels.extend(torch.argmax(y[:,start:end,:], 2).view(-1).cpu().numpy())\n",
    "    return accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  13/13  -- Loss: 0.69443\n",
      "Average Loss: 0.693386\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #2  : Batch  13/13  -- Loss: 0.69331\n",
      "Average Loss: 0.693343\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #3  : Batch  13/13  -- Loss: 0.69357\n",
      "Average Loss: 0.693359\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #4  : Batch  13/13  -- Loss: 0.69306\n",
      "Average Loss: 0.693332\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #5  : Batch  13/13  -- Loss: 0.69342\n",
      "Average Loss: 0.69336\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #6  : Batch  13/13  -- Loss: 0.69287\n",
      "Average Loss: 0.693332\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #7  : Batch  13/13  -- Loss: 0.69411\n",
      "Average Loss: 0.693389\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #8  : Batch  13/13  -- Loss: 0.69361\n",
      "Average Loss: 0.693351\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #9  : Batch  13/13  -- Loss: 0.69341\n",
      "Average Loss: 0.69335\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n",
      "Epoch #10 : Batch  13/13  -- Loss: 0.69238\n",
      "Average Loss: 0.69331\n",
      "Training Acc.: 0.43\n",
      "Test Acc.: 0.416\n",
      "==========================================\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "pytorch = train_pytorch(pytorch, train_x.to(device), target.to(device), batch_size, 10, loss_fn, optimizer,\n",
    "                        test_x=test_x.to(device), test_y=test_target.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
