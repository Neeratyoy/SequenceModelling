{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:24.959291Z",
     "start_time": "2019-07-15T15:08:24.948513Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sBcy8b8FfO7s"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:25.195815Z",
     "start_time": "2019-07-15T15:08:25.182154Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sXdUbR6wfO7z"
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "learning_rate = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKOC3ygVfO73"
   },
   "source": [
    "# Double-copy task dataset\n",
    "\n",
    "The entire dataset comprises of the binary representation of all numbers uptil a range defined. The binary sequence from left to right (most significant to least significant) is the input. The target is just the reverse sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:26.349925Z",
     "start_time": "2019-07-15T15:08:26.320901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3J6ueqfdfO74",
    "outputId": "a5508cad-15ab-41c2-b89c-3cfdc3f5ef3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 12)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Generating data\n",
    "state_size = 12\n",
    "data_x = []\n",
    "for i in range(pow(2, state_size)):\n",
    "    data_x.append([int(x) for x in list(np.binary_repr(i, width=state_size))])\n",
    "data_x = np.array(data_x)\n",
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:28.674217Z",
     "start_time": "2019-07-15T15:08:28.660222Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "660POB_afO7_",
    "outputId": "59eec18a-a3e1-458c-f1be-d1d99a90e11f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 4096, 2]), torch.Size([12, 4096, 2]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping for tensors\n",
    "data_x = np.transpose(data_x).reshape(state_size, pow(2, state_size), 1)\n",
    "data_x = torch.from_numpy(data_x).float()\n",
    "data_x = torch.zeros(data_x.shape[0], data_x.shape[1], 2).scatter_(2, data_x.long(), 1)\n",
    "data_y = data_x.clone()\n",
    "data_x.shape, data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:32.460676Z",
     "start_time": "2019-07-15T15:08:32.451859Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OWSEkkPDfO8D"
   },
   "outputs": [],
   "source": [
    "# Doubling the target sequence\n",
    "data_y = torch.cat((data_y, data_y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:32.822236Z",
     "start_time": "2019-07-15T15:08:32.812280Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GRKZM2s4fO8G"
   },
   "outputs": [],
   "source": [
    "# add a start-of-sequence tag (1,1)\n",
    "START = torch.ones(data_x.shape[-1]).long().to(device)\n",
    "data_x = torch.nn.functional.pad(data_x, (0,0,0,0,1,0), 'constant', 1)\n",
    "data_y = torch.nn.functional.pad(data_y, (0,0,0,0,1,0), 'constant', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:08:33.272474Z",
     "start_time": "2019-07-15T15:08:33.255192Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1HeboSbSfO8J",
    "outputId": "b3c77abb-764b-40f8-ae7d-9fc2c1c8d44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 3072, 2]) torch.Size([25, 3072, 2]) torch.Size([13, 1024, 2]) torch.Size([25, 1024, 2])\n"
     ]
    }
   ],
   "source": [
    "# Creating training and test sets\n",
    "train_size = 0.75\n",
    "ordering = torch.randperm(pow(2, state_size))\n",
    "data_x = data_x[:, ordering, :]\n",
    "data_y = data_y[:, ordering, :]\n",
    "train_x = data_x[:,:int(train_size * len(ordering)),:]\n",
    "train_y = data_y[:,:int(train_size * len(ordering)),:]\n",
    "test_x = data_x[:,int(train_size * len(ordering)):,:]\n",
    "test_y = data_y[:,int(train_size * len(ordering)):,:]\n",
    "\n",
    "# Creating training and validation sets\n",
    "## TODO\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Saf7gxgfO8N"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxY9-DGFfO8O"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:21:05.976660Z",
     "start_time": "2019-07-15T14:21:05.973383Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HnOXsr8-fO8P"
   },
   "outputs": [],
   "source": [
    "# Input dim\n",
    "input_dim = 2\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 16\n",
    "# Number of output nodes\n",
    "output_dim = 2\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = True\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:21:08.255028Z",
     "start_time": "2019-07-15T14:21:08.212623Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "brXeEoqTfO8S"
   },
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer, teacher_forcing=0.5):\n",
    "    train_size = train_x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if train_x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    layers = model.layers\n",
    "    hidden_dim = model.hidden_dim\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[:,ordering,:]\n",
    "        \n",
    "        epoch_time = time.time()\n",
    "        \n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "                \n",
    "            st = time.time()\n",
    "            \n",
    "            if model.bidirectional:\n",
    "                hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            o = model(train_x[:,start:end,:], train_y[:,start:end,:], hidden_state, \n",
    "                      cell_state, teacher_forcing)\n",
    "            gt = torch.argmax(train_y[:,start:end,:], 2, keepdim=True).view(-1)\n",
    "            loss = loss_fn(o.view(-1, 2), gt)\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}  time: {:2.5}\".format(i, j+1, int(train_size/batch_size), \n",
    "                                        loss_tracker[-1], (time.time()-st)), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate(model, train_x, train_y)\n",
    "        f1_test = evaluate(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Total time: {:2.5}\".format(time.time() - epoch_time))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    layers = model.layers\n",
    "    hidden_dim = model.hidden_dim\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if model.bidirectional:\n",
    "            hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], y[:,start:end,:], hidden_state, cell_state, teacher_forcing=0)\n",
    "        pred = torch.argmax(o, 2, keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        label = torch.argmax(y[:,start:end,:], 2, \n",
    "                             keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        labels.extend(label)\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsF2gYnpfO8Y"
   },
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:21:11.852830Z",
     "start_time": "2019-07-15T14:21:11.820654Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3dBJkN4SfO8Z"
   },
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "\n",
    "class LSTMSeq2SeqDifferent(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-many-different)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers=1,\n",
    "                 bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.encoder = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = LSTM(input_dim=output_dim, hidden_dim=2 * hidden_dim, layers=layers,\n",
    "                                bidirectional=False, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = LSTM(input_dim=output_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                                bidirectional=False, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, target, hidden_state, cell_state, teacher_forcing=0.5):\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        # encoding\n",
    "        _, (hidden_state, cell_state) = self.encoder(x, hidden_state, cell_state)\n",
    "        batch_size = x.shape[1]\n",
    "        timesteps = target.shape[0]\n",
    "        x = torch.zeros(1, batch_size, self.output_dim).to(device)\n",
    "        output = torch.tensor([]).to(device)\n",
    "        if self.bidirectional:\n",
    "            # concatenating hidden states from two directions\n",
    "            hidden_state = torch.cat((hidden_state[:self.layers,:,:], \n",
    "                                      hidden_state[self.layers:,:,:]), dim=2)\n",
    "            cell_state = torch.cat((cell_state[:self.layers,:,:], \n",
    "                                    cell_state[self.layers:,:,:]), dim=2)\n",
    "        # decoding\n",
    "        for t in range(timesteps):           \n",
    "            x, (hidden_state, cell_state) = self.decoder(x, hidden_state, cell_state)            \n",
    "            x = self.softmax(self.fc(x))\n",
    "            output = torch.cat((output, x), dim=0)\n",
    "            choice = random.random() \n",
    "            if choice < teacher_forcing:\n",
    "                x = target[t].float().to(device)\n",
    "                x = x.unsqueeze(0)\n",
    "            else:\n",
    "                # converting x to a one-hot encoding\n",
    "                x = torch.zeros(x.shape).to(device).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:21:13.769830Z",
     "start_time": "2019-07-15T14:21:13.754866Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UVbkHXdyfO8c",
    "outputId": "ce0ecfc1-7be0-491a-a22a-c065a5fbcee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6978\n"
     ]
    }
   ],
   "source": [
    "our = LSTMSeq2SeqDifferent(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(our.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(our.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T14:24:00.896170Z",
     "start_time": "2019-07-15T14:21:15.877307Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SO3y5soufO8g",
    "outputId": "b15771fa-25ba-4f67-c7c2-8ce6b200d5f9"
   },
   "outputs": [],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "our = train(our, train_x, train_y, test_x, test_y, epochs=30, loss_fn=loss_fn, optimizer=optimizer, teacher_forcing=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ia91yC1gfO8k"
   },
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvsFTqHufO8l"
   },
   "outputs": [],
   "source": [
    "class PyTorchBaseline(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-many-different)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers=1,\n",
    "                 bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                         bidirectional=bidirectional) #, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.LSTM(input_size=output_dim, hidden_size=2 * hidden_dim, num_layers=layers,\n",
    "                                bidirectional=False) #, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(input_size=output_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                                bidirectional=False) #, layernorm=layernorm)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, target, hidden_state, cell_state, teacher_forcing=0.5):\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        # encoding\n",
    "        _, (hidden_state, cell_state) = self.encoder(x, (hidden_state, cell_state))\n",
    "        batch_size = x.shape[1]\n",
    "        timesteps = target.shape[0]\n",
    "        x = torch.zeros(1, batch_size, self.output_dim).to(device)\n",
    "        output = torch.tensor([]).to(device)\n",
    "        if self.bidirectional:\n",
    "            # concatenating hidden states from two directions\n",
    "            hidden_state = torch.cat((hidden_state[:self.layers,:,:], \n",
    "                                      hidden_state[self.layers:,:,:]), dim=2)\n",
    "            cell_state = torch.cat((cell_state[:self.layers,:,:], \n",
    "                                    cell_state[self.layers:,:,:]), dim=2)\n",
    "        # decoding\n",
    "        for t in range(timesteps):           \n",
    "            x, (hidden_state, cell_state) = self.decoder(x, (hidden_state, cell_state))\n",
    "            x = self.softmax(self.fc(x))\n",
    "            output = torch.cat((output, x), dim=0)\n",
    "            choice = random.random() \n",
    "            if choice < teacher_forcing:\n",
    "                x = target[t].float().to(device)\n",
    "                x = x.unsqueeze(0)\n",
    "            else:\n",
    "                # converting x to a one-hot encoding\n",
    "                x = torch.zeros(x.shape).to(device).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nZMZwGLtfO8q",
    "outputId": "f5990689-7d2e-4fb7-f44f-0bccb6773f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7234\n"
     ]
    }
   ],
   "source": [
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, layers, bidirectional).to(device)\n",
    "print(pytorch.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pytorch.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4FVColEKfO8v",
    "outputId": "06243534-b73f-45cf-c53c-77e77326898c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.64964\n",
      "Average Loss: 0.672174\n",
      "Training F1: 0.6418\n",
      "Test F1: 0.6348\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.62229\n",
      "Average Loss: 0.62696\n",
      "Training F1: 0.6057\n",
      "Test F1: 0.5931\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.61447\n",
      "Average Loss: 0.592932\n",
      "Training F1: 0.6805\n",
      "Test F1: 0.6745\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.57014\n",
      "Average Loss: 0.568732\n",
      "Training F1: 0.6969\n",
      "Test F1: 0.6871\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.57319\n",
      "Average Loss: 0.555202\n",
      "Training F1: 0.7054\n",
      "Test F1: 0.699\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.55299\n",
      "Average Loss: 0.547974\n",
      "Training F1: 0.6893\n",
      "Test F1: 0.6802\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.51903\n",
      "Average Loss: 0.542967\n",
      "Training F1: 0.6951\n",
      "Test F1: 0.6885\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.53368\n",
      "Average Loss: 0.534724\n",
      "Training F1: 0.7279\n",
      "Test F1: 0.7192\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.50965\n",
      "Average Loss: 0.528096\n",
      "Training F1: 0.7049\n",
      "Test F1: 0.6949\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.51742\n",
      "Average Loss: 0.523615\n",
      "Training F1: 0.7273\n",
      "Test F1: 0.7285\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.51311\n",
      "Average Loss: 0.512752\n",
      "Training F1: 0.7226\n",
      "Test F1: 0.711\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.54437\n",
      "Average Loss: 0.503418\n",
      "Training F1: 0.755\n",
      "Test F1: 0.7453\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.50806\n",
      "Average Loss: 0.489452\n",
      "Training F1: 0.7582\n",
      "Test F1: 0.7494\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.43372\n",
      "Average Loss: 0.474878\n",
      "Training F1: 0.7887\n",
      "Test F1: 0.7772\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.48927\n",
      "Average Loss: 0.460922\n",
      "Training F1: 0.7987\n",
      "Test F1: 0.7926\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.4333\n",
      "Average Loss: 0.445958\n",
      "Training F1: 0.815\n",
      "Test F1: 0.8028\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.33606\n",
      "Average Loss: 0.397277\n",
      "Training F1: 0.9218\n",
      "Test F1: 0.9186\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.34148\n",
      "Average Loss: 0.358245\n",
      "Training F1: 0.9519\n",
      "Test F1: 0.9483\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.32483\n",
      "Average Loss: 0.339771\n",
      "Training F1: 0.9887\n",
      "Test F1: 0.9889\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.32129\n",
      "Average Loss: 0.333388\n",
      "Training F1: 0.9602\n",
      "Test F1: 0.9531\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.31773\n",
      "Average Loss: 0.325147\n",
      "Training F1: 0.9948\n",
      "Test F1: 0.9937\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.34756\n",
      "Average Loss: 0.324679\n",
      "Training F1: 0.9948\n",
      "Test F1: 0.9951\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.31658\n",
      "Average Loss: 0.325268\n",
      "Training F1: 0.9993\n",
      "Test F1: 0.9994\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.31579\n",
      "Average Loss: 0.318734\n",
      "Training F1: 0.999\n",
      "Test F1: 0.9978\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.31808\n",
      "Average Loss: 0.323279\n",
      "Training F1: 0.9982\n",
      "Test F1: 0.9983\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.31451\n",
      "Average Loss: 0.316476\n",
      "Training F1: 0.9997\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.31679\n",
      "Average Loss: 0.31868\n",
      "Training F1: 0.999\n",
      "Test F1: 0.9984\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.31952\n",
      "Average Loss: 0.315796\n",
      "Training F1: 0.9637\n",
      "Test F1: 0.9625\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.34978\n",
      "Average Loss: 0.325783\n",
      "Training F1: 0.9595\n",
      "Test F1: 0.9553\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.31407\n",
      "Average Loss: 0.322816\n",
      "Training F1: 0.9997\n",
      "Test F1: 0.9993\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "pytorch = train(pytorch, train_x, train_y, test_x, test_y, epochs=30, loss_fn=loss_fn, optimizer=optimizer, teacher_forcing=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaxgR0nifO85"
   },
   "outputs": [],
   "source": [
    "print(\"Our implementation\\n{}\".format(\"=\" * len(\"Our implementation\")))\n",
    "print(\"# of parameters: {}\".format(our.count_parameters()))\n",
    "for name, param in our.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "xxXCyUh6fO9C",
    "outputId": "e2a3e959-553f-47ef-f662-0956ab277889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch implementation\n",
      "======================\n",
      "# of parameters: 7234\n",
      "encoder.weight_ih_l0          : torch.Size([64, 2])\n",
      "encoder.weight_hh_l0          : torch.Size([64, 16])\n",
      "encoder.bias_ih_l0            : torch.Size([64])\n",
      "encoder.bias_hh_l0            : torch.Size([64])\n",
      "encoder.weight_ih_l0_reverse  : torch.Size([64, 2])\n",
      "encoder.weight_hh_l0_reverse  : torch.Size([64, 16])\n",
      "encoder.bias_ih_l0_reverse    : torch.Size([64])\n",
      "encoder.bias_hh_l0_reverse    : torch.Size([64])\n",
      "decoder.weight_ih_l0          : torch.Size([128, 2])\n",
      "decoder.weight_hh_l0          : torch.Size([128, 32])\n",
      "decoder.bias_ih_l0            : torch.Size([128])\n",
      "decoder.bias_hh_l0            : torch.Size([128])\n",
      "fc.weight                     : torch.Size([2, 32])\n",
      "fc.bias                       : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch implementation\\n{}\".format(\"=\" * len(\"PyTorch implementation\")))\n",
    "print(\"# of parameters: {}\".format(pytorch.count_parameters()))\n",
    "for name, param in pytorch.named_parameters():\n",
    "    print(\"{:<30}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Xu0c56ofO9I"
   },
   "source": [
    "PyTorch uses $Wh + b_h + Wx + b_x$ whereas we are using $Wx' + b$, where $x'$ is $h, x$ concatenated. Therefore PyTorch has an extra set of biases for each direction for the encoder and also for the decoder.\n",
    "\n",
    "For one direction - 64 <br>\n",
    "For reverse direction - 64 <br>\n",
    "For the decoder - 128 <br>\n",
    "\n",
    "Our model has $6978$ parameters while the PyTorch model has $6978 + 64 + 64 + 128 = 7234$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHl5Rdn_fO9K"
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "FEANP5Dqgfft"
   },
   "source": [
    "## harvard NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "5AQWGif_gkyy"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "## Data stuff\n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "\n",
    "      \n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(2, V, size=(batch, 10)))\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data.repeat(1, 2), requires_grad=False)\n",
    "        src = torch.nn.functional.pad(src, (1,0), 'constant', 1)\n",
    "        tgt = torch.nn.functional.pad(tgt, (1,0), 'constant', 1)\n",
    "        yield Batch(src, tgt, 0)\n",
    "        \n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "        \n",
    "        \n",
    "## Optimizer\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "  \n",
    "\n",
    "## Loss function\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "      \n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, criterion, opt=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm, plot_grad=None):\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        # model as parameters for plotting gradient\n",
    "        if plot_grad:\n",
    "            plot_grad_flow(plot_grad.named_parameters())\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss * norm\n",
    "\n",
    "      \n",
    "## Training loop\n",
    "def run_epoch(data_iter, model, loss_compute, plot_grad=False):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    plot_grad = model if plot_grad else None\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens, plot_grad)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                (i, loss.item() / float(batch.ntokens), tokens.item() / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Pzw33jaWgrOg",
    "outputId": "6d130695-611e-438b-f60a-2017f7b5c775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 9, 5, 5, 9, 8, 4, 7, 2, 6])\n",
      "tensor([1, 2, 9, 5, 5, 9, 8, 4, 7, 2, 6, 2, 9, 5, 5, 9, 8, 4, 7, 2])\n",
      "tensor([2, 9, 5, 5, 9, 8, 4, 7, 2, 6, 2, 9, 5, 5, 9, 8, 4, 7, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "V = 10\n",
    "b = batch = next(data_gen(V, 30, 20))\n",
    "print(b.src[0])\n",
    "print(b.trg[0])\n",
    "print(b.trg_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "XhD07i1EY7Em"
   },
   "outputs": [],
   "source": [
    "import transformer_baseline as tb\n",
    "\n",
    "## Model\n",
    "class TransformerBaselineNLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=True, pad=tb.Constants.PAD):\n",
    "        super().__init__()\n",
    "        self.name = 'transformer'\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.encoder = tb.Models.Encoder(\n",
    "            n_src_vocab=in_dim, len_max_seq=max_len,\n",
    "            d_word_vec=model_dim, d_model=model_dim, d_inner=ff_dim,\n",
    "            n_layers=N, n_head=heads, d_k=key_dim, d_v=value_dim,\n",
    "            dropout=0.0, embedding='embed')\n",
    "        \n",
    "        self.decoder = tb.Models.Decoder(\n",
    "            n_tgt_vocab=out_dim, len_max_seq=max_len,\n",
    "            d_word_vec=model_dim, d_model=model_dim, d_inner=ff_dim,\n",
    "            n_layers=N, n_head=heads, d_k=key_dim, d_v=value_dim,\n",
    "            dropout=0.0, embedding='embed')\n",
    "        \n",
    "        self.fc = nn.Linear(model_dim, out_dim, bias=False)\n",
    "        \n",
    "        # This was important from their code. \n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "            t = t.transpose(0,1)\n",
    "        \n",
    "        # encoder requires source sequence & positions of each \n",
    "        x_pos = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0], 1)\n",
    "        t_pos = torch.arange(t.shape[1]).unsqueeze(0).repeat(t.shape[0], 1)\n",
    "        \n",
    "        # -- Prepare masks\n",
    "        # encoder\n",
    "        e_slf_attn_mask = tb.Models.get_attn_key_pad_mask(seq_k=x, seq_q=x, pad=self.pad)\n",
    "        e_non_pad_mask = torch.ones(x.shape[0], x.shape[1], 1)\n",
    "        e_non_pad_mask = tb.Models.get_non_pad_mask(x, pad=self.pad)\n",
    "        \n",
    "        # decoder\n",
    "        d_non_pad_mask = tb.Models.get_non_pad_mask(t)\n",
    "        slf_attn_mask_subseq = tb.Models.get_subsequent_mask(t)\n",
    "        slf_attn_mask_keypad = tb.Models.get_attn_key_pad_mask(seq_k=t, seq_q=t)\n",
    "        d_slf_attn_mask = (slf_attn_mask_keypad + slf_attn_mask_subseq).gt(0)\n",
    "        d_dec_enc_attn_mask = tb.Models.get_attn_key_pad_mask(seq_k=x, seq_q=t)\n",
    "        \n",
    "        enc_attn, *_ = self.encoder(x, x_pos, e_slf_attn_mask, e_non_pad_mask)\n",
    "        attn, *_ = self.decoder(t, t_pos, x, enc_attn, \n",
    "                                  d_slf_attn_mask, d_non_pad_mask, d_dec_enc_attn_mask)\n",
    "        x = self.fc(attn)\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        return x\n",
    "    \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum\n",
    "    \n",
    "    def generate(self, x, start_symbol, max_len=1):\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        \n",
    "        # initialize target with start symbol\n",
    "        t = torch.tensor(start_symbol).view(1,-1).repeat(x.shape[0],1).long()\n",
    "        \n",
    "        for i in range(max_len-1):    \n",
    "            pred = self.forward(x, t)\n",
    "            # get last prediction & combine with target for next iteration\n",
    "            # pred = torch.zeros_like(pred).scatter_(2, torch.argmax(pred, -1, keepdim=True), 1)\n",
    "            pred = torch.argmax(pred, -1)\n",
    "            t = torch.cat((t, pred[:,-1:]), dim=1)\n",
    "            \n",
    "        if not self.batch_first:\n",
    "            t = t.transpose(0,1)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  8208\n"
     ]
    }
   ],
   "source": [
    "# Train the simple copy task.\n",
    "V = 11\n",
    "# criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = TransformerBaselineNLP(in_dim=V, out_dim=V, N=1, heads=4, model_dim=16, key_dim=4, value_dim=4, ff_dim=64, \n",
    "                            batch_first=True, pad=0)\n",
    "\n",
    "print('Model parameters: ', model.count_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Kzqdfl8KjB93",
    "outputId": "64672cd3-f5ac-4cfc-a744-51722437e002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  8208\n",
      "---- Epoch # 0\n",
      "Epoch Step: 1 Loss: 0.004773 Tokens per Sec: 13282.045679\n",
      "Epoch Step: 1 Loss: 0.004339 Tokens per Sec: 15194.276296\n",
      "---- Epoch # 1\n",
      "Epoch Step: 1 Loss: 0.004164 Tokens per Sec: 13937.381586\n",
      "Epoch Step: 1 Loss: 0.003875 Tokens per Sec: 15196.983046\n",
      "---- Epoch # 2\n",
      "Epoch Step: 1 Loss: 0.003827 Tokens per Sec: 14078.585754\n",
      "Epoch Step: 1 Loss: 0.003746 Tokens per Sec: 14890.755785\n",
      "---- Epoch # 3\n",
      "Epoch Step: 1 Loss: 0.003720 Tokens per Sec: 13823.350343\n",
      "Epoch Step: 1 Loss: 0.003687 Tokens per Sec: 14976.358778\n",
      "---- Epoch # 4\n",
      "Epoch Step: 1 Loss: 0.003668 Tokens per Sec: 13827.565763\n",
      "Epoch Step: 1 Loss: 0.003636 Tokens per Sec: 15156.893556\n",
      "---- Epoch # 5\n",
      "Epoch Step: 1 Loss: 0.003623 Tokens per Sec: 13110.204422\n",
      "Epoch Step: 1 Loss: 0.003518 Tokens per Sec: 15111.840774\n",
      "---- Epoch # 6\n",
      "Epoch Step: 1 Loss: 0.003520 Tokens per Sec: 14081.815664\n",
      "Epoch Step: 1 Loss: 0.003419 Tokens per Sec: 15122.828925\n",
      "---- Epoch # 7\n",
      "Epoch Step: 1 Loss: 0.003387 Tokens per Sec: 13890.340279\n",
      "Epoch Step: 1 Loss: 0.003267 Tokens per Sec: 15117.605291\n",
      "---- Epoch # 8\n",
      "Epoch Step: 1 Loss: 0.003300 Tokens per Sec: 13937.690346\n",
      "Epoch Step: 1 Loss: 0.003214 Tokens per Sec: 15120.693612\n",
      "---- Epoch # 9\n",
      "Epoch Step: 1 Loss: 0.003172 Tokens per Sec: 13907.881059\n",
      "Epoch Step: 1 Loss: 0.002951 Tokens per Sec: 14122.314940\n",
      "---- Epoch # 10\n",
      "Epoch Step: 1 Loss: 0.002981 Tokens per Sec: 13721.972977\n",
      "Epoch Step: 1 Loss: 0.002597 Tokens per Sec: 15056.237829\n",
      "---- Epoch # 11\n",
      "Epoch Step: 1 Loss: 0.002781 Tokens per Sec: 13567.103614\n",
      "Epoch Step: 1 Loss: 0.002349 Tokens per Sec: 15277.523380\n",
      "---- Epoch # 12\n",
      "Epoch Step: 1 Loss: 0.002512 Tokens per Sec: 13865.238591\n",
      "Epoch Step: 1 Loss: 0.002177 Tokens per Sec: 14281.439277\n",
      "---- Epoch # 13\n",
      "Epoch Step: 1 Loss: 0.002328 Tokens per Sec: 14097.513619\n",
      "Epoch Step: 1 Loss: 0.001644 Tokens per Sec: 14983.849053\n",
      "---- Epoch # 14\n",
      "Epoch Step: 1 Loss: 0.001741 Tokens per Sec: 13790.777223\n",
      "Epoch Step: 1 Loss: 0.000916 Tokens per Sec: 15177.461017\n",
      "---- Epoch # 15\n",
      "Epoch Step: 1 Loss: 0.001311 Tokens per Sec: 14003.496725\n",
      "Epoch Step: 1 Loss: 0.000420 Tokens per Sec: 15009.005108\n",
      "---- Epoch # 16\n",
      "Epoch Step: 1 Loss: 0.000992 Tokens per Sec: 13984.975743\n",
      "Epoch Step: 1 Loss: 0.000097 Tokens per Sec: 15354.423900\n",
      "---- Epoch # 17\n",
      "Epoch Step: 1 Loss: 0.000621 Tokens per Sec: 13809.772159\n",
      "Epoch Step: 1 Loss: 0.000060 Tokens per Sec: 15076.397359\n",
      "---- Epoch # 18\n",
      "Epoch Step: 1 Loss: 0.000312 Tokens per Sec: 14008.524519\n",
      "Epoch Step: 1 Loss: 0.000018 Tokens per Sec: 15090.681442\n",
      "---- Epoch # 19\n",
      "Epoch Step: 1 Loss: 0.000174 Tokens per Sec: 13283.517945\n",
      "Epoch Step: 1 Loss: 0.000009 Tokens per Sec: 15212.002370\n"
     ]
    }
   ],
   "source": [
    "model_opt = NoamOpt(16, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(\"---- Epoch #\", epoch)\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 30, 20), model, \n",
    "              SimpleLossCompute(criterion, model_opt), plot_grad=False)\n",
    "    model.eval()\n",
    "    run_epoch(data_gen(V, 30, 10), model, \n",
    "              SimpleLossCompute(criterion, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Z4RwZGBzs-u2",
    "outputId": "0d76ef8f-c8d6-4a4e-95ca-cfa17e65f9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 8, 7, 4, 8, 6, 3, 6, 9, 4, 9]])\n",
      "tensor([[1, 8, 7, 4, 8, 6, 3, 6, 9, 4, 9, 8, 7, 4, 8, 6, 3, 6, 9]])\n",
      "---\n",
      "tensor([[8, 7, 4, 8, 6, 3, 6, 9, 4, 9, 8, 7, 4, 8, 6, 3, 6, 9, 4]])\n",
      "=====\n",
      "predictions\n",
      "tensor([[8, 7, 4, 8, 6, 3, 6, 9, 4, 9, 8, 7, 4, 8, 6, 3, 6, 9, 4]])\n",
      "=====\n",
      "inference\n",
      "tensor([[1, 8, 7, 4, 8, 6, 3, 6, 9, 4, 9, 8, 7, 4, 8, 6, 3, 6, 9, 4]])\n"
     ]
    }
   ],
   "source": [
    "print(b.src[:1])\n",
    "print(b.trg[:1, :-1])\n",
    "print('---')\n",
    "print(b.trg_y[:1, :-1])\n",
    "print('=====')\n",
    "print('predictions')\n",
    "pred = model(b.src[:1], b.trg[:1, :-1]).argmax(-1)\n",
    "print(pred)\n",
    "print('=====')\n",
    "print('inference')\n",
    "gen = model.generate(b.src[:1], start_symbol=1, max_len=b.trg.shape[1])\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-eE5YKefO9M"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:20:50.442630Z",
     "start_time": "2019-07-15T15:20:50.423684Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qI-jAckofO9N"
   },
   "outputs": [],
   "source": [
    "def train_transformer(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if train_x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        \n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[:,ordering,:]\n",
    "        \n",
    "        epoch_time = time.time()\n",
    "        \n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            # 0:(n-1) of target is sent to model\n",
    "            # 1:n is used as label to predict\n",
    "            target = train_y[:-1,start:end,:]\n",
    "            label = train_y[1:,start:end,:]\n",
    "            \n",
    "            st = time.time()\n",
    "            \n",
    "            o = model(train_x[:,start:end,:], target)\n",
    "            \n",
    "            o = o.contiguous().view(-1, 2)\n",
    "            gt = torch.argmax(label, 2, keepdim=True).view(-1)\n",
    "            loss = loss_fn(o, gt)\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}  time: {:2.5}\".format(i, j+1, int(train_size/batch_size), \n",
    "                                        loss_tracker[-1], (time.time()-st)), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate_transformer(model, train_x, train_y)\n",
    "        f1_test = evaluate_transformer(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Total time: {:2.5}\".format(time.time() - epoch_time))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_transformer(model, x, y, start_token=START):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            o = model.generate(x[:,start:end,:], start_token=start_token, max_len=y.shape[0])\n",
    "        pred = torch.argmax(o, 2, keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        label = torch.argmax(y[:,start:end,:], 2, \n",
    "                             keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        labels.extend(label)\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyFi5y3FkVxi"
   },
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:29:53.945622Z",
     "start_time": "2019-07-15T15:29:53.912989Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3NNu8QaQkZ8U"
   },
   "outputs": [],
   "source": [
    "from transformer import PositionalEncoding, Encoder, Decoder, get_subsequent_mask\n",
    "\n",
    "class TransformerSeq2SeqDifferent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.name = 'transformer'\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # define layers\n",
    "        # embedding layers\n",
    "        self.src_embed = nn.Linear(in_dim, model_dim)\n",
    "        self.tgt_embed = nn.Linear(in_dim, model_dim)\n",
    "        self.pos_enc = PositionalEncoding(model_dim, max_len)\n",
    "        # encoder-decoder\n",
    "        self.encoder = Encoder(N, heads, model_dim, key_dim, value_dim, ff_dim)\n",
    "        self.decoder = Decoder(N, heads, model_dim, key_dim, value_dim, ff_dim)\n",
    "        # final output layer\n",
    "        self.fc = nn.Linear(model_dim, out_dim)\n",
    "    \n",
    "        # xavier initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            src = src.transpose(0, 1)\n",
    "            tgt = tgt.transpose(0, 1)\n",
    "        \n",
    "        # get subsequent mask for target sequence\n",
    "        tgt_subseq_mask = get_subsequent_mask(tgt)\n",
    "        # combine with tgt mask if provided\n",
    "        if tgt_mask is not None:\n",
    "            tgt_subseq_mask = (tgt_mask + tgt_subseq_mask).gt(0)\n",
    "        \n",
    "        ## get encoder attention from source\n",
    "        src = self.src_embed(src)\n",
    "        src = self.pos_enc(src)\n",
    "        src_attn = self.encoder(src, src_mask)\n",
    "        \n",
    "        ## get decoder attention from target & source attention\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "        x = self.decoder(src_attn, tgt, src_mask, tgt_subseq_mask)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        return x\n",
    "        \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum\n",
    "\n",
    "    def generate(self, src, start_token, src_mask=None, max_len=1):\n",
    "        \n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            src = src.transpose(0, 1)\n",
    "        \n",
    "        ## get encoder attention from source\n",
    "        src = self.src_embed(src)\n",
    "        src = self.pos_enc(src)\n",
    "        src_attn = self.encoder(src, src_mask)\n",
    "        \n",
    "        # initialize target with start symbol - 1 x b x dim\n",
    "        tgt = torch.tensor(start_token).view(1,1,-1).repeat(src.shape[0],1,1).float()\n",
    "        \n",
    "        for i in range(max_len-1):\n",
    "            # generate subsequent mask for target sequence\n",
    "            tgt_subseq_mask = get_subsequent_mask(tgt)\n",
    "            ## get decoder attention from target & source attention\n",
    "            tgt_embed = self.tgt_embed(tgt)\n",
    "            tgt_embed = self.pos_enc(tgt_embed)\n",
    "            x = self.decoder(src_attn, tgt_embed, src_mask, tgt_subseq_mask)\n",
    "            # get last predictions and combine with target for next iteration\n",
    "            x = self.fc(x[:,-1:])\n",
    "            x = torch.zeros_like(x).scatter_(2, torch.argmax(x, -1, keepdim=True), 1)            \n",
    "            tgt = torch.cat((tgt, x), dim=1)\n",
    "            \n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            tgt = tgt.transpose(0, 1)\n",
    "        return tgt\n",
    "    \n",
    "#     def generate(self, src, start_token, src_mask=None, max_len=1):\n",
    "\n",
    "#         # initialize target with start symbol - 1 x b x dim\n",
    "#         t = torch.tensor(start_token).view(1,1,-1).repeat(1,src.shape[1],1).float()\n",
    "\n",
    "#         for i in range(max_len-1):\n",
    "#             # get the last prediction only\n",
    "#             pred = self.forward(src, t, src_mask)[-1:]\n",
    "#             # get last prediction & combine with target for next iteration\n",
    "#             pred = torch.zeros_like(pred).scatter_(2, torch.argmax(pred, -1, keepdim=True), 1)\n",
    "#             t = torch.cat((t, pred), dim=0)\n",
    "\n",
    "#         return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:29:55.311707Z",
     "start_time": "2019-07-15T15:29:55.299858Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "QVvw-6DIlqJ-",
    "outputId": "433038f6-ea73-4f0d-f080-d4bbae2dd233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7414\n"
     ]
    }
   ],
   "source": [
    "pad = torch.tensor([0,0]).float()\n",
    "\n",
    "transformer = TransformerSeq2SeqDifferent(in_dim=2, out_dim=2, N=1, heads=4, \n",
    "                                          model_dim=16, key_dim=4, value_dim=3, ff_dim=64,\n",
    "                                          batch_first=False)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "print(transformer.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T15:52:25.094743Z",
     "start_time": "2019-07-15T15:31:28.360595Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P25eHawmmH-8",
    "outputId": "fb759666-d50a-41da-ab18-a409a33e93b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.023669  time: 0.0229065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0441945\n",
      "Total time: 38.951\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.02499  time: 0.01881367\n",
      "Average Loss: 0.0253661\n",
      "Total time: 39.108\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.028833  time: 0.0302924\n",
      "Average Loss: 0.0196663\n",
      "Total time: 44.601\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.0022561  time: 0.0152979\n",
      "Average Loss: 0.0132855\n",
      "Total time: 35.416\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.0013465  time: 0.0176073\n",
      "Average Loss: 0.0108105\n",
      "Total time: 41.438\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.00057886  time: 0.025523\n",
      "Average Loss: 0.00908422\n",
      "Total time: 49.995\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.00056462  time: 0.018888\n",
      "Average Loss: 0.00711359\n",
      "Total time: 49.959\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.00039035  time: 0.021207\n",
      "Average Loss: 0.00690112\n",
      "Total time: 53.431\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.0013625  time: 0.0251772\n",
      "Average Loss: 0.00628198\n",
      "Total time: 48.2\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.0027381  time: 0.0288174\n",
      "Average Loss: 0.00565668\n",
      "Total time: 47.921\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.016443  time: 0.01764866\n",
      "Average Loss: 0.00445755\n",
      "Total time: 48.298\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.0015412  time: 0.0221469\n",
      "Average Loss: 0.00493653\n",
      "Total time: 48.1\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.0022483  time: 0.0166294\n",
      "Average Loss: 0.00404175\n",
      "Total time: 44.234\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.00066894  time: 0.015757\n",
      "Average Loss: 0.00358166\n",
      "Total time: 39.141\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.00039912  time: 0.016568\n",
      "Average Loss: 0.00356262\n",
      "Total time: 32.768\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.018893  time: 0.01561812\n",
      "Average Loss: 0.00363895\n",
      "Total time: 38.672\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.00058784  time: 0.019052\n",
      "Average Loss: 0.0026211\n",
      "Total time: 44.517\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.00044817  time: 0.017228\n",
      "Average Loss: 0.0021156\n",
      "Total time: 30.748\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.00028521  time: 0.015056\n",
      "Average Loss: 0.00265861\n",
      "Total time: 30.64\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.042209  time: 0.01517499\n",
      "Average Loss: 0.00255309\n",
      "Total time: 42.181\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 5.882e-05  time: 0.0309599\n",
      "Average Loss: 0.00331494\n",
      "Total time: 52.771\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.029697  time: 0.02344635\n",
      "Average Loss: 0.00248193\n",
      "Total time: 52.351\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.00086481  time: 0.019335\n",
      "Average Loss: 0.00213655\n",
      "Total time: 39.286\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.0001307  time: 0.0189944\n",
      "Average Loss: 0.00217393\n",
      "Total time: 33.459\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.00015272  time: 0.031725\n",
      "Average Loss: 0.00337484\n",
      "Total time: 38.67\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.00028088  time: 0.016077\n",
      "Average Loss: 0.00167688\n",
      "Total time: 31.789\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 3.4474e-05  time: 0.015631\n",
      "Average Loss: 0.00108784\n",
      "Total time: 31.763\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 3.2917e-05  time: 0.014099\n",
      "Average Loss: 0.00162701\n",
      "Total time: 41.644\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.0020972  time: 0.0431776\n",
      "Average Loss: 0.00125152\n",
      "Total time: 41.347\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.0001559  time: 0.0156185\n",
      "Average Loss: 0.00217377\n",
      "Total time: 45.246\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "transformer = train_transformer(transformer, train_x, train_y, test_x, test_y, \n",
    "                                epochs=30, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAi4qIOpfO9R"
   },
   "source": [
    "## Baseline implementation\n",
    "\n",
    "From github: https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T13:59:01.988938Z",
     "start_time": "2019-07-15T13:59:01.967448Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5qn0cW1KfO9T"
   },
   "outputs": [],
   "source": [
    "import transformer_baseline as tb\n",
    "\n",
    "\n",
    "## Padding masks - for 3 dim input\n",
    "def get_attn_key_pad_mask(seq_k, seq_q, pad=tb.Constants.PAD):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    assert seq_k.dim() == 3 and seq_q.dim() == 3\n",
    "    \n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = torch.all(seq_k.eq(pad), dim=-1)  # b x lq\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "    return padding_mask\n",
    "\n",
    "def get_non_pad_mask(seq, pad=tb.Constants.PAD):\n",
    "    assert seq.dim() == 3\n",
    "    padding_mask = ~torch.all(seq.ne(pad), dim=-1)  # b x l\n",
    "#     padding_mask = padding_mask.repeat(1, 1, seq.shape[-1])  # b x l x d (repeated)\n",
    "    return padding_mask.type(torch.float).unsqueeze(-1)\n",
    "\n",
    "\n",
    "## Model\n",
    "class TransformerBaseline(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=True, pad=tb.Constants.PAD):\n",
    "        super().__init__()\n",
    "        self.name = 'transformer'\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.encoder = tb.Models.Encoder(\n",
    "            n_src_vocab=in_dim, len_max_seq=max_len,\n",
    "            d_word_vec=model_dim, d_model=model_dim, d_inner=ff_dim,\n",
    "            n_layers=N, n_head=heads, d_k=key_dim, d_v=value_dim,\n",
    "            dropout=0.0, embedding='linear')\n",
    "        \n",
    "        self.decoder = tb.Models.Decoder(\n",
    "            n_tgt_vocab=out_dim, len_max_seq=max_len,\n",
    "            d_word_vec=model_dim, d_model=model_dim, d_inner=ff_dim,\n",
    "            n_layers=N, n_head=heads, d_k=key_dim, d_v=value_dim,\n",
    "            dropout=0.0, embedding='linear')\n",
    "        \n",
    "        self.fc = nn.Linear(model_dim, out_dim, bias=False)\n",
    "        \n",
    "        # This was important from their code. \n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "            t = t.transpose(0,1)\n",
    "        \n",
    "        # encoder requires source sequence & positions of each \n",
    "        x_pos = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0], 1)\n",
    "        t_pos = torch.arange(t.shape[1]).unsqueeze(0).repeat(t.shape[0], 1)\n",
    "        \n",
    "        # -- Prepare masks\n",
    "        # encoder\n",
    "        e_slf_attn_mask = get_attn_key_pad_mask(seq_k=x, seq_q=x, pad=self.pad)\n",
    "        e_non_pad_mask = torch.ones(x.shape[0], x.shape[1], 1)\n",
    "        e_non_pad_mask = get_non_pad_mask(x, pad=self.pad)\n",
    "        \n",
    "        # decoder\n",
    "        d_non_pad_mask = get_non_pad_mask(t)\n",
    "        slf_attn_mask_subseq = tb.Models.get_subsequent_mask(t)\n",
    "        slf_attn_mask_keypad = get_attn_key_pad_mask(seq_k=t, seq_q=t)\n",
    "        d_slf_attn_mask = (slf_attn_mask_keypad + slf_attn_mask_subseq).gt(0)\n",
    "        d_dec_enc_attn_mask = get_attn_key_pad_mask(seq_k=x, seq_q=t)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        enc_attn, *_ = self.encoder(x, x_pos, e_slf_attn_mask, e_non_pad_mask)\n",
    "        \n",
    "        print('enc time...', time.time() - start)\n",
    "        \n",
    "        attn, *_ = self.decoder(t, t_pos, x, enc_attn, \n",
    "                                  d_slf_attn_mask, d_non_pad_mask, d_dec_enc_attn_mask)\n",
    "        print('decoder time............', time.time() - start)\n",
    "        \n",
    "        x = self.fc(attn)\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        return x\n",
    "      \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T13:59:03.748434Z",
     "start_time": "2019-07-15T13:59:03.147297Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NFiOaTC5fO9V",
    "outputId": "8c3af626-75b7-4416-f717-cda1ee157b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7412\n"
     ]
    }
   ],
   "source": [
    "pad = torch.tensor([0,0]).float()\n",
    "\n",
    "baseline = TransformerBaseline(in_dim=2, out_dim=2, N=1, heads=4, model_dim=16, key_dim=4, value_dim=3, ff_dim=64,\n",
    "                               batch_first=False, pad=0)\n",
    "baseline = baseline.to(device)\n",
    "\n",
    "print(baseline.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T13:59:04.568071Z",
     "start_time": "2019-07-15T13:59:04.557358Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HWWRctTdfO9Z",
    "outputId": "aae18d32-3e5b-41ac-a671-fa903d39fb49"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-83e1c81303d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "baseline = train_transformer(baseline, train_x, train_y, test_x, test_y, epochs=30, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FEANP5Dqgfft"
   ],
   "name": "[TOY] Seq2Seq-different.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
