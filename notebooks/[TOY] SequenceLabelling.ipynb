{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:36.994072Z",
     "start_time": "2019-07-14T20:58:33.061762Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:37.008237Z",
     "start_time": "2019-07-14T20:58:36.997259Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "learning_rate = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR task\n",
    "\n",
    "The entire dataset comprises of the binary representation of all numbers uptil a range defined. The binary sequence from left to right (most significant to least significant) is the input. While the y or the output for an input is calculated as: $a1 \\oplus a10 \\wedge a3 \\oplus a7$. Where, the most significant bit is a1, the least significant bit is a10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:37.156974Z",
     "start_time": "2019-07-14T20:58:37.012251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating data\n",
    "state_size = 10\n",
    "data_x = []\n",
    "for i in range(pow(2, state_size)):\n",
    "    data_x.append([int(x) for x in list(np.binary_repr(i, width=state_size))])\n",
    "data_x = np.array(data_x)\n",
    "\n",
    "data_y = []\n",
    "for x in data_x:\n",
    "    # a1 xor a10 ^ a3 xor a7\n",
    "    data_y.append(np.bitwise_and(np.bitwise_xor(x[0], x[9]), \n",
    "                                 np.bitwise_xor(x[2], x[6])))\n",
    "data_y = np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:37.236713Z",
     "start_time": "2019-07-14T20:58:37.163282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshaping for tensors\n",
    "data_x = np.transpose(data_x).reshape(state_size, pow(2, state_size), 1)\n",
    "data_x = torch.from_numpy(data_x).float()\n",
    "data_y = torch.from_numpy(data_y).float()\n",
    "\n",
    "# Reshaping X to 2-input dimensions\n",
    "data_x = torch.zeros(data_x.shape[0], data_x.shape[1], 2).scatter_(2, data_x.long(), 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:37.254976Z",
     "start_time": "2019-07-14T20:58:37.244428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:37.308925Z",
     "start_time": "2019-07-14T20:58:37.257506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating training and test sets\n",
    "train_size = 0.7\n",
    "ordering = torch.randperm(pow(2, state_size))\n",
    "data_x = data_x[:, ordering, :]\n",
    "data_y = data_y[ordering]\n",
    "train_x = data_x[:,:int(train_size * len(ordering)),:]\n",
    "train_y = data_y[:int(train_size * len(ordering))]\n",
    "test_x = data_x[:,int(train_size * len(ordering)):,:]\n",
    "test_y = data_y[int(train_size * len(ordering)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:37.348992Z",
     "start_time": "2019-07-14T20:58:37.311984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 716, 2]) torch.Size([716]) torch.Size([10, 308, 2]) torch.Size([308])\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:42:19.802670Z",
     "start_time": "2019-07-10T22:42:19.797161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input dim\n",
    "input_dim = 2\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 16\n",
    "# Number of output nodes\n",
    "output_dim = 1\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = True\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:42:20.737545Z",
     "start_time": "2019-07-10T22:42:20.719199Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_lstm(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[ordering]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            if model.bidirectional:\n",
    "                hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            o = model(train_x[:,start:end,:], hidden_state, cell_state)\n",
    "            loss = loss_fn(o.view(-1), train_y[start:end])\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size) + 1, \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate_lstm(model, train_x, train_y)\n",
    "        f1_test = evaluate_lstm(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_lstm(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if model.bidirectional:\n",
    "            hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], hidden_state, cell_state)\n",
    "        pred = torch.round(torch.sigmoid(o.view(-1))).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        labels.extend(y[start:end].int().detach().cpu().numpy())\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:42:39.452954Z",
     "start_time": "2019-07-10T22:42:39.424034Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from lstm import LSTMCell\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A complete LSTM architecture\n",
    "\n",
    "    Allows to stack multiple LSTM cells and also\n",
    "    create a bidirectional LSTM network.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: Dimension of input data\n",
    "    hidden_dim: Size of hidden state\n",
    "    layernorm: True/False\n",
    "    layers: Number of LSTM cells to stack\n",
    "    bidirectional: True/False\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        if self.layers < 1:\n",
    "            raise ValueError(\"layers need to be > 1\")\n",
    "        self.model = []\n",
    "        for i in range(self.layers):\n",
    "            self.model.append(LSTMCell(input_dim, hidden_dim, layernorm))\n",
    "        self.model = nn.ModuleList(self.model)\n",
    "        if self.bidirectional:\n",
    "            self.model_rev = []\n",
    "            for i in range(self.layers):\n",
    "                self.model_rev.append(LSTMCell(input_dim, hidden_dim, layernorm))\n",
    "            self.model_rev = nn.ModuleList(self.model_rev)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \"\"\"Forward pass for the LSTM network\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        x: [sequence_length, batch_size, input_dim]\n",
    "        hidden_state: [1, batch_size, hidden_dim]\n",
    "        cell_state: [1, batch_size, hidden_dim]\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        output, (hidden_state, cell_state)\n",
    "            output: [sequence_length, batch_size, hidden_dim]\n",
    "                contains the output/hidden_state from all the timesteps\n",
    "                for the final layer in sequence 1...T\n",
    "            hidden_state: [layers, batch_size, hidden_dim]\n",
    "                contains the hidden_state from the last timestep T\n",
    "                from all the layers\n",
    "            cell_state: [layers, batch_size, hidden_dim]\n",
    "                contains the cell_state from the last timestep T\n",
    "                from all the layers\n",
    "\n",
    "            If bidirectional=True\n",
    "                output: [sequence_length, batch_size, 2 * hidden_dim]\n",
    "                    [:,:,:hidden_dim] - for left-to-right\n",
    "                    [:,:,hidden_dim:] - for right-to-left\n",
    "                hidden_state: [2 * layers, batch_size, hidden_dim]\n",
    "                    [:layers,:,:] - for left-to-right\n",
    "                    [layers:,:,:] - for right-to-left\n",
    "                cell_state: [layers, batch_size, hidden_dim]\n",
    "                    [:layers,:,:] - for left-to-right\n",
    "                    [layers:,:,:] - for right-to-left\n",
    "        \"\"\"\n",
    "        device = 'cpu'\n",
    "        if x.is_cuda:\n",
    "            device = 'cuda'\n",
    "        seq_length = x.shape[0]\n",
    "        # Left-to-right pass\n",
    "        # index of states is equivalent to index of layer in LSTM stack\n",
    "        hidden_states = hidden_state[:self.layers,:,:].view(self.layers, 1,\n",
    "                                                            hidden_state.shape[1],\n",
    "                                                            hidden_state.shape[2])\n",
    "        cell_states = cell_state[:self.layers,:,:].view(self.layers, 1,\n",
    "                                                        cell_state.shape[1],\n",
    "                                                        cell_state.shape[2])\n",
    "        output = torch.tensor([], requires_grad=True).to(device)\n",
    "        # forward pass for one cell at a time along layers\n",
    "        for j in range(self.layers):\n",
    "            output, (hidden_states[j], cell_states[j]) = self.model[j](x, hidden_states[j].clone(),\n",
    "                                                                  cell_states[j].clone())\n",
    "        hidden_states = hidden_states.squeeze(1)\n",
    "        cell_states = cell_states.squeeze(1)\n",
    "\n",
    "        ## TODO:\n",
    "        ## The current code will work if bidirectional=False and\n",
    "        ## the hidden_state.shape[0] > self.layers owing to [:layers,:,:].\n",
    "        ## Maybe a warning should be raised without termination.\n",
    "\n",
    "        # Right-to-left pass\n",
    "        if self.bidirectional:\n",
    "            # flipping inputs/rearranging x to be in reverse timestep order\n",
    "            x = torch.flip(x, [0])  # reversing only the sequence dimension\n",
    "            # index of states is equivalent to index of layer in LSTM stack\n",
    "            hidden_states_rev = hidden_state[self.layers:,:,:].view(self.layers, 1,\n",
    "                                                                hidden_state.shape[1],\n",
    "                                                                hidden_state.shape[2])\n",
    "            cell_states_rev = cell_state[self.layers:,:,:].view(self.layers, 1,\n",
    "                                                             cell_state.shape[1],\n",
    "                                                             cell_state.shape[2])\n",
    "            output_rev = torch.tensor([], requires_grad=True).to(device)\n",
    "            # forward pass for one cell at a time along layers\n",
    "            for j in range(self.layers):\n",
    "                output_rev, (hidden_states_rev[j], cell_states_rev[j]) = self.model_rev[j](x,\n",
    "                                                                        hidden_states_rev[j].clone(),\n",
    "                                                                        cell_states_rev[j].clone())\n",
    "            # flipping outputs to be in correct timestep order\n",
    "            output_rev = torch.flip(output_rev, [0]) # reversing only the sequence dimension\n",
    "            hidden_states_rev = hidden_states_rev.squeeze(1)\n",
    "            cell_states_rev = cell_states_rev.squeeze(1)\n",
    "            # concatenating tensors\n",
    "            ## creating tensors as expected in\n",
    "            ## here: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "            hidden_states = torch.cat((hidden_states,\n",
    "                                       hidden_states_rev), dim=0)\n",
    "            cell_states = torch.cat((cell_states,\n",
    "                                     cell_states_rev), dim=0)\n",
    "            output = torch.cat((output,\n",
    "                                output_rev), dim=2)\n",
    "\n",
    "        return output, (hidden_states, cell_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:42:40.700885Z",
     "start_time": "2019-07-10T22:42:40.684845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from lstm import LSTM\n",
    "\n",
    "class LSTMSeqLabel(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence Labelling (many-to-one)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    vocab_len: int from imdb dataset\n",
    "    embed_dim: dimensions of the embeddings\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    pretrained_vec: weights from imdb object\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "        \n",
    "        self.lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        output, (_, _) = self.lstm(x, hidden_state, cell_state)\n",
    "        output = output[-1].unsqueeze(0)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:42:42.210661Z",
     "start_time": "2019-07-10T22:42:42.193562Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSeqLabel(input_dim, hidden_dim, output_dim, bidirectional=True, layers=layers).to(device)\n",
    "print(model.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:42:54.176354Z",
     "start_time": "2019-07-10T22:42:43.991129Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.57015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.619753\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.27476\n",
      "Average Loss: 0.561626\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.84467\n",
      "Average Loss: 0.566193\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.84708\n",
      "Average Loss: 0.565686\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.83391\n",
      "Average Loss: 0.566083\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.85345\n",
      "Average Loss: 0.566442\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  27/90  -- Loss: 0.42597\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9862787b518d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b9e8c6c76f2a>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train_lstm(model, train_x, train_y, test_x, test_y, epochs=500, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PyTorch baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:43:10.040503Z",
     "start_time": "2019-07-10T22:43:10.013795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using PyTorch LSTM module\n",
    "class PyTorchBaseline(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_output, \n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = n_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layers = layers\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.lstm = nn.LSTM(n_input, n_hidden, bidirectional=self.bidirectional, num_layers=layers)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * n_hidden, n_output)\n",
    "        else:\n",
    "            self.fc = nn.Linear(n_hidden, n_output)\n",
    "        if self.layernorm and self.bidirectional:\n",
    "            self.ln = LayerNorm(2 * self.hidden_dim)\n",
    "        elif self.layernorm:\n",
    "            self.ln = LayerNorm(self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        o, (_, _) = self.lstm(x, (h, c))\n",
    "        o = o[-1].unsqueeze(0)\n",
    "        if self.layernorm:\n",
    "            output = self.fc(self.ln(o))\n",
    "        else:\n",
    "            output = self.fc(o)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:43:13.382581Z",
     "start_time": "2019-07-10T22:43:13.377419Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2593\n"
     ]
    }
   ],
   "source": [
    "model = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(model.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:43:35.459595Z",
     "start_time": "2019-07-10T22:43:29.011353Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.57037\n",
      "Average Loss: 0.617523\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.56855\n",
      "Average Loss: 0.564826\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.56483\n",
      "Average Loss: 0.563855\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.55875\n",
      "Average Loss: 0.563758\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.28745\n",
      "Average Loss: 0.561983\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.56647\n",
      "Average Loss: 0.56343\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  90/90  -- Loss: 0.56415\n",
      "Average Loss: 0.563175\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #8  : Batch  90/90  -- Loss: 0.56221\n",
      "Average Loss: 0.563539\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #9  : Batch  90/90  -- Loss: 0.29676\n",
      "Average Loss: 0.562005\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #10 : Batch  64/90  -- Loss: 0.56243\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9862787b518d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b9e8c6c76f2a>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train_lstm(model, train_x, train_y, test_x, test_y, epochs=500, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "our = LSTMSeqLabel(input_dim, hidden_dim, output_dim, bidirectional=True, layers=layers).to(device)\n",
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "==================\n",
      "# of parameters: 2465\n",
      "lstm.model.0.weights     : torch.Size([18, 64])\n",
      "lstm.model.0.bias        : torch.Size([64])\n",
      "lstm.model_rev.0.weights : torch.Size([18, 64])\n",
      "lstm.model_rev.0.bias    : torch.Size([64])\n",
      "fc.weight                : torch.Size([1, 32])\n",
      "fc.bias                  : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\\n{}\".format(\"=\" * len(\"Our implementation\")))\n",
    "print(\"# of parameters: {}\".format(our.count_parameters()))\n",
    "for name, param in our.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch implementation\n",
      "======================\n",
      "# of parameters: 2593\n",
      "lstm.weight_ih_l0        : torch.Size([64, 2])\n",
      "lstm.weight_hh_l0        : torch.Size([64, 16])\n",
      "lstm.bias_ih_l0          : torch.Size([64])\n",
      "lstm.bias_hh_l0          : torch.Size([64])\n",
      "lstm.weight_ih_l0_reverse: torch.Size([64, 2])\n",
      "lstm.weight_hh_l0_reverse: torch.Size([64, 16])\n",
      "lstm.bias_ih_l0_reverse  : torch.Size([64])\n",
      "lstm.bias_hh_l0_reverse  : torch.Size([64])\n",
      "fc.weight                : torch.Size([1, 32])\n",
      "fc.bias                  : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch implementation\\n{}\".format(\"=\" * len(\"PyTorch implementation\")))\n",
    "print(\"# of parameters: {}\".format(pytorch.count_parameters()))\n",
    "for name, param in pytorch.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PyTorch uses $Wh + b_h + Wx + b_x$ whereas we are using $Wx' + b$, where $x'$ is $h, x$ concatenated. Therefore PyTorch has an extra set of biases for each direction.\n",
    "\n",
    "For one direction - 64 <br>\n",
    "For reverse direction - 64 <br>\n",
    "\n",
    "Our model has $6978$ parameters while the PyTorch model has $6978 + 64 + 64 + 128 = 7234$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T20:58:43.345608Z",
     "start_time": "2019-07-14T20:58:43.325460Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_transformer(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[ordering]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            o = model(train_x[:,start:end,:])\n",
    "            # transform output to be of same dim as label\n",
    "            o = o.mean(dim=0)  # mean over all attention output\n",
    "            loss = loss_fn(o.view(-1), train_y[start:end])\n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            # plot_grad_flow(model.named_parameters())\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size) + 1, \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate_transformer(model, train_x, train_y)\n",
    "        f1_test = evaluate_transformer(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_transformer(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:])\n",
    "            # transform output to be of same dim as label\n",
    "            o = o.mean(dim=0)  # mean over all attention output\n",
    "        pred = torch.round(torch.sigmoid(o.view(-1))).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        labels.extend(y[start:end].int().detach().cpu().numpy())\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:04:04.993643Z",
     "start_time": "2019-07-14T21:04:04.980259Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformer import PositionalEncoding, Encoder\n",
    "\n",
    "class TransformerSeqLabel(nn.Module):\n",
    "    \"\"\" Transformer Class for Sequence Labelling (many-to-one)\n",
    "\n",
    "    The class creates the Transformer encoder architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the attention to output_dim.\n",
    "    The final prediction is averaged over sequence length to get final score\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    in_dim: input vocab size from imdb dataset\n",
    "    out_dim: output dimensions of the model\n",
    "    model_dim: embedding dimension, also the dimensionality at which the transformer operates\n",
    "    key_dim: dimensions for query & key in attention calculation\n",
    "    value_dim: dimensions for value in attention calculation\n",
    "    ff_dim: dimensions for Positionwise feed-forward sublayer\n",
    "    max_len: max length to generate positional encodings (default=10000)\n",
    "    batch_first: transposes the 1st 2 dimensions of the input to have 'batch' as the first\n",
    "        if the input dimensions are of form [seq_len, batch, dim] (default=True)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.embed = nn.Linear(in_dim, model_dim)\n",
    "        self.pos_enc = PositionalEncoding(model_dim, max_len)\n",
    "        self.encoder = Encoder(N, heads, model_dim, key_dim, value_dim, ff_dim)\n",
    "        # final output layer\n",
    "        self.fc = nn.Linear(model_dim, out_dim)\n",
    "        \n",
    "        # xavier initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "            \n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x, mask)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # transpose back to original [seq_len, batch, dim]\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        return x\n",
    "        \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:07:31.307438Z",
     "start_time": "2019-07-14T21:07:31.298192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2437\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerSeqLabel(in_dim=2, out_dim=1, N=1, heads=4, model_dim=12, \n",
    "                                  key_dim=4, value_dim=3, ff_dim=64)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "print(transformer.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:08:57.104787Z",
     "start_time": "2019-07-14T21:07:43.798310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.23617\n",
      "Average Loss: 0.580998\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.87053\n",
      "Average Loss: 0.578611\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.55954\n",
      "Average Loss: 0.57496\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.56821\n",
      "Average Loss: 0.574023\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.32981\n",
      "Average Loss: 0.574171\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.56612\n",
      "Average Loss: 0.575961\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  90/90  -- Loss: 0.56355\n",
      "Average Loss: 0.575225\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #8  : Batch  90/90  -- Loss: 0.88615\n",
      "Average Loss: 0.578117\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #9  : Batch  90/90  -- Loss: 0.85113\n",
      "Average Loss: 0.576824\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #10 : Batch  90/90  -- Loss: 0.57687\n",
      "Average Loss: 0.570679\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #11 : Batch  90/90  -- Loss: 0.85584\n",
      "Average Loss: 0.574465\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #12 : Batch  90/90  -- Loss: 0.29558\n",
      "Average Loss: 0.57447\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #13 : Batch  90/90  -- Loss: 0.25816\n",
      "Average Loss: 0.570931\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #14 : Batch  90/90  -- Loss: 0.82862\n",
      "Average Loss: 0.575479\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #15 : Batch  90/90  -- Loss: 0.29521\n",
      "Average Loss: 0.57194\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #16 : Batch  90/90  -- Loss: 0.86036\n",
      "Average Loss: 0.575748\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #17 : Batch  90/90  -- Loss: 0.56298\n",
      "Average Loss: 0.571024\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #18 : Batch  90/90  -- Loss: 0.89305\n",
      "Average Loss: 0.574154\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #19 : Batch  90/90  -- Loss: 0.28874\n",
      "Average Loss: 0.572368\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #20 : Batch  90/90  -- Loss: 0.32529\n",
      "Average Loss: 0.568884\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #21 : Batch  90/90  -- Loss: 0.52594\n",
      "Average Loss: 0.565534\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #22 : Batch  90/90  -- Loss: 0.79284\n",
      "Average Loss: 0.56031\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #23 : Batch  90/90  -- Loss: 1.20369\n",
      "Average Loss: 0.570954\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #24 : Batch  90/90  -- Loss: 0.51357\n",
      "Average Loss: 0.549547\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #25 : Batch  90/90  -- Loss: 0.56429\n",
      "Average Loss: 0.538998\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #26 : Batch  90/90  -- Loss: 0.29774\n",
      "Average Loss: 0.517523\n",
      "Training F1: 0.4043\n",
      "Test F1: 0.3604\n",
      "Epoch #27 : Batch  90/90  -- Loss: 0.34759\n",
      "Average Loss: 0.457822\n",
      "Training F1: 0.6405\n",
      "Test F1: 0.5864\n",
      "Epoch #28 : Batch  90/90  -- Loss: 0.257953\n",
      "Average Loss: 0.391326\n",
      "Training F1: 0.5528\n",
      "Test F1: 0.4564\n",
      "Epoch #29 : Batch  90/90  -- Loss: 0.281771\n",
      "Average Loss: 0.395278\n",
      "Training F1: 0.6447\n",
      "Test F1: 0.5426\n",
      "Epoch #30 : Batch  90/90  -- Loss: 0.823267\n",
      "Average Loss: 0.378473\n",
      "Training F1: 0.6416\n",
      "Test F1: 0.5773\n",
      "Epoch #31 : Batch  90/90  -- Loss: 0.408918\n",
      "Average Loss: 0.373642\n",
      "Training F1: 0.627\n",
      "Test F1: 0.5\n",
      "Epoch #32 : Batch  90/90  -- Loss: 0.719519\n",
      "Average Loss: 0.381548\n",
      "Training F1: 0.6758\n",
      "Test F1: 0.6111\n",
      "Epoch #33 : Batch  90/90  -- Loss: 0.242559\n",
      "Average Loss: 0.365998\n",
      "Training F1: 0.6624\n",
      "Test F1: 0.5829\n",
      "Epoch #34 : Batch  90/90  -- Loss: 0.464536\n",
      "Average Loss: 0.363987\n",
      "Training F1: 0.6785\n",
      "Test F1: 0.6047\n",
      "Epoch #35 : Batch  90/90  -- Loss: 0.472426\n",
      "Average Loss: 0.363179\n",
      "Training F1: 0.5085\n",
      "Test F1: 0.535\n",
      "Epoch #36 : Batch  90/90  -- Loss: 0.448495\n",
      "Average Loss: 0.363879\n",
      "Training F1: 0.6774\n",
      "Test F1: 0.619\n",
      "Epoch #37 : Batch  90/90  -- Loss: 0.568125\n",
      "Average Loss: 0.358039\n",
      "Training F1: 0.6723\n",
      "Test F1: 0.6117\n",
      "Epoch #38 : Batch  90/90  -- Loss: 0.42281\n",
      "Average Loss: 0.348057\n",
      "Training F1: 0.6831\n",
      "Test F1: 0.6\n",
      "Epoch #39 : Batch  90/90  -- Loss: 0.404152\n",
      "Average Loss: 0.358866\n",
      "Training F1: 0.6651\n",
      "Test F1: 0.6316\n",
      "Epoch #40 : Batch  90/90  -- Loss: 0.072538\n",
      "Average Loss: 0.35401\n",
      "Training F1: 0.6753\n",
      "Test F1: 0.5787\n",
      "Epoch #41 : Batch  90/90  -- Loss: 0.328583\n",
      "Average Loss: 0.357775\n",
      "Training F1: 0.6884\n",
      "Test F1: 0.5829\n",
      "Epoch #42 : Batch  90/90  -- Loss: 0.621755\n",
      "Average Loss: 0.347966\n",
      "Training F1: 0.6807\n",
      "Test F1: 0.5682\n",
      "Epoch #43 : Batch  90/90  -- Loss: 0.147936\n",
      "Average Loss: 0.339904\n",
      "Training F1: 0.7302\n",
      "Test F1: 0.6455\n",
      "Epoch #44 : Batch  90/90  -- Loss: 0.090904\n",
      "Average Loss: 0.327891\n",
      "Training F1: 0.7597\n",
      "Test F1: 0.6774\n",
      "Epoch #45 : Batch  90/90  -- Loss: 0.288775\n",
      "Average Loss: 0.311124\n",
      "Training F1: 0.7739\n",
      "Test F1: 0.6947\n",
      "Epoch #46 : Batch  90/90  -- Loss: 0.579376\n",
      "Average Loss: 0.297405\n",
      "Training F1: 0.8065\n",
      "Test F1: 0.7486\n",
      "Epoch #47 : Batch  90/90  -- Loss: 0.480676\n",
      "Average Loss: 0.28247\n",
      "Training F1: 0.7936\n",
      "Test F1: 0.7891\n",
      "Epoch #48 : Batch  90/90  -- Loss: 0.247444\n",
      "Average Loss: 0.249228\n",
      "Training F1: 0.6253\n",
      "Test F1: 0.5257\n",
      "Epoch #49 : Batch  90/90  -- Loss: 0.0055715\n",
      "Average Loss: 0.206919\n",
      "Training F1: 0.8384\n",
      "Test F1: 0.7619\n",
      "Epoch #50 : Batch  90/90  -- Loss: 0.0213432\n",
      "Average Loss: 0.199264\n",
      "Training F1: 0.9488\n",
      "Test F1: 0.9128\n",
      "Epoch #51 : Batch  90/90  -- Loss: 0.0479457\n",
      "Average Loss: 0.134664\n",
      "Training F1: 0.9501\n",
      "Test F1: 0.9272\n",
      "Epoch #52 : Batch  90/90  -- Loss: 0.0636156\n",
      "Average Loss: 0.13719\n",
      "Training F1: 0.8788\n",
      "Test F1: 0.8774\n",
      "Epoch #53 : Batch  90/90  -- Loss: 0.0046064\n",
      "Average Loss: 0.0837885\n",
      "Training F1: 0.9786\n",
      "Test F1: 0.9595\n",
      "Epoch #54 : Batch  90/90  -- Loss: 0.4387396\n",
      "Average Loss: 0.0938721\n",
      "Training F1: 0.8634\n",
      "Test F1: 0.8447\n",
      "Epoch #55 : Batch  90/90  -- Loss: 0.0228616\n",
      "Average Loss: 0.0744247\n",
      "Training F1: 0.9946\n",
      "Test F1: 1.0\n",
      "Epoch #56 : Batch  90/90  -- Loss: 0.00063677\n",
      "Average Loss: 0.0518467\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n",
      "Epoch #57 : Batch  90/90  -- Loss: 0.0509822\n",
      "Average Loss: 0.0576009\n",
      "Training F1: 0.9973\n",
      "Test F1: 1.0\n",
      "Epoch #58 : Batch  90/90  -- Loss: 0.0095396\n",
      "Average Loss: 0.0273337\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #59 : Batch  90/90  -- Loss: 0.00889896\n",
      "Average Loss: 0.0219522\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #60 : Batch  90/90  -- Loss: 0.00433742\n",
      "Average Loss: 0.0152807\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #61 : Batch  90/90  -- Loss: 0.01742774\n",
      "Average Loss: 0.0154758\n",
      "Training F1: 0.9973\n",
      "Test F1: 1.0\n",
      "Epoch #62 : Batch  90/90  -- Loss: 0.00027857\n",
      "Average Loss: 0.01737\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #63 : Batch  90/90  -- Loss: 0.00114958\n",
      "Average Loss: 0.0344225\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #64 : Batch  90/90  -- Loss: 0.00329559\n",
      "Average Loss: 0.0138462\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #65 : Batch  90/90  -- Loss: 0.00212961\n",
      "Average Loss: 0.0134586\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #66 : Batch  90/90  -- Loss: 0.00125793\n",
      "Average Loss: 0.00565605\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #67 : Batch  90/90  -- Loss: 0.00348581\n",
      "Average Loss: 0.00613605\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #68 : Batch  90/90  -- Loss: 0.00078831\n",
      "Average Loss: 0.00622699\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #69 : Batch  90/90  -- Loss: 0.00081071\n",
      "Average Loss: 0.00396771\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #70 : Batch  90/90  -- Loss: 0.00527256\n",
      "Average Loss: 0.0060285\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #71 : Batch  90/90  -- Loss: 0.08678931\n",
      "Average Loss: 0.00548822\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #72 : Batch  90/90  -- Loss: 0.00018106\n",
      "Average Loss: 0.00747854\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #73 : Batch  90/90  -- Loss: 0.00396438\n",
      "Average Loss: 0.00563648\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #74 : Batch  90/90  -- Loss: 0.01766628\n",
      "Average Loss: 0.00317431\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #75 : Batch  90/90  -- Loss: 2.9891e-05\n",
      "Average Loss: 0.00215894\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #76 : Batch  90/90  -- Loss: 0.00311083\n",
      "Average Loss: 0.00260823\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #77 : Batch  90/90  -- Loss: 0.00012652\n",
      "Average Loss: 0.00318396\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #78 : Batch  90/90  -- Loss: 0.00512971\n",
      "Average Loss: 0.00277851\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #79 : Batch  90/90  -- Loss: 0.00017096\n",
      "Average Loss: 0.00179329\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #80 : Batch  90/90  -- Loss: 0.00502072\n",
      "Average Loss: 0.00564795\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #81 : Batch  90/90  -- Loss: 0.00521728\n",
      "Average Loss: 0.00608607\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #82 : Batch  90/90  -- Loss: 0.00096174\n",
      "Average Loss: 0.0936885\n",
      "Training F1: 0.9635\n",
      "Test F1: 0.9342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #83 : Batch  90/90  -- Loss: 0.00014658\n",
      "Average Loss: 0.0109825\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #84 : Batch  90/90  -- Loss: 0.00124386\n",
      "Average Loss: 0.00317428\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #85 : Batch  90/90  -- Loss: 0.00233798\n",
      "Average Loss: 0.00511538\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #86 : Batch  90/90  -- Loss: 0.00070885\n",
      "Average Loss: 0.00390235\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #87 : Batch  90/90  -- Loss: 9.3216e-05\n",
      "Average Loss: 0.00563968\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #88 : Batch  90/90  -- Loss: 2.3722e-05\n",
      "Average Loss: 0.00190103\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #89 : Batch  90/90  -- Loss: 0.00049761\n",
      "Average Loss: 0.002579\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #90 : Batch  90/90  -- Loss: 0.00051212\n",
      "Average Loss: 0.00335914\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #91 : Batch  90/90  -- Loss: 0.00011241\n",
      "Average Loss: 0.00586544\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #92 : Batch  90/90  -- Loss: 0.00054141\n",
      "Average Loss: 0.00320768\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #93 : Batch  90/90  -- Loss: 4.1096e-05\n",
      "Average Loss: 0.00142087\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #94 : Batch  90/90  -- Loss: 0.00012739\n",
      "Average Loss: 0.0016735\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #95 : Batch  90/90  -- Loss: 0.00026006\n",
      "Average Loss: 0.00188218\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #96 : Batch  90/90  -- Loss: 0.00099626\n",
      "Average Loss: 0.00247832\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #97 : Batch  90/90  -- Loss: 0.00039237\n",
      "Average Loss: 0.000958165\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #98 : Batch  90/90  -- Loss: 0.00036829\n",
      "Average Loss: 0.00134928\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #99 : Batch  90/90  -- Loss: 5.4713e-05\n",
      "Average Loss: 0.00294034\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #100: Batch  90/90  -- Loss: 6.2757e-05\n",
      "Average Loss: 0.00120956\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "transformer = train_transformer(transformer, train_x, train_y, test_x, test_y, epochs=epochs, \n",
    "                  loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline implementation\n",
    "\n",
    "From github: https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:09:03.265665Z",
     "start_time": "2019-07-14T21:09:03.225046Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformer_baseline as tb\n",
    "\n",
    "\n",
    "## Padding masks - for 3 dim input\n",
    "def get_attn_key_pad_mask(seq_k, seq_q, pad=tb.Constants.PAD):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    assert seq_k.dim() == 3 and seq_q.dim() == 3\n",
    "    \n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = torch.all(seq_q.eq(pad), dim=-1)  # b x lq\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "    return padding_mask\n",
    "\n",
    "def get_non_pad_mask(seq, pad=tb.Constants.PAD):\n",
    "    assert seq.dim() == 3\n",
    "    padding_mask = ~torch.all(seq.ne(pad), dim=-1)  # b x l\n",
    "#     print(padding_mask.shape, '....')\n",
    "#     padding_mask = padding_mask.repeat(1, 1, seq.shape[-1])  # b x l x d (repeated)\n",
    "    return padding_mask.type(torch.float).unsqueeze(-1)\n",
    "\n",
    "\n",
    "## Model\n",
    "class TransformerBaseline(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=True, pad=tb.Constants.PAD):\n",
    "        super().__init__()\n",
    "        self.name = 'transformer'\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.encoder = tb.Models.Encoder(\n",
    "            n_src_vocab=in_dim, len_max_seq=max_len,\n",
    "            d_word_vec=model_dim, d_model=model_dim, d_inner=ff_dim,\n",
    "            n_layers=N, n_head=heads, d_k=key_dim, d_v=value_dim,\n",
    "            dropout=0.0, embedding='linear')\n",
    "        \n",
    "        self.fc = nn.Linear(model_dim, out_dim)\n",
    "        \n",
    "        # This was important from their code. \n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        \n",
    "        # encoder requires source sequence & positions of each \n",
    "        x_pos = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0], 1)\n",
    "\n",
    "        # -- Prepare masks\n",
    "        # encoder\n",
    "        e_slf_attn_mask = get_attn_key_pad_mask(seq_k=x, seq_q=x, pad=self.pad)\n",
    "        e_non_pad_mask = torch.ones(x.shape[0], x.shape[1], 1)\n",
    "        e_non_pad_mask = get_non_pad_mask(x, pad=self.pad)\n",
    "        \n",
    "        attn, *_ = self.encoder(x, x_pos, e_slf_attn_mask, e_non_pad_mask)\n",
    "        x = self.fc(attn)\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        return x\n",
    "      \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:09:06.504536Z",
     "start_time": "2019-07-14T21:09:06.492795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2436\n"
     ]
    }
   ],
   "source": [
    "pad = torch.tensor([0,0]).float()\n",
    "\n",
    "baseline = TransformerBaseline(in_dim=2, out_dim=1, N=1, heads=4, model_dim=12, \n",
    "                               key_dim=4, value_dim=3, ff_dim=64, max_len=100, \n",
    "                               batch_first=False, pad=pad)\n",
    "baseline = baseline.to(device)\n",
    "\n",
    "print(baseline.count_parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:11:15.643622Z",
     "start_time": "2019-07-14T21:09:08.748635Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch  90/90  -- Loss: 0.74986\n",
      "Average Loss: 0.636718\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #2  : Batch  90/90  -- Loss: 0.57023\n",
      "Average Loss: 0.578866\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #3  : Batch  90/90  -- Loss: 0.56595\n",
      "Average Loss: 0.581646\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #4  : Batch  90/90  -- Loss: 0.55542\n",
      "Average Loss: 0.57248\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #5  : Batch  90/90  -- Loss: 0.84022\n",
      "Average Loss: 0.574285\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #6  : Batch  90/90  -- Loss: 0.84121\n",
      "Average Loss: 0.579182\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #7  : Batch  90/90  -- Loss: 1.20136\n",
      "Average Loss: 0.574237\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #8  : Batch  90/90  -- Loss: 0.42253\n",
      "Average Loss: 0.573607\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #9  : Batch  90/90  -- Loss: 0.77886\n",
      "Average Loss: 0.573453\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #10 : Batch  90/90  -- Loss: 0.24967\n",
      "Average Loss: 0.564493\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #11 : Batch  90/90  -- Loss: 0.55871\n",
      "Average Loss: 0.557644\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #12 : Batch  90/90  -- Loss: 0.33182\n",
      "Average Loss: 0.550495\n",
      "Training F1: 0.0\n",
      "Test F1: 0.02778\n",
      "Epoch #13 : Batch  90/90  -- Loss: 0.27832\n",
      "Average Loss: 0.521746\n",
      "Training F1: 0.0\n",
      "Test F1: 0.0\n",
      "Epoch #14 : Batch  90/90  -- Loss: 0.36634\n",
      "Average Loss: 0.487824\n",
      "Training F1: 0.528\n",
      "Test F1: 0.4605\n",
      "Epoch #15 : Batch  90/90  -- Loss: 0.38785\n",
      "Average Loss: 0.454398\n",
      "Training F1: 0.5504\n",
      "Test F1: 0.5032\n",
      "Epoch #16 : Batch  90/90  -- Loss: 0.090361\n",
      "Average Loss: 0.42687\n",
      "Training F1: 0.5677\n",
      "Test F1: 0.5062\n",
      "Epoch #17 : Batch  90/90  -- Loss: 0.807296\n",
      "Average Loss: 0.424709\n",
      "Training F1: 0.3827\n",
      "Test F1: 0.2679\n",
      "Epoch #18 : Batch  90/90  -- Loss: 0.26521\n",
      "Average Loss: 0.413669\n",
      "Training F1: 0.6376\n",
      "Test F1: 0.5789\n",
      "Epoch #19 : Batch  90/90  -- Loss: 0.222936\n",
      "Average Loss: 0.394369\n",
      "Training F1: 0.615\n",
      "Test F1: 0.5475\n",
      "Epoch #20 : Batch  90/90  -- Loss: 0.27113\n",
      "Average Loss: 0.384259\n",
      "Training F1: 0.5028\n",
      "Test F1: 0.4234\n",
      "Epoch #21 : Batch  90/90  -- Loss: 0.290464\n",
      "Average Loss: 0.382908\n",
      "Training F1: 0.6784\n",
      "Test F1: 0.6351\n",
      "Epoch #22 : Batch  90/90  -- Loss: 0.52403\n",
      "Average Loss: 0.389125\n",
      "Training F1: 0.6196\n",
      "Test F1: 0.5525\n",
      "Epoch #23 : Batch  90/90  -- Loss: 0.22313\n",
      "Average Loss: 0.371924\n",
      "Training F1: 0.4906\n",
      "Test F1: 0.4186\n",
      "Epoch #24 : Batch  90/90  -- Loss: 0.662017\n",
      "Average Loss: 0.369108\n",
      "Training F1: 0.5269\n",
      "Test F1: 0.4932\n",
      "Epoch #25 : Batch  90/90  -- Loss: 0.63141\n",
      "Average Loss: 0.38109\n",
      "Training F1: 0.6627\n",
      "Test F1: 0.6154\n",
      "Epoch #26 : Batch  90/90  -- Loss: 0.79358\n",
      "Average Loss: 0.373484\n",
      "Training F1: 0.6444\n",
      "Test F1: 0.5851\n",
      "Epoch #27 : Batch  90/90  -- Loss: 0.013204\n",
      "Average Loss: 0.366809\n",
      "Training F1: 0.6797\n",
      "Test F1: 0.6316\n",
      "Epoch #28 : Batch  90/90  -- Loss: 0.337222\n",
      "Average Loss: 0.365401\n",
      "Training F1: 0.6456\n",
      "Test F1: 0.5618\n",
      "Epoch #29 : Batch  90/90  -- Loss: 0.532589\n",
      "Average Loss: 0.362439\n",
      "Training F1: 0.6831\n",
      "Test F1: 0.6422\n",
      "Epoch #30 : Batch  90/90  -- Loss: 0.33396\n",
      "Average Loss: 0.360501\n",
      "Training F1: 0.6507\n",
      "Test F1: 0.5938\n",
      "Epoch #31 : Batch  90/90  -- Loss: 0.26852\n",
      "Average Loss: 0.363167\n",
      "Training F1: 0.5873\n",
      "Test F1: 0.4868\n",
      "Epoch #32 : Batch  90/90  -- Loss: 0.139546\n",
      "Average Loss: 0.353033\n",
      "Training F1: 0.6514\n",
      "Test F1: 0.5587\n",
      "Epoch #33 : Batch  90/90  -- Loss: 0.30798\n",
      "Average Loss: 0.359435\n",
      "Training F1: 0.6623\n",
      "Test F1: 0.603\n",
      "Epoch #34 : Batch  90/90  -- Loss: 0.317371\n",
      "Average Loss: 0.35757\n",
      "Training F1: 0.5078\n",
      "Test F1: 0.5039\n",
      "Epoch #35 : Batch  90/90  -- Loss: 0.342857\n",
      "Average Loss: 0.354449\n",
      "Training F1: 0.4191\n",
      "Test F1: 0.3333\n",
      "Epoch #36 : Batch  90/90  -- Loss: 0.506631\n",
      "Average Loss: 0.367465\n",
      "Training F1: 0.687\n",
      "Test F1: 0.6393\n",
      "Epoch #37 : Batch  90/90  -- Loss: 0.474273\n",
      "Average Loss: 0.346148\n",
      "Training F1: 0.4985\n",
      "Test F1: 0.391\n",
      "Epoch #38 : Batch  90/90  -- Loss: 0.398439\n",
      "Average Loss: 0.36655\n",
      "Training F1: 0.5668\n",
      "Test F1: 0.4768\n",
      "Epoch #39 : Batch  90/90  -- Loss: 0.406218\n",
      "Average Loss: 0.352152\n",
      "Training F1: 0.6909\n",
      "Test F1: 0.6408\n",
      "Epoch #40 : Batch  90/90  -- Loss: 0.020051\n",
      "Average Loss: 0.357036\n",
      "Training F1: 0.6794\n",
      "Test F1: 0.6154\n",
      "Epoch #41 : Batch  90/90  -- Loss: 0.4581563\n",
      "Average Loss: 0.356525\n",
      "Training F1: 0.6929\n",
      "Test F1: 0.6602\n",
      "Epoch #42 : Batch  90/90  -- Loss: 0.010443\n",
      "Average Loss: 0.341254\n",
      "Training F1: 0.6889\n",
      "Test F1: 0.6359\n",
      "Epoch #43 : Batch  90/90  -- Loss: 0.204476\n",
      "Average Loss: 0.333141\n",
      "Training F1: 0.6945\n",
      "Test F1: 0.6481\n",
      "Epoch #44 : Batch  90/90  -- Loss: 0.500398\n",
      "Average Loss: 0.346897\n",
      "Training F1: 0.6036\n",
      "Test F1: 0.5278\n",
      "Epoch #45 : Batch  90/90  -- Loss: 0.187027\n",
      "Average Loss: 0.33427\n",
      "Training F1: 0.6996\n",
      "Test F1: 0.6731\n",
      "Epoch #46 : Batch  90/90  -- Loss: 0.083776\n",
      "Average Loss: 0.33772\n",
      "Training F1: 0.6872\n",
      "Test F1: 0.6396\n",
      "Epoch #47 : Batch  90/90  -- Loss: 0.242067\n",
      "Average Loss: 0.352795\n",
      "Training F1: 0.7063\n",
      "Test F1: 0.6699\n",
      "Epoch #48 : Batch  90/90  -- Loss: 0.341132\n",
      "Average Loss: 0.345165\n",
      "Training F1: 0.7076\n",
      "Test F1: 0.6495\n",
      "Epoch #49 : Batch  90/90  -- Loss: 0.543691\n",
      "Average Loss: 0.338806\n",
      "Training F1: 0.7176\n",
      "Test F1: 0.6597\n",
      "Epoch #50 : Batch  90/90  -- Loss: 0.0046035\n",
      "Average Loss: 0.333852\n",
      "Training F1: 0.7185\n",
      "Test F1: 0.6531\n",
      "Epoch #51 : Batch  90/90  -- Loss: 0.713459\n",
      "Average Loss: 0.340803\n",
      "Training F1: 0.6794\n",
      "Test F1: 0.6264\n",
      "Epoch #52 : Batch  90/90  -- Loss: 0.2820446\n",
      "Average Loss: 0.329176\n",
      "Training F1: 0.7137\n",
      "Test F1: 0.6526\n",
      "Epoch #53 : Batch  90/90  -- Loss: 0.335126\n",
      "Average Loss: 0.330859\n",
      "Training F1: 0.7311\n",
      "Test F1: 0.6735\n",
      "Epoch #54 : Batch  90/90  -- Loss: 0.3403828\n",
      "Average Loss: 0.329316\n",
      "Training F1: 0.7078\n",
      "Test F1: 0.6774\n",
      "Epoch #55 : Batch  90/90  -- Loss: 0.153163\n",
      "Average Loss: 0.32171\n",
      "Training F1: 0.7233\n",
      "Test F1: 0.6667\n",
      "Epoch #56 : Batch  90/90  -- Loss: 0.140556\n",
      "Average Loss: 0.316313\n",
      "Training F1: 0.5357\n",
      "Test F1: 0.5273\n",
      "Epoch #57 : Batch  90/90  -- Loss: 0.113994\n",
      "Average Loss: 0.313538\n",
      "Training F1: 0.7185\n",
      "Test F1: 0.6816\n",
      "Epoch #58 : Batch  90/90  -- Loss: 0.179698\n",
      "Average Loss: 0.318101\n",
      "Training F1: 0.7203\n",
      "Test F1: 0.6931\n",
      "Epoch #59 : Batch  90/90  -- Loss: 0.133184\n",
      "Average Loss: 0.307551\n",
      "Training F1: 0.7475\n",
      "Test F1: 0.6627\n",
      "Epoch #60 : Batch  90/90  -- Loss: 0.426045\n",
      "Average Loss: 0.319938\n",
      "Training F1: 0.7564\n",
      "Test F1: 0.7077\n",
      "Epoch #61 : Batch  90/90  -- Loss: 0.055877\n",
      "Average Loss: 0.299747\n",
      "Training F1: 0.7418\n",
      "Test F1: 0.7041\n",
      "Epoch #62 : Batch  90/90  -- Loss: 0.072526\n",
      "Average Loss: 0.287252\n",
      "Training F1: 0.7953\n",
      "Test F1: 0.7541\n",
      "Epoch #63 : Batch  90/90  -- Loss: 0.297427\n",
      "Average Loss: 0.295762\n",
      "Training F1: 0.7852\n",
      "Test F1: 0.7419\n",
      "Epoch #64 : Batch  90/90  -- Loss: 0.109681\n",
      "Average Loss: 0.274576\n",
      "Training F1: 0.8194\n",
      "Test F1: 0.7541\n",
      "Epoch #65 : Batch  90/90  -- Loss: 0.231592\n",
      "Average Loss: 0.262857\n",
      "Training F1: 0.8294\n",
      "Test F1: 0.7738\n",
      "Epoch #66 : Batch  90/90  -- Loss: 0.384253\n",
      "Average Loss: 0.252135\n",
      "Training F1: 0.8878\n",
      "Test F1: 0.8263\n",
      "Epoch #67 : Batch  90/90  -- Loss: 0.269272\n",
      "Average Loss: 0.246227\n",
      "Training F1: 0.9006\n",
      "Test F1: 0.8828\n",
      "Epoch #68 : Batch  90/90  -- Loss: 0.2106929\n",
      "Average Loss: 0.255793\n",
      "Training F1: 0.8983\n",
      "Test F1: 0.8313\n",
      "Epoch #69 : Batch  90/90  -- Loss: 0.204079\n",
      "Average Loss: 0.244695\n",
      "Training F1: 0.8318\n",
      "Test F1: 0.7624\n",
      "Epoch #70 : Batch  90/90  -- Loss: 0.124182\n",
      "Average Loss: 0.213494\n",
      "Training F1: 0.9602\n",
      "Test F1: 0.9079\n",
      "Epoch #71 : Batch  90/90  -- Loss: 0.150586\n",
      "Average Loss: 0.200673\n",
      "Training F1: 0.8995\n",
      "Test F1: 0.8023\n",
      "Epoch #72 : Batch  90/90  -- Loss: 0.1411248\n",
      "Average Loss: 0.192979\n",
      "Training F1: 0.9418\n",
      "Test F1: 0.902\n",
      "Epoch #73 : Batch  90/90  -- Loss: 0.526095\n",
      "Average Loss: 0.194045\n",
      "Training F1: 0.9289\n",
      "Test F1: 0.807\n",
      "Epoch #74 : Batch  90/90  -- Loss: 0.114128\n",
      "Average Loss: 0.19044\n",
      "Training F1: 0.9223\n",
      "Test F1: 0.8284\n",
      "Epoch #75 : Batch  90/90  -- Loss: 0.472931\n",
      "Average Loss: 0.170241\n",
      "Training F1: 0.9679\n",
      "Test F1: 0.92\n",
      "Epoch #76 : Batch  90/90  -- Loss: 0.059041\n",
      "Average Loss: 0.161826\n",
      "Training F1: 0.9786\n",
      "Test F1: 0.9583\n",
      "Epoch #77 : Batch  90/90  -- Loss: 0.017211\n",
      "Average Loss: 0.154005\n",
      "Training F1: 0.976\n",
      "Test F1: 0.9262\n",
      "Epoch #78 : Batch  90/90  -- Loss: 0.0012605\n",
      "Average Loss: 0.153173\n",
      "Training F1: 0.9918\n",
      "Test F1: 0.9857\n",
      "Epoch #79 : Batch  90/90  -- Loss: 0.250283\n",
      "Average Loss: 0.134118\n",
      "Training F1: 0.9918\n",
      "Test F1: 0.9857\n",
      "Epoch #80 : Batch  90/90  -- Loss: 0.0433769\n",
      "Average Loss: 0.132519\n",
      "Training F1: 0.9919\n",
      "Test F1: 0.9655\n",
      "Epoch #81 : Batch  90/90  -- Loss: 0.0425786\n",
      "Average Loss: 0.130745\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #82 : Batch  90/90  -- Loss: 0.1573321\n",
      "Average Loss: 0.102991\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n",
      "Epoch #83 : Batch  90/90  -- Loss: 0.02472742\n",
      "Average Loss: 0.129153\n",
      "Training F1: 0.9686\n",
      "Test F1: 0.9211\n",
      "Epoch #84 : Batch  90/90  -- Loss: 0.1437944\n",
      "Average Loss: 0.103316\n",
      "Training F1: 0.9918\n",
      "Test F1: 0.9929\n",
      "Epoch #85 : Batch  90/90  -- Loss: 0.0716562\n",
      "Average Loss: 0.102813\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n",
      "Epoch #86 : Batch  90/90  -- Loss: 0.0034786\n",
      "Average Loss: 0.0883656\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n",
      "Epoch #87 : Batch  90/90  -- Loss: 0.00396269\n",
      "Average Loss: 0.0888478\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n",
      "Epoch #88 : Batch  90/90  -- Loss: 0.1994835\n",
      "Average Loss: 0.0626708\n",
      "Training F1: 0.9973\n",
      "Test F1: 0.9929\n",
      "Epoch #89 : Batch  90/90  -- Loss: 0.0534288\n",
      "Average Loss: 0.0649114\n",
      "Training F1: 0.9918\n",
      "Test F1: 0.9784\n",
      "Epoch #90 : Batch  90/90  -- Loss: 0.0287147\n",
      "Average Loss: 0.0736369\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #91 : Batch  90/90  -- Loss: 0.0618817\n",
      "Average Loss: 0.0555237\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #92 : Batch  90/90  -- Loss: 0.2139849\n",
      "Average Loss: 0.0806581\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #93 : Batch  90/90  -- Loss: 0.00699095\n",
      "Average Loss: 0.0501382\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #94 : Batch  90/90  -- Loss: 0.0046618\n",
      "Average Loss: 0.0436299\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #95 : Batch  90/90  -- Loss: 0.00082145\n",
      "Average Loss: 0.0552385\n",
      "Training F1: 1.0\n",
      "Test F1: 0.993\n",
      "Epoch #96 : Batch  90/90  -- Loss: 0.0195052\n",
      "Average Loss: 0.0521057\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #97 : Batch  90/90  -- Loss: 0.0540654\n",
      "Average Loss: 0.0421558\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #98 : Batch  90/90  -- Loss: 0.00011166\n",
      "Average Loss: 0.0296707\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #99 : Batch  90/90  -- Loss: 0.0095043\n",
      "Average Loss: 0.0552261\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "Epoch #100: Batch  90/90  -- Loss: 0.00330315\n",
      "Average Loss: 0.0476405\n",
      "Training F1: 1.0\n",
      "Test F1: 0.9861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerBaseline(\n",
       "  (encoder): Encoder(\n",
       "    (src_word_emb): Linear(in_features=2, out_features=12, bias=True)\n",
       "    (position_enc): Embedding(101, 12)\n",
       "    (layer_stack): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=12, out_features=16, bias=True)\n",
       "          (w_ks): Linear(in_features=12, out_features=16, bias=True)\n",
       "          (w_vs): Linear(in_features=12, out_features=12, bias=True)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (softmax): Softmax()\n",
       "          )\n",
       "          (layer_norm): LayerNorm(torch.Size([12]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Conv1d(12, 64, kernel_size=(1,), stride=(1,))\n",
       "          (w_2): Conv1d(64, 12, kernel_size=(1,), stride=(1,))\n",
       "          (layer_norm): LayerNorm(torch.Size([12]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=12, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = train_transformer(baseline, train_x, train_y, test_x, test_y, epochs=epochs, \n",
    "                  loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "237.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
