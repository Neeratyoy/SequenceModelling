{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:12:56.433263Z",
     "start_time": "2019-07-14T21:12:55.681059Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:12:56.443567Z",
     "start_time": "2019-07-14T21:12:56.435073Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "batch_size = 8\n",
    "# Percentage of training data\n",
    "learning_rate = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse-copy task dataset\n",
    "\n",
    "The entire dataset comprises of the binary representation of all numbers uptil a range defined. The binary sequence from left to right (most significant to least significant) is the input. The target is just the reverse sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:12:56.614599Z",
     "start_time": "2019-07-14T21:12:56.446486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Generating data\n",
    "state_size = 12\n",
    "data_x = []\n",
    "for i in range(pow(2, state_size)):\n",
    "    data_x.append([int(x) for x in list(np.binary_repr(i, width=state_size))])\n",
    "data_x = np.array(data_x)\n",
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:12:56.672059Z",
     "start_time": "2019-07-14T21:12:56.616626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 4096, 2]), torch.Size([12, 4096, 2]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping for tensors\n",
    "data_x = np.transpose(data_x).reshape(state_size, pow(2, state_size), 1)\n",
    "data_x = torch.from_numpy(data_x).float()\n",
    "data_x = torch.zeros(data_x.shape[0], data_x.shape[1], 2).scatter_(2, data_x.long(), 1)\n",
    "data_y = data_x.clone()\n",
    "data_x.shape, data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:12:56.700525Z",
     "start_time": "2019-07-14T21:12:56.673917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute for reverse-copy (comment for copy-task)\n",
    "data_y = torch.flip(data_y, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:12:56.745603Z",
     "start_time": "2019-07-14T21:12:56.703270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3072, 2]) torch.Size([12, 3072, 2]) torch.Size([12, 1024, 2]) torch.Size([12, 1024, 2])\n"
     ]
    }
   ],
   "source": [
    "# Creating training and test sets\n",
    "train_size = 0.75\n",
    "ordering = torch.randperm(pow(2, state_size))\n",
    "data_x = data_x[:, ordering, :]\n",
    "data_y = data_y[:, ordering, :]\n",
    "train_x = data_x[:,:int(train_size * len(ordering)),:]\n",
    "train_y = data_y[:,:int(train_size * len(ordering)),:]\n",
    "test_x = data_x[:,int(train_size * len(ordering)):,:]\n",
    "test_y = data_y[:,int(train_size * len(ordering)):,:]\n",
    "\n",
    "# Creating training and validation sets\n",
    "## TODO\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T22:55:01.653027Z",
     "start_time": "2019-07-10T22:55:01.648057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input dim\n",
    "input_dim = 2\n",
    "# Number of hidden nodes\n",
    "hidden_dim = 16\n",
    "# Number of output nodes\n",
    "output_dim = 2\n",
    "# Number of LSTMs cells to be stacked\n",
    "layers = 1\n",
    "# Boolean value for bidirectioanl or not\n",
    "bidirectional = True\n",
    "# Boolean value to use LayerNorm or not\n",
    "layernorm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if train_x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    layers = model.layers\n",
    "    hidden_dim = model.hidden_dim\n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[:,ordering,:]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            if model.bidirectional:\n",
    "                hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            else:\n",
    "                hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "                \n",
    "            o = model(train_x[:,start:end,:], hidden_state, cell_state)\n",
    "            gt = torch.argmax(train_y[:,start:end,:], 2, keepdim=True).view(-1)\n",
    "            loss = loss_fn(o.view(-1, train_x.shape[-1]), gt)            \n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size), \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate(model, train_x, train_y)\n",
    "        f1_test = evaluate(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]\n",
    "    device = torch.device(\"cpu\")\n",
    "    if x.is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    layers = model.layers\n",
    "    hidden_dim = model.hidden_dim\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        if model.bidirectional:\n",
    "            hidden_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(2 * layers, batch, hidden_dim).to(device)\n",
    "        else:\n",
    "            hidden_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "            cell_state = torch.zeros(layers, batch, hidden_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:], hidden_state, cell_state)\n",
    "        pred = torch.argmax(o, 2, keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        label = torch.argmax(y[:,start:end,:], 2, \n",
    "                             keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        labels.extend(label)\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "\n",
    "class LSTMSeq2SeqSame(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence to Sequence (many-to-many same)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: input dimensions\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, layers=layers,\n",
    "                         bidirectional=bidirectional, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        output, (_, _) = self.lstm(x, hidden_state, cell_state)\n",
    "        orig_dims = output.shape\n",
    "        # fc computation for each element\n",
    "        output = self.fc(output.view(-1, output.shape[-1]))  \n",
    "        # reshaping to have (seq_len, batch, output)\n",
    "        output = output.view(orig_dims[0], orig_dims[1], output.shape[1])  \n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498\n"
     ]
    }
   ],
   "source": [
    "our = LSTMSeq2SeqSame(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(our.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(our.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.64726\n",
      "Average Loss: 0.654597\n",
      "Training F1: 0.6457\n",
      "Test F1: 0.6415\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.62763\n",
      "Average Loss: 0.627149\n",
      "Training F1: 0.6482\n",
      "Test F1: 0.6483\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.61301\n",
      "Average Loss: 0.620728\n",
      "Training F1: 0.6592\n",
      "Test F1: 0.6543\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.54931\n",
      "Average Loss: 0.604173\n",
      "Training F1: 0.6756\n",
      "Test F1: 0.6758\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.57186\n",
      "Average Loss: 0.588624\n",
      "Training F1: 0.6523\n",
      "Test F1: 0.6496\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.57483\n",
      "Average Loss: 0.57562\n",
      "Training F1: 0.7473\n",
      "Test F1: 0.7428\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.54522\n",
      "Average Loss: 0.564409\n",
      "Training F1: 0.7227\n",
      "Test F1: 0.7208\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.60681\n",
      "Average Loss: 0.554765\n",
      "Training F1: 0.7592\n",
      "Test F1: 0.7528\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.53719\n",
      "Average Loss: 0.544467\n",
      "Training F1: 0.7726\n",
      "Test F1: 0.7628\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.55513\n",
      "Average Loss: 0.537218\n",
      "Training F1: 0.779\n",
      "Test F1: 0.7754\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.47089\n",
      "Average Loss: 0.528701\n",
      "Training F1: 0.778\n",
      "Test F1: 0.7742\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.51519\n",
      "Average Loss: 0.522769\n",
      "Training F1: 0.7937\n",
      "Test F1: 0.7877\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.52076\n",
      "Average Loss: 0.518839\n",
      "Training F1: 0.8013\n",
      "Test F1: 0.796\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.51426\n",
      "Average Loss: 0.511266\n",
      "Training F1: 0.7982\n",
      "Test F1: 0.7921\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.49483\n",
      "Average Loss: 0.506767\n",
      "Training F1: 0.8101\n",
      "Test F1: 0.802\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.51134\n",
      "Average Loss: 0.497265\n",
      "Training F1: 0.8195\n",
      "Test F1: 0.8158\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.55044\n",
      "Average Loss: 0.488772\n",
      "Training F1: 0.8081\n",
      "Test F1: 0.8043\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.49272\n",
      "Average Loss: 0.481073\n",
      "Training F1: 0.8412\n",
      "Test F1: 0.8375\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.47252\n",
      "Average Loss: 0.474247\n",
      "Training F1: 0.8431\n",
      "Test F1: 0.8402\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.49423\n",
      "Average Loss: 0.468778\n",
      "Training F1: 0.8529\n",
      "Test F1: 0.8493\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.46468\n",
      "Average Loss: 0.465336\n",
      "Training F1: 0.8594\n",
      "Test F1: 0.8596\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.41227\n",
      "Average Loss: 0.460076\n",
      "Training F1: 0.87\n",
      "Test F1: 0.8673\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.48365\n",
      "Average Loss: 0.454462\n",
      "Training F1: 0.869\n",
      "Test F1: 0.8665\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.43838\n",
      "Average Loss: 0.449948\n",
      "Training F1: 0.8741\n",
      "Test F1: 0.8739\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.47091\n",
      "Average Loss: 0.443903\n",
      "Training F1: 0.8602\n",
      "Test F1: 0.8592\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.47978\n",
      "Average Loss: 0.440063\n",
      "Training F1: 0.886\n",
      "Test F1: 0.8846\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.45017\n",
      "Average Loss: 0.431839\n",
      "Training F1: 0.8925\n",
      "Test F1: 0.8905\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.38627\n",
      "Average Loss: 0.427754\n",
      "Training F1: 0.8945\n",
      "Test F1: 0.8929\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.47781\n",
      "Average Loss: 0.423815\n",
      "Training F1: 0.9022\n",
      "Test F1: 0.9013\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.36905\n",
      "Average Loss: 0.42017\n",
      "Training F1: 0.9032\n",
      "Test F1: 0.9023\n",
      "==================================================\n",
      "Epoch #31 : Batch 384/384 -- Loss: 0.43229\n",
      "Average Loss: 0.416297\n",
      "Training F1: 0.9035\n",
      "Test F1: 0.9023\n",
      "==================================================\n",
      "Epoch #32 : Batch 384/384 -- Loss: 0.40158\n",
      "Average Loss: 0.410933\n",
      "Training F1: 0.9154\n",
      "Test F1: 0.9145\n",
      "==================================================\n",
      "Epoch #33 : Batch 384/384 -- Loss: 0.40231\n",
      "Average Loss: 0.408359\n",
      "Training F1: 0.9174\n",
      "Test F1: 0.9161\n",
      "==================================================\n",
      "Epoch #34 : Batch 384/384 -- Loss: 0.44418\n",
      "Average Loss: 0.407367\n",
      "Training F1: 0.9191\n",
      "Test F1: 0.9181\n",
      "==================================================\n",
      "Epoch #35 : Batch 384/384 -- Loss: 0.43083\n",
      "Average Loss: 0.401017\n",
      "Training F1: 0.917\n",
      "Test F1: 0.9171\n",
      "==================================================\n",
      "Epoch #36 : Batch 384/384 -- Loss: 0.41878\n",
      "Average Loss: 0.398368\n",
      "Training F1: 0.9238\n",
      "Test F1: 0.924\n",
      "==================================================\n",
      "Epoch #37 : Batch 384/384 -- Loss: 0.37957\n",
      "Average Loss: 0.394089\n",
      "Training F1: 0.9274\n",
      "Test F1: 0.9289\n",
      "==================================================\n",
      "Epoch #38 : Batch 384/384 -- Loss: 0.41456\n",
      "Average Loss: 0.393064\n",
      "Training F1: 0.924\n",
      "Test F1: 0.9237\n",
      "==================================================\n",
      "Epoch #39 : Batch 384/384 -- Loss: 0.38092\n",
      "Average Loss: 0.389735\n",
      "Training F1: 0.9325\n",
      "Test F1: 0.9337\n",
      "==================================================\n",
      "Epoch #40 : Batch 384/384 -- Loss: 0.34477\n",
      "Average Loss: 0.386608\n",
      "Training F1: 0.9336\n",
      "Test F1: 0.9339\n",
      "==================================================\n",
      "Epoch #41 : Batch 384/384 -- Loss: 0.38398\n",
      "Average Loss: 0.384158\n",
      "Training F1: 0.9357\n",
      "Test F1: 0.9363\n",
      "==================================================\n",
      "Epoch #42 : Batch 384/384 -- Loss: 0.37419\n",
      "Average Loss: 0.383316\n",
      "Training F1: 0.9326\n",
      "Test F1: 0.9331\n",
      "==================================================\n",
      "Epoch #43 : Batch 384/384 -- Loss: 0.40865\n",
      "Average Loss: 0.381428\n",
      "Training F1: 0.9307\n",
      "Test F1: 0.9302\n",
      "==================================================\n",
      "Epoch #44 : Batch 384/384 -- Loss: 0.39488\n",
      "Average Loss: 0.379963\n",
      "Training F1: 0.9369\n",
      "Test F1: 0.9386\n",
      "==================================================\n",
      "Epoch #45 : Batch 384/384 -- Loss: 0.39178\n",
      "Average Loss: 0.380476\n",
      "Training F1: 0.9414\n",
      "Test F1: 0.9432\n",
      "==================================================\n",
      "Epoch #46 : Batch 384/384 -- Loss: 0.34482\n",
      "Average Loss: 0.38059\n",
      "Training F1: 0.9375\n",
      "Test F1: 0.9389\n",
      "==================================================\n",
      "Epoch #47 : Batch 384/384 -- Loss: 0.37509\n",
      "Average Loss: 0.374768\n",
      "Training F1: 0.9432\n",
      "Test F1: 0.9443\n",
      "==================================================\n",
      "Epoch #48 : Batch 384/384 -- Loss: 0.40796\n",
      "Average Loss: 0.374586\n",
      "Training F1: 0.9422\n",
      "Test F1: 0.9431\n",
      "==================================================\n",
      "Epoch #49 : Batch 384/384 -- Loss: 0.33888\n",
      "Average Loss: 0.372132\n",
      "Training F1: 0.9453\n",
      "Test F1: 0.9461\n",
      "==================================================\n",
      "Epoch #50 : Batch 384/384 -- Loss: 0.38009\n",
      "Average Loss: 0.371768\n",
      "Training F1: 0.9441\n",
      "Test F1: 0.9458\n",
      "==================================================\n",
      "Epoch #51 : Batch 384/384 -- Loss: 0.35216\n",
      "Average Loss: 0.370955\n",
      "Training F1: 0.9428\n",
      "Test F1: 0.9444\n",
      "==================================================\n",
      "Epoch #52 : Batch 384/384 -- Loss: 0.34432\n",
      "Average Loss: 0.373925\n",
      "Training F1: 0.9472\n",
      "Test F1: 0.9473\n",
      "==================================================\n",
      "Epoch #53 : Batch 384/384 -- Loss: 0.36883\n",
      "Average Loss: 0.368892\n",
      "Training F1: 0.9485\n",
      "Test F1: 0.9492\n",
      "==================================================\n",
      "Epoch #54 : Batch 384/384 -- Loss: 0.38301\n",
      "Average Loss: 0.368\n",
      "Training F1: 0.9475\n",
      "Test F1: 0.9488\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55 : Batch 384/384 -- Loss: 0.34667\n",
      "Average Loss: 0.367135\n",
      "Training F1: 0.9486\n",
      "Test F1: 0.9493\n",
      "==================================================\n",
      "Epoch #56 : Batch 384/384 -- Loss: 0.37733\n",
      "Average Loss: 0.367866\n",
      "Training F1: 0.949\n",
      "Test F1: 0.9496\n",
      "==================================================\n",
      "Epoch #57 : Batch 384/384 -- Loss: 0.35764\n",
      "Average Loss: 0.367203\n",
      "Training F1: 0.9491\n",
      "Test F1: 0.9497\n",
      "==================================================\n",
      "Epoch #58 : Batch 384/384 -- Loss: 0.37835\n",
      "Average Loss: 0.365407\n",
      "Training F1: 0.9499\n",
      "Test F1: 0.9503\n",
      "==================================================\n",
      "Epoch #59 : Batch 384/384 -- Loss: 0.39697\n",
      "Average Loss: 0.366656\n",
      "Training F1: 0.9504\n",
      "Test F1: 0.9511\n",
      "==================================================\n",
      "Epoch #60 : Batch 384/384 -- Loss: 0.36733\n",
      "Average Loss: 0.36454\n",
      "Training F1: 0.9526\n",
      "Test F1: 0.953\n",
      "==================================================\n",
      "Epoch #61 : Batch 384/384 -- Loss: 0.34664\n",
      "Average Loss: 0.362859\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #62 : Batch 384/384 -- Loss: 0.32481\n",
      "Average Loss: 0.364039\n",
      "Training F1: 0.9527\n",
      "Test F1: 0.9532\n",
      "==================================================\n",
      "Epoch #63 : Batch 384/384 -- Loss: 0.34672\n",
      "Average Loss: 0.362009\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #64 : Batch 384/384 -- Loss: 0.35545\n",
      "Average Loss: 0.362458\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #65 : Batch 384/384 -- Loss: 0.36572\n",
      "Average Loss: 0.364259\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #66 : Batch 384/384 -- Loss: 0.36726\n",
      "Average Loss: 0.361424\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #67 : Batch 384/384 -- Loss: 0.35589\n",
      "Average Loss: 0.361164\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #68 : Batch 384/384 -- Loss: 0.36572\n",
      "Average Loss: 0.361104\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #69 : Batch 384/384 -- Loss: 0.36577\n",
      "Average Loss: 0.360929\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #70 : Batch 384/384 -- Loss: 0.38674\n",
      "Average Loss: 0.364714\n",
      "Training F1: 0.9524\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #71 : Batch 384/384 -- Loss: 0.35533\n",
      "Average Loss: 0.362751\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #72 : Batch 384/384 -- Loss: 0.35519\n",
      "Average Loss: 0.360825\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #73 : Batch 384/384 -- Loss: 0.38661\n",
      "Average Loss: 0.360774\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #74 : Batch 384/384 -- Loss: 0.35525\n",
      "Average Loss: 0.360728\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #75 : Batch 384/384 -- Loss: 0.38918\n",
      "Average Loss: 0.361622\n",
      "Training F1: 0.917\n",
      "Test F1: 0.9155\n",
      "==================================================\n",
      "Epoch #76 : Batch 384/384 -- Loss: 0.36571\n",
      "Average Loss: 0.366904\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #77 : Batch 384/384 -- Loss: 0.34485\n",
      "Average Loss: 0.360717\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #78 : Batch 384/384 -- Loss: 0.37597\n",
      "Average Loss: 0.360667\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #79 : Batch 384/384 -- Loss: 0.34469\n",
      "Average Loss: 0.360637\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #80 : Batch 384/384 -- Loss: 0.34476\n",
      "Average Loss: 0.360605\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #81 : Batch 384/384 -- Loss: 0.36602\n",
      "Average Loss: 0.366963\n",
      "Training F1: 0.9511\n",
      "Test F1: 0.9519\n",
      "==================================================\n",
      "Epoch #82 : Batch 384/384 -- Loss: 0.34474\n",
      "Average Loss: 0.360824\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #83 : Batch 384/384 -- Loss: 0.36575\n",
      "Average Loss: 0.360619\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #84 : Batch 384/384 -- Loss: 0.33422\n",
      "Average Loss: 0.360585\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #85 : Batch 384/384 -- Loss: 0.36553\n",
      "Average Loss: 0.36056\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #86 : Batch 384/384 -- Loss: 0.38635\n",
      "Average Loss: 0.360557\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #87 : Batch 384/384 -- Loss: 0.34475\n",
      "Average Loss: 0.36053\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #88 : Batch 384/384 -- Loss: 0.34461\n",
      "Average Loss: 0.360517\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #89 : Batch 384/384 -- Loss: 0.34455\n",
      "Average Loss: 0.360503\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #90 : Batch 384/384 -- Loss: 0.35498\n",
      "Average Loss: 0.36049\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #91 : Batch 384/384 -- Loss: 0.36543\n",
      "Average Loss: 0.360492\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #92 : Batch 384/384 -- Loss: 0.35511\n",
      "Average Loss: 0.360469\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #93 : Batch 384/384 -- Loss: 0.34457\n",
      "Average Loss: 0.373902\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #94 : Batch 384/384 -- Loss: 0.36568\n",
      "Average Loss: 0.360549\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #95 : Batch 384/384 -- Loss: 0.37592\n",
      "Average Loss: 0.360509\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #96 : Batch 384/384 -- Loss: 0.37588\n",
      "Average Loss: 0.36049\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #97 : Batch 384/384 -- Loss: 0.34467\n",
      "Average Loss: 0.360478\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #98 : Batch 384/384 -- Loss: 0.35514\n",
      "Average Loss: 0.360468\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #99 : Batch 384/384 -- Loss: 0.37588\n",
      "Average Loss: 0.360461\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n",
      "Epoch #100: Batch 384/384 -- Loss: 0.37589\n",
      "Average Loss: 0.360454\n",
      "Training F1: 0.9528\n",
      "Test F1: 0.9533\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMSeq2SeqSame(\n",
       "  (lstm): LSTM(\n",
       "    (model): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (model_rev): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (g1): Sigmoid()\n",
       "        (g2): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(our, train_x, train_y, test_x, test_y, epochs=100, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchBaseline(nn.Module):\n",
    "    \"\"\" LSTM Class for Sequence to Sequence (many-to-many same)\n",
    "\n",
    "    The class creates the LSTM architecture as specified by the parameters.\n",
    "    A fully connected layer is added to reduce the last hidden state to output_dim.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    input_dim: input dimensions\n",
    "    hidden_dim: number of hidden nodes required\n",
    "    output_dim: numer of output nodes required (1 for sentiment analysis)\n",
    "    layers: number of LSTM cells to be stacked for depth\n",
    "    bidirectional: boolean\n",
    "    layernorm: boolean\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 layers=1, bidirectional=False, layernorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=layers,\n",
    "                         bidirectional=bidirectional) #, layernorm=layernorm)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        output, (_, _) = self.lstm(x, (hidden_state, cell_state))\n",
    "        orig_dims = output.shape\n",
    "        # fc computation for each element\n",
    "        output = self.fc(output.view(-1, output.shape[-1]))  \n",
    "        # reshaping to have (seq_len, batch, output)\n",
    "        output = output.view(orig_dims[0], orig_dims[1], output.shape[1])  \n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        tot_sum += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "        return tot_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2626\n"
     ]
    }
   ],
   "source": [
    "pytorch = PyTorchBaseline(input_dim, hidden_dim, output_dim, bidirectional=bidirectional, layers=layers).to(device)\n",
    "print(pytorch.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pytorch.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.62225\n",
      "Average Loss: 0.652653\n",
      "Training F1: 0.6388\n",
      "Test F1: 0.6373\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.60822\n",
      "Average Loss: 0.626826\n",
      "Training F1: 0.6306\n",
      "Test F1: 0.6264\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.62273\n",
      "Average Loss: 0.613017\n",
      "Training F1: 0.7228\n",
      "Test F1: 0.7238\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.57933\n",
      "Average Loss: 0.575599\n",
      "Training F1: 0.7427\n",
      "Test F1: 0.7443\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.56489\n",
      "Average Loss: 0.556984\n",
      "Training F1: 0.7554\n",
      "Test F1: 0.7574\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.53143\n",
      "Average Loss: 0.540668\n",
      "Training F1: 0.7786\n",
      "Test F1: 0.7814\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.54065\n",
      "Average Loss: 0.525078\n",
      "Training F1: 0.7945\n",
      "Test F1: 0.7987\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.45112\n",
      "Average Loss: 0.512275\n",
      "Training F1: 0.8036\n",
      "Test F1: 0.8036\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.58038\n",
      "Average Loss: 0.503921\n",
      "Training F1: 0.8117\n",
      "Test F1: 0.8129\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.50724\n",
      "Average Loss: 0.496757\n",
      "Training F1: 0.8219\n",
      "Test F1: 0.8207\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.51304\n",
      "Average Loss: 0.490108\n",
      "Training F1: 0.8307\n",
      "Test F1: 0.8295\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.48816\n",
      "Average Loss: 0.48469\n",
      "Training F1: 0.8312\n",
      "Test F1: 0.8329\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.44484\n",
      "Average Loss: 0.478059\n",
      "Training F1: 0.8311\n",
      "Test F1: 0.8334\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.44137\n",
      "Average Loss: 0.47345\n",
      "Training F1: 0.8479\n",
      "Test F1: 0.8467\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.47246\n",
      "Average Loss: 0.467306\n",
      "Training F1: 0.8438\n",
      "Test F1: 0.8452\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.43124\n",
      "Average Loss: 0.464568\n",
      "Training F1: 0.8359\n",
      "Test F1: 0.8357\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.42397\n",
      "Average Loss: 0.458333\n",
      "Training F1: 0.8602\n",
      "Test F1: 0.8585\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.44871\n",
      "Average Loss: 0.45537\n",
      "Training F1: 0.8613\n",
      "Test F1: 0.8599\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.47323\n",
      "Average Loss: 0.452947\n",
      "Training F1: 0.8677\n",
      "Test F1: 0.8671\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.45399\n",
      "Average Loss: 0.446166\n",
      "Training F1: 0.8684\n",
      "Test F1: 0.8679\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.47711\n",
      "Average Loss: 0.440999\n",
      "Training F1: 0.8604\n",
      "Test F1: 0.8595\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.46203\n",
      "Average Loss: 0.438085\n",
      "Training F1: 0.8812\n",
      "Test F1: 0.8799\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.42223\n",
      "Average Loss: 0.432459\n",
      "Training F1: 0.8884\n",
      "Test F1: 0.8884\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.41548\n",
      "Average Loss: 0.427706\n",
      "Training F1: 0.8908\n",
      "Test F1: 0.8902\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.41362\n",
      "Average Loss: 0.423024\n",
      "Training F1: 0.8962\n",
      "Test F1: 0.8947\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.40151\n",
      "Average Loss: 0.419328\n",
      "Training F1: 0.8986\n",
      "Test F1: 0.8992\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.38219\n",
      "Average Loss: 0.412427\n",
      "Training F1: 0.9109\n",
      "Test F1: 0.9115\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.39976\n",
      "Average Loss: 0.407175\n",
      "Training F1: 0.9093\n",
      "Test F1: 0.911\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.42541\n",
      "Average Loss: 0.402318\n",
      "Training F1: 0.92\n",
      "Test F1: 0.9195\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.35103\n",
      "Average Loss: 0.396473\n",
      "Training F1: 0.9242\n",
      "Test F1: 0.924\n",
      "==================================================\n",
      "Epoch #31 : Batch 384/384 -- Loss: 0.38661\n",
      "Average Loss: 0.390556\n",
      "Training F1: 0.9316\n",
      "Test F1: 0.9318\n",
      "==================================================\n",
      "Epoch #32 : Batch 384/384 -- Loss: 0.41574\n",
      "Average Loss: 0.385928\n",
      "Training F1: 0.9379\n",
      "Test F1: 0.939\n",
      "==================================================\n",
      "Epoch #33 : Batch 384/384 -- Loss: 0.38541\n",
      "Average Loss: 0.3816\n",
      "Training F1: 0.9419\n",
      "Test F1: 0.9445\n",
      "==================================================\n",
      "Epoch #34 : Batch 384/384 -- Loss: 0.44689\n",
      "Average Loss: 0.377601\n",
      "Training F1: 0.9458\n",
      "Test F1: 0.9469\n",
      "==================================================\n",
      "Epoch #35 : Batch 384/384 -- Loss: 0.36415\n",
      "Average Loss: 0.373829\n",
      "Training F1: 0.948\n",
      "Test F1: 0.9484\n",
      "==================================================\n",
      "Epoch #36 : Batch 384/384 -- Loss: 0.38424\n",
      "Average Loss: 0.370581\n",
      "Training F1: 0.9554\n",
      "Test F1: 0.9569\n",
      "==================================================\n",
      "Epoch #37 : Batch 384/384 -- Loss: 0.40014\n",
      "Average Loss: 0.367286\n",
      "Training F1: 0.9582\n",
      "Test F1: 0.9607\n",
      "==================================================\n",
      "Epoch #38 : Batch 384/384 -- Loss: 0.35646\n",
      "Average Loss: 0.363616\n",
      "Training F1: 0.9565\n",
      "Test F1: 0.9588\n",
      "==================================================\n",
      "Epoch #39 : Batch 384/384 -- Loss: 0.36136\n",
      "Average Loss: 0.360911\n",
      "Training F1: 0.9567\n",
      "Test F1: 0.9613\n",
      "==================================================\n",
      "Epoch #40 : Batch 384/384 -- Loss: 0.36251\n",
      "Average Loss: 0.35733\n",
      "Training F1: 0.9559\n",
      "Test F1: 0.9582\n",
      "==================================================\n",
      "Epoch #41 : Batch 384/384 -- Loss: 0.34696\n",
      "Average Loss: 0.353503\n",
      "Training F1: 0.9698\n",
      "Test F1: 0.9733\n",
      "==================================================\n",
      "Epoch #42 : Batch 384/384 -- Loss: 0.34882\n",
      "Average Loss: 0.350323\n",
      "Training F1: 0.9703\n",
      "Test F1: 0.9727\n",
      "==================================================\n",
      "Epoch #43 : Batch 384/384 -- Loss: 0.34145\n",
      "Average Loss: 0.345448\n",
      "Training F1: 0.9743\n",
      "Test F1: 0.9767\n",
      "==================================================\n",
      "Epoch #44 : Batch 384/384 -- Loss: 0.33145\n",
      "Average Loss: 0.345236\n",
      "Training F1: 0.9756\n",
      "Test F1: 0.9786\n",
      "==================================================\n",
      "Epoch #45 : Batch 384/384 -- Loss: 0.33106\n",
      "Average Loss: 0.340974\n",
      "Training F1: 0.9804\n",
      "Test F1: 0.982\n",
      "==================================================\n",
      "Epoch #46 : Batch 384/384 -- Loss: 0.32256\n",
      "Average Loss: 0.340257\n",
      "Training F1: 0.9788\n",
      "Test F1: 0.9804\n",
      "==================================================\n",
      "Epoch #47 : Batch 384/384 -- Loss: 0.35641\n",
      "Average Loss: 0.337354\n",
      "Training F1: 0.9808\n",
      "Test F1: 0.9821\n",
      "==================================================\n",
      "Epoch #48 : Batch 384/384 -- Loss: 0.31595\n",
      "Average Loss: 0.336681\n",
      "Training F1: 0.982\n",
      "Test F1: 0.9843\n",
      "==================================================\n",
      "Epoch #49 : Batch 384/384 -- Loss: 0.33704\n",
      "Average Loss: 0.33666\n",
      "Training F1: 0.9814\n",
      "Test F1: 0.9832\n",
      "==================================================\n",
      "Epoch #50 : Batch 384/384 -- Loss: 0.35162\n",
      "Average Loss: 0.336783\n",
      "Training F1: 0.9747\n",
      "Test F1: 0.9764\n",
      "==================================================\n",
      "Epoch #51 : Batch 384/384 -- Loss: 0.32562\n",
      "Average Loss: 0.335092\n",
      "Training F1: 0.9845\n",
      "Test F1: 0.9856\n",
      "==================================================\n",
      "Epoch #52 : Batch 384/384 -- Loss: 0.33382\n",
      "Average Loss: 0.334338\n",
      "Training F1: 0.9743\n",
      "Test F1: 0.9755\n",
      "==================================================\n",
      "Epoch #53 : Batch 384/384 -- Loss: 0.33178\n",
      "Average Loss: 0.331483\n",
      "Training F1: 0.9863\n",
      "Test F1: 0.9876\n",
      "==================================================\n",
      "Epoch #54 : Batch 384/384 -- Loss: 0.31473\n",
      "Average Loss: 0.331309\n",
      "Training F1: 0.9872\n",
      "Test F1: 0.9888\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55 : Batch 384/384 -- Loss: 0.32171\n",
      "Average Loss: 0.330995\n",
      "Training F1: 0.9874\n",
      "Test F1: 0.9883\n",
      "==================================================\n",
      "Epoch #56 : Batch 384/384 -- Loss: 0.31439\n",
      "Average Loss: 0.330461\n",
      "Training F1: 0.9866\n",
      "Test F1: 0.9876\n",
      "==================================================\n",
      "Epoch #57 : Batch 384/384 -- Loss: 0.31405\n",
      "Average Loss: 0.328323\n",
      "Training F1: 0.9882\n",
      "Test F1: 0.9892\n",
      "==================================================\n",
      "Epoch #58 : Batch 384/384 -- Loss: 0.31426\n",
      "Average Loss: 0.327092\n",
      "Training F1: 0.9884\n",
      "Test F1: 0.9892\n",
      "==================================================\n",
      "Epoch #59 : Batch 384/384 -- Loss: 0.32476\n",
      "Average Loss: 0.329888\n",
      "Training F1: 0.9889\n",
      "Test F1: 0.9901\n",
      "==================================================\n",
      "Epoch #60 : Batch 384/384 -- Loss: 0.32083\n",
      "Average Loss: 0.327942\n",
      "Training F1: 0.9871\n",
      "Test F1: 0.9874\n",
      "==================================================\n",
      "Epoch #61 : Batch 384/384 -- Loss: 0.31494\n",
      "Average Loss: 0.327057\n",
      "Training F1: 0.9896\n",
      "Test F1: 0.99\n",
      "==================================================\n",
      "Epoch #62 : Batch 384/384 -- Loss: 0.33469\n",
      "Average Loss: 0.324783\n",
      "Training F1: 0.9896\n",
      "Test F1: 0.9909\n",
      "==================================================\n",
      "Epoch #63 : Batch 384/384 -- Loss: 0.31474\n",
      "Average Loss: 0.328818\n",
      "Training F1: 0.9905\n",
      "Test F1: 0.9914\n",
      "==================================================\n",
      "Epoch #64 : Batch 384/384 -- Loss: 0.32407\n",
      "Average Loss: 0.324127\n",
      "Training F1: 0.9831\n",
      "Test F1: 0.9854\n",
      "==================================================\n",
      "Epoch #65 : Batch 384/384 -- Loss: 0.31356\n",
      "Average Loss: 0.324933\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #66 : Batch 384/384 -- Loss: 0.33832\n",
      "Average Loss: 0.327105\n",
      "Training F1: 0.9903\n",
      "Test F1: 0.9911\n",
      "==================================================\n",
      "Epoch #67 : Batch 384/384 -- Loss: 0.33495\n",
      "Average Loss: 0.324546\n",
      "Training F1: 0.9905\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #68 : Batch 384/384 -- Loss: 0.32508\n",
      "Average Loss: 0.324278\n",
      "Training F1: 0.9905\n",
      "Test F1: 0.9915\n",
      "==================================================\n",
      "Epoch #69 : Batch 384/384 -- Loss: 0.35975\n",
      "Average Loss: 0.326153\n",
      "Training F1: 0.9581\n",
      "Test F1: 0.962\n",
      "==================================================\n",
      "Epoch #70 : Batch 384/384 -- Loss: 0.32425\n",
      "Average Loss: 0.324401\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #71 : Batch 384/384 -- Loss: 0.31371\n",
      "Average Loss: 0.328379\n",
      "Training F1: 0.9886\n",
      "Test F1: 0.9899\n",
      "==================================================\n",
      "Epoch #72 : Batch 384/384 -- Loss: 0.33448\n",
      "Average Loss: 0.325509\n",
      "Training F1: 0.9905\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #73 : Batch 384/384 -- Loss: 0.31366\n",
      "Average Loss: 0.324226\n",
      "Training F1: 0.9897\n",
      "Test F1: 0.9908\n",
      "==================================================\n",
      "Epoch #74 : Batch 384/384 -- Loss: 0.31354\n",
      "Average Loss: 0.32313\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #75 : Batch 384/384 -- Loss: 0.31369\n",
      "Average Loss: 0.322986\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #76 : Batch 384/384 -- Loss: 0.33884\n",
      "Average Loss: 0.328001\n",
      "Training F1: 0.94\n",
      "Test F1: 0.9437\n",
      "==================================================\n",
      "Epoch #77 : Batch 384/384 -- Loss: 0.34473\n",
      "Average Loss: 0.328428\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #78 : Batch 384/384 -- Loss: 0.34465\n",
      "Average Loss: 0.323007\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #79 : Batch 384/384 -- Loss: 0.32378\n",
      "Average Loss: 0.322962\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #80 : Batch 384/384 -- Loss: 0.33428\n",
      "Average Loss: 0.322925\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #81 : Batch 384/384 -- Loss: 0.31342\n",
      "Average Loss: 0.322869\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #82 : Batch 384/384 -- Loss: 0.32481\n",
      "Average Loss: 0.328915\n",
      "Training F1: 0.9889\n",
      "Test F1: 0.9898\n",
      "==================================================\n",
      "Epoch #83 : Batch 384/384 -- Loss: 0.33451\n",
      "Average Loss: 0.323737\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #84 : Batch 384/384 -- Loss: 0.31342\n",
      "Average Loss: 0.322862\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #85 : Batch 384/384 -- Loss: 0.31381\n",
      "Average Loss: 0.322829\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #86 : Batch 384/384 -- Loss: 0.32375\n",
      "Average Loss: 0.322816\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #87 : Batch 384/384 -- Loss: 0.33418\n",
      "Average Loss: 0.322797\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #88 : Batch 384/384 -- Loss: 0.32392\n",
      "Average Loss: 0.331673\n",
      "Training F1: 0.9905\n",
      "Test F1: 0.9915\n",
      "==================================================\n",
      "Epoch #89 : Batch 384/384 -- Loss: 0.32558\n",
      "Average Loss: 0.325391\n",
      "Training F1: 0.9904\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #90 : Batch 384/384 -- Loss: 0.33425\n",
      "Average Loss: 0.322906\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #91 : Batch 384/384 -- Loss: 0.32386\n",
      "Average Loss: 0.322828\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #92 : Batch 384/384 -- Loss: 0.33355\n",
      "Average Loss: 0.323889\n",
      "Training F1: 0.9841\n",
      "Test F1: 0.9844\n",
      "==================================================\n",
      "Epoch #93 : Batch 384/384 -- Loss: 0.33421\n",
      "Average Loss: 0.323298\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #94 : Batch 384/384 -- Loss: 0.31332\n",
      "Average Loss: 0.322891\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #95 : Batch 384/384 -- Loss: 0.31341\n",
      "Average Loss: 0.322786\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #96 : Batch 384/384 -- Loss: 0.31343\n",
      "Average Loss: 0.322759\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #97 : Batch 384/384 -- Loss: 0.31348\n",
      "Average Loss: 0.329745\n",
      "Training F1: 0.9905\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #98 : Batch 384/384 -- Loss: 0.32375\n",
      "Average Loss: 0.322825\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #99 : Batch 384/384 -- Loss: 0.31338\n",
      "Average Loss: 0.322773\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n",
      "Epoch #100: Batch 384/384 -- Loss: 0.32378\n",
      "Average Loss: 0.322756\n",
      "Training F1: 0.9906\n",
      "Test F1: 0.9916\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyTorchBaseline(\n",
       "  (lstm): LSTM(2, 16, bidirectional=True)\n",
       "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "train(pytorch, train_x, train_y, test_x, test_y, epochs=100, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "==================\n",
      "# of parameters: 2498\n",
      "lstm.model.0.weights     : torch.Size([18, 64])\n",
      "lstm.model.0.bias        : torch.Size([64])\n",
      "lstm.model_rev.0.weights : torch.Size([18, 64])\n",
      "lstm.model_rev.0.bias    : torch.Size([64])\n",
      "fc.weight                : torch.Size([2, 32])\n",
      "fc.bias                  : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\\n{}\".format(\"=\" * len(\"Our implementation\")))\n",
    "print(\"# of parameters: {}\".format(our.count_parameters()))\n",
    "for name, param in our.named_parameters():\n",
    "    print(\"{:<25}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch implementation\n",
      "======================\n",
      "# of parameters: 2626\n",
      "lstm.weight_ih_l0             : torch.Size([64, 2])\n",
      "lstm.weight_hh_l0             : torch.Size([64, 16])\n",
      "lstm.bias_ih_l0               : torch.Size([64])\n",
      "lstm.bias_hh_l0               : torch.Size([64])\n",
      "lstm.weight_ih_l0_reverse     : torch.Size([64, 2])\n",
      "lstm.weight_hh_l0_reverse     : torch.Size([64, 16])\n",
      "lstm.bias_ih_l0_reverse       : torch.Size([64])\n",
      "lstm.bias_hh_l0_reverse       : torch.Size([64])\n",
      "fc.weight                     : torch.Size([2, 32])\n",
      "fc.bias                       : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch implementation\\n{}\".format(\"=\" * len(\"PyTorch implementation\")))\n",
    "print(\"# of parameters: {}\".format(pytorch.count_parameters()))\n",
    "for name, param in pytorch.named_parameters():\n",
    "    print(\"{:<30}: {}\".format(name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses $Wh + b_h + Wx + b_x$ whereas we are using $Wx' + b$, where $x'$ is $h, x$ concatenated. Therefore PyTorch has an extra set of biases for each direction for the encoder and also for the decoder.\n",
    "\n",
    "For one direction - 64 <br>\n",
    "For reverse direction - 64 <br>\n",
    "For the decoder - 128 <br>\n",
    "\n",
    "Our model has $2498$ parameters while the PyTorch model has $2498 + 64 + 64 = 2626$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:13:20.695010Z",
     "start_time": "2019-07-14T21:13:20.676695Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_transformer(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer):\n",
    "    train_size = train_x.shape[1]\n",
    "    \n",
    "    for i in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_tracker = []\n",
    "        ordering = torch.randperm(train_size)\n",
    "        train_x = train_x[:,ordering,:]\n",
    "        train_y = train_y[:,ordering,:]\n",
    "        for j in range(int(float(train_size)/batch_size) + 1):\n",
    "            optimizer.zero_grad()\n",
    "            start = j*batch_size\n",
    "            end = min((j+1)*batch_size, train_size)\n",
    "            batch = end - start\n",
    "            if batch is 0:\n",
    "                continue\n",
    "            # forward pass\n",
    "            o = model(train_x[:,start:end,:])\n",
    "            # backward pass\n",
    "            o = o.contiguous().view(-1, train_x.shape[-1])\n",
    "            gt = torch.argmax(train_y[:,start:end,:], 2, keepdim=True).view(-1)\n",
    "            loss = loss_fn(o, gt)            \n",
    "            loss_tracker.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
    "                  \"Loss: {:2.5}\".format(i, j+1, int(train_size/batch_size), \n",
    "                                        loss_tracker[-1]), end='\\r')\n",
    "        print()\n",
    "        f1_train = evaluate_transformer(model, train_x, train_y)\n",
    "        f1_test = evaluate_transformer(model, test_x, test_y)\n",
    "        print(\"Average Loss: {:2.6}\".format(np.mean(loss_tracker)))\n",
    "        print(\"Training F1: {:3.4}\".format(f1_train))\n",
    "        print(\"Test F1: {:3.4}\".format(f1_test))\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_transformer(model, x, y):\n",
    "    model.eval()\n",
    "    test_size = x.shape[1]    \n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    for j in range(int(test_size/batch_size) + 1):\n",
    "        optimizer.zero_grad()\n",
    "        start = j*batch_size\n",
    "        end = min((j+1)*batch_size, test_size)\n",
    "        batch = end - start\n",
    "        if batch == 0:\n",
    "            continue\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            o = model(x[:,start:end,:])\n",
    "        # get predictions\n",
    "        pred = torch.argmax(o, 2, keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        preds.extend(pred)\n",
    "        label = torch.argmax(y[:,start:end,:], 2, \n",
    "                             keepdim=True).view(-1).cpu().detach().numpy()\n",
    "        labels.extend(label)\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:13:47.123402Z",
     "start_time": "2019-07-14T21:13:47.113658Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformer import PositionalEncoding, Encoder\n",
    "\n",
    "class TransformerSeq2SeqSame(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # define layers\n",
    "        # embedding layers\n",
    "        self.src_embed = nn.Linear(in_dim, model_dim)\n",
    "        self.pos_enc = PositionalEncoding(model_dim, max_len)\n",
    "        # encoder-decoder\n",
    "        self.encoder = Encoder(N, heads, model_dim, key_dim, value_dim, ff_dim)\n",
    "        # final output layer\n",
    "        self.fc = nn.Linear(model_dim, out_dim)\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            src = src.transpose(0, 1)\n",
    "            \n",
    "        ## get encoder attention from source\n",
    "        src = self.src_embed(src)\n",
    "        src = self.pos_enc(src)\n",
    "        x = self.encoder(src, src_mask)\n",
    "          \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # transpose to use [batch, seq_len, dim]\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        return x\n",
    "        \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:13:49.769891Z",
     "start_time": "2019-07-14T21:13:49.748486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerSeq2SeqSame(in_dim=2, out_dim=2, N=1, heads=4, model_dim=12, \n",
    "                                     key_dim=3, value_dim=3, ff_dim=64)\n",
    "\n",
    "print(transformer.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T21:16:31.526021Z",
     "start_time": "2019-07-14T21:13:51.158041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.56472\n",
      "Average Loss: 0.64766\n",
      "Training F1: 0.6942\n",
      "Test F1: 0.6869\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.39044\n",
      "Average Loss: 0.508765\n",
      "Training F1: 0.8178\n",
      "Test F1: 0.8112\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.41734\n",
      "Average Loss: 0.4053\n",
      "Training F1: 0.877\n",
      "Test F1: 0.871\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.14318\n",
      "Average Loss: 0.286434\n",
      "Training F1: 0.9308\n",
      "Test F1: 0.9274\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.217268\n",
      "Average Loss: 0.16742\n",
      "Training F1: 0.9738\n",
      "Test F1: 0.9733\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.108818\n",
      "Average Loss: 0.116724\n",
      "Training F1: 0.9902\n",
      "Test F1: 0.9902\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.061353\n",
      "Average Loss: 0.0835265\n",
      "Training F1: 0.9966\n",
      "Test F1: 0.9966\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.0126483\n",
      "Average Loss: 0.0675894\n",
      "Training F1: 0.9996\n",
      "Test F1: 0.9997\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.0817935\n",
      "Average Loss: 0.0529117\n",
      "Training F1: 0.9711\n",
      "Test F1: 0.9715\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.0157022\n",
      "Average Loss: 0.0457065\n",
      "Training F1: 0.9983\n",
      "Test F1: 0.9984\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.0362017\n",
      "Average Loss: 0.0382736\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.0181571\n",
      "Average Loss: 0.0330032\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.0078229\n",
      "Average Loss: 0.027211\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.0285282\n",
      "Average Loss: 0.0257316\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.00834189\n",
      "Average Loss: 0.0198788\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.00478964\n",
      "Average Loss: 0.0186717\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.00478959\n",
      "Average Loss: 0.0164241\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 0.00571911\n",
      "Average Loss: 0.0160143\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.00159239\n",
      "Average Loss: 0.0121808\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.00053197\n",
      "Average Loss: 0.0113094\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.01925554\n",
      "Average Loss: 0.0106291\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.00190334\n",
      "Average Loss: 0.00882395\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.00071874\n",
      "Average Loss: 0.00840545\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.02374735\n",
      "Average Loss: 0.00801689\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 0.01614849\n",
      "Average Loss: 0.00869007\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.00895935\n",
      "Average Loss: 0.00694127\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.00027903\n",
      "Average Loss: 0.00773958\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.00089463\n",
      "Average Loss: 0.00439603\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 0.01056745\n",
      "Average Loss: 0.00443732\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.00255265\n",
      "Average Loss: 0.00519408\n",
      "Training F1: 0.9965\n",
      "Test F1: 0.996\n",
      "==================================================\n",
      "Epoch #31 : Batch 384/384 -- Loss: 0.00020472\n",
      "Average Loss: 0.0047956\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #32 : Batch 384/384 -- Loss: 2.8563e-05\n",
      "Average Loss: 0.00513302\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #33 : Batch 384/384 -- Loss: 0.00021388\n",
      "Average Loss: 0.00422593\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #34 : Batch 384/384 -- Loss: 0.00013341\n",
      "Average Loss: 0.00417629\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #35 : Batch 384/384 -- Loss: 0.00594435\n",
      "Average Loss: 0.00409838\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #36 : Batch 384/384 -- Loss: 0.00963515\n",
      "Average Loss: 0.0036327\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #37 : Batch 384/384 -- Loss: 0.00013028\n",
      "Average Loss: 0.00307906\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #38 : Batch 384/384 -- Loss: 0.01306541\n",
      "Average Loss: 0.00290886\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #39 : Batch 384/384 -- Loss: 0.00933279\n",
      "Average Loss: 0.00482051\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9974\n",
      "==================================================\n",
      "Epoch #40 : Batch 384/384 -- Loss: 2.865e-055\n",
      "Average Loss: 0.00245747\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #41 : Batch 384/384 -- Loss: 0.00238068\n",
      "Average Loss: 0.00203107\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #42 : Batch 384/384 -- Loss: 9.2688e-05\n",
      "Average Loss: 0.00255906\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #43 : Batch 384/384 -- Loss: 0.00116624\n",
      "Average Loss: 0.00258677\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #44 : Batch 384/384 -- Loss: 0.00103315\n",
      "Average Loss: 0.00251287\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #45 : Batch 384/384 -- Loss: 7.4799e-05\n",
      "Average Loss: 0.00266751\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #46 : Batch 384/384 -- Loss: 0.00041746\n",
      "Average Loss: 0.00167212\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #47 : Batch 384/384 -- Loss: 3.3279e-06\n",
      "Average Loss: 0.00142414\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #48 : Batch 384/384 -- Loss: 0.00015541\n",
      "Average Loss: 0.00181992\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #49 : Batch 384/384 -- Loss: 0.00293654\n",
      "Average Loss: 0.00252916\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #50 : Batch 265/384 -- Loss: 0.00320325\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b7d4328097c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer = train_transformer(transformer, train_x, train_y, test_x, test_y, \n\u001b[0;32m----> 2\u001b[0;31m                                 epochs=epochs, loss_fn=loss_fn, optimizer=optimizer)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c7a74e88b768>\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-53b93231a161>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-project/SequenceModelling/src/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# get attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;31m# x = self.norm(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-project/SequenceModelling/src/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# position wise feed forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-project/SequenceModelling/src/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# add and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 157\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \"\"\"\n\u001b[1;32m   1724\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 1725\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer = train_transformer(transformer, train_x, train_y, test_x, test_y, \n",
    "                                epochs=epochs, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline implementation\n",
    "\n",
    "From github: https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T19:48:54.625152Z",
     "start_time": "2019-07-13T19:48:54.605340Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformer_baseline as tb\n",
    "\n",
    "\n",
    "## Padding masks - for 3 dim input\n",
    "def get_attn_key_pad_mask(seq_k, seq_q, pad=tb.Constants.PAD):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    assert seq_k.dim() == 3 and seq_q.dim() == 3\n",
    "    \n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = torch.all(seq_q.eq(pad), dim=-1)  # b x lq\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "    return padding_mask\n",
    "\n",
    "def get_non_pad_mask(seq, pad=tb.Constants.PAD):\n",
    "    assert seq.dim() == 3\n",
    "    padding_mask = ~torch.all(seq.ne(pad), dim=-1)  # b x l\n",
    "#     padding_mask = padding_mask.repeat(1, 1, seq.shape[-1])  # b x l x d (repeated)\n",
    "    return padding_mask.type(torch.float).unsqueeze(-1)\n",
    "\n",
    "\n",
    "## Model\n",
    "class TransformerBaseline(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, N, heads, model_dim, key_dim, value_dim, ff_dim, \n",
    "                 max_len=10000, batch_first=True, pad=tb.Constants.PAD):\n",
    "        super().__init__()\n",
    "        self.name = 'transformer'\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.encoder = tb.Models.Encoder(\n",
    "            n_src_vocab=in_dim, len_max_seq=max_len,\n",
    "            d_word_vec=model_dim, d_model=model_dim, d_inner=ff_dim,\n",
    "            n_layers=N, n_head=heads, d_k=key_dim, d_v=value_dim,\n",
    "            dropout=0.0, embedding='linear')\n",
    "        \n",
    "        self.fc = nn.Linear(model_dim, out_dim)\n",
    "        \n",
    "        # This was important from their code. \n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        \n",
    "        # encoder requires source sequence & positions of each \n",
    "        x_pos = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0], 1)\n",
    "        \n",
    "        # -- Prepare masks\n",
    "        # encoder\n",
    "        e_slf_attn_mask = get_attn_key_pad_mask(seq_k=x, seq_q=x, pad=self.pad)\n",
    "        e_non_pad_mask = torch.ones(x.shape[0], x.shape[1], 1)\n",
    "        e_non_pad_mask = get_non_pad_mask(x, pad=self.pad)\n",
    "        \n",
    "        attn, *_ = self.encoder(x, x_pos, e_slf_attn_mask, e_non_pad_mask)\n",
    "        x = self.fc(attn)\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0,1)\n",
    "        return x\n",
    "      \n",
    "    def save(self, file_path='./model.pkl'):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def count_parameters(self):\n",
    "        tot_sum = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return tot_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T19:48:56.120893Z",
     "start_time": "2019-07-13T19:48:55.866513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344\n"
     ]
    }
   ],
   "source": [
    "pad = torch.tensor([0,0]).float()\n",
    "\n",
    "baseline = TransformerBaseline(in_dim=2, out_dim=2, N=1, heads=4, model_dim=12, \n",
    "                               key_dim=3, value_dim=3, ff_dim=64, \n",
    "                               batch_first=False, pad=pad)\n",
    "\n",
    "print(baseline.count_parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T19:51:26.645742Z",
     "start_time": "2019-07-13T19:48:57.821008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1  : Batch 384/384 -- Loss: 0.56578\n",
      "Average Loss: 0.638133\n",
      "Training F1: 0.711\n",
      "Test F1: 0.7042\n",
      "==================================================\n",
      "Epoch #2  : Batch 384/384 -- Loss: 0.26251\n",
      "Average Loss: 0.409439\n",
      "Training F1: 0.9372\n",
      "Test F1: 0.9353\n",
      "==================================================\n",
      "Epoch #3  : Batch 384/384 -- Loss: 0.106741\n",
      "Average Loss: 0.175441\n",
      "Training F1: 0.9952\n",
      "Test F1: 0.9948\n",
      "==================================================\n",
      "Epoch #4  : Batch 384/384 -- Loss: 0.0804535\n",
      "Average Loss: 0.0773654\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #5  : Batch 384/384 -- Loss: 0.0165111\n",
      "Average Loss: 0.042578\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #6  : Batch 384/384 -- Loss: 0.02163742\n",
      "Average Loss: 0.0233776\n",
      "Training F1: 0.9983\n",
      "Test F1: 0.998\n",
      "==================================================\n",
      "Epoch #7  : Batch 384/384 -- Loss: 0.01174728\n",
      "Average Loss: 0.0159691\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #8  : Batch 384/384 -- Loss: 0.00117437\n",
      "Average Loss: 0.0135939\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #9  : Batch 384/384 -- Loss: 0.00192751\n",
      "Average Loss: 0.0116191\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #10 : Batch 384/384 -- Loss: 0.00142676\n",
      "Average Loss: 0.00964245\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #11 : Batch 384/384 -- Loss: 0.01741371\n",
      "Average Loss: 0.00808301\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #12 : Batch 384/384 -- Loss: 0.05985632\n",
      "Average Loss: 0.00906201\n",
      "Training F1: 0.9974\n",
      "Test F1: 0.9969\n",
      "==================================================\n",
      "Epoch #13 : Batch 384/384 -- Loss: 0.00505296\n",
      "Average Loss: 0.00702815\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #14 : Batch 384/384 -- Loss: 0.00017518\n",
      "Average Loss: 0.00607508\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #15 : Batch 384/384 -- Loss: 0.00251625\n",
      "Average Loss: 0.00535041\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #16 : Batch 384/384 -- Loss: 0.00019942\n",
      "Average Loss: 0.00551781\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #17 : Batch 384/384 -- Loss: 0.00046311\n",
      "Average Loss: 0.00449128\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #18 : Batch 384/384 -- Loss: 4.3017e-05\n",
      "Average Loss: 0.00405127\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #19 : Batch 384/384 -- Loss: 0.00010287\n",
      "Average Loss: 0.00422408\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #20 : Batch 384/384 -- Loss: 0.00040424\n",
      "Average Loss: 0.00358811\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #21 : Batch 384/384 -- Loss: 0.00263439\n",
      "Average Loss: 0.00486536\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #22 : Batch 384/384 -- Loss: 0.00044573\n",
      "Average Loss: 0.00382586\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #23 : Batch 384/384 -- Loss: 0.00120095\n",
      "Average Loss: 0.0021661\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #24 : Batch 384/384 -- Loss: 0.00035199\n",
      "Average Loss: 0.00351471\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #25 : Batch 384/384 -- Loss: 3.1588e-05\n",
      "Average Loss: 0.0032384\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #26 : Batch 384/384 -- Loss: 0.00030386\n",
      "Average Loss: 0.00352623\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #27 : Batch 384/384 -- Loss: 0.01741637\n",
      "Average Loss: 0.00315638\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #28 : Batch 384/384 -- Loss: 0.00018172\n",
      "Average Loss: 0.00232762\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #29 : Batch 384/384 -- Loss: 3.7889e-05\n",
      "Average Loss: 0.00234404\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #30 : Batch 384/384 -- Loss: 0.00019384\n",
      "Average Loss: 0.00276833\n",
      "Training F1: 1.0\n",
      "Test F1: 1.0\n",
      "==================================================\n",
      "Epoch #31 : Batch 363/384 -- Loss: 5.4239e-05\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f4590fe23f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m baseline = train_transformer(baseline, train_x, train_y, test_x, test_y, \n\u001b[0;32m----> 2\u001b[0;31m                                 epochs=100, loss_fn=loss_fn, optimizer=optimizer)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-c7a74e88b768>\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(model, train_x, train_y, test_x, test_y, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             print(\"Epoch #{:<3d}: Batch {:>3d}/{:<3d} -- \"\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ashwin/OS/Users/Ashwin/Documents/My Documents/MS/Freiburg/2019-SS/DLlab/dl-lab-ss19/labenv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline = train_transformer(baseline, train_x, train_y, test_x, test_y, \n",
    "                                epochs=100, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
